["^ ","~:output",["^ ","~:js","goog.provide('personal_website.content.writings.blog_posts.transformer_post.post');\npersonal_website.content.writings.blog_posts.transformer_post.post.post_preview = \"The transformer \\u2013 a neural network model desiged to replace traditional\\n  seq2seq architecture \\u2013 introduces a powerful abstraction to think about\\n  attention.\";\npersonal_website.content.writings.blog_posts.transformer_post.post.post_content = new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,\"div\",\"div\",1057191632),new cljs.core.PersistentVector(null, 6, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,\"p\",\"p\",151049309),\"The transformer \\u2014 a neural network model proposed in \",personal_website.content.utils.link(\"Vsawani et. al\",\"https://arxiv.org/pdf/1706.03762.pdf\"),\" \\u2013 has taken the NLP world by storm in the past few years, contributing to the recent\\n    success of \",personal_website.content.utils.link(\"OpenAI\\u2019s GPT-3\",\"https://arxiv.org/pdf/2005.14165.pdf\"),\" model among many others. However while there is an abundance of online material\\n    (see further reading) describing the transformer architecture, a crucial idea\\n    present in the original paper \\u2013 the key, query, and value abstraction \\u2013 lacks a clear accessible\\n    explanation online, especially from the perspective of what motivated it and why it works.\\n    The focus of this post is to hopefully provide some of that intuition\\n    and demonstrate why it is so powerful.\"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,\"p\",\"p\",151049309),\"The transformer\\u2019s attention abstraction is an incredibly helpful tool with which to think\\n   about the concept of self-attention, and even more broadly representation learning in general.\\n   Specifically, it operates on the principle that computing multiple representations of a single\\n   entity which each highlight particular characteristics of that entity is more feasible than\\n   trying to compute a single representation which is able to highlight those same characteristics\\n   with similarly granular detail and still be manageably produced and operated over by a neural net.\\n   That is a veritable mouthful, but in other words: many [smaller] representations are better than\\n   [a larger] one.\"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,\"p\",\"p\",151049309),\"Before we delve further into how the transformer phrases self-attention mathematically though, it's worth clarifying what exactly \\u201Cself-attention\\u201D means. In a single sentence, it is the ability for the elements of an input to selectively incorporate information from other elements of that same input when being processed by a neural net, where the degree to which they incorporate information from those other elements is determined by a learnable function.  This approach has proven to be especially useful in language tasks \\u2013 although it is in no way limited to the NLP field \\u2013 as it allows the mapping of informational dependencies across the entire input, whereas other approaches such as RNNs degrade the farther away elements of the input are. By allowing individual elements of the input to \\u201Cattend\\u201D to any other element implicit context can be shared throughout the entire input, allowing networks to build up more robust semantic representations of the text they are fed and to work with longer, more complicated text while still being capable of similar comprehension.\"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,\"p\",\"p\",151049309),\"Again, while the concept of self-attention is not exclusive to language-based tasks, the transformer \\u2013 and even the idea of attention itself \\u2013 was designed with this domain in mind, and so approaching the concept from this perspective will make laying down the necessary intuition much easier. From there, we can extrapolate the concept to other problem domains.  With this in mind, let\\u2019s consider how we might program self-attention into a neural net for some language task. A key part of the attentional paradigm is being able to selectively incorporate information from other sources  \\u2013 in the case of self-attention specifically, from other elements of the input. In our example, this equates to other words. But how exactly do we represent this information?\\n\\nA naive implementation might encode each word into a one-hot vector that considers the first thousand-or-so most common words. This would (obviously) not only be computationally inefficient, but fail to make full use of all the properties of the embedding space. For one, the space would be very sparsely populated, with only the pure unit vectors () of the space holding any real semantic meaning, and as a consequence all representations would be orthogonal to one another \\u2013 in other words, equally different. This is not a faithful depiction of the words being represented though, as some are definitely more similar than others and giving the networks this information explicitly will help in making their outputs similar too. But how exactly do we make words with similar meanings \\u201Ccloser\\u201D and vice versa?\\n\\nThis is a problem the famous word2vec paper solved with learnable embeddings (in other words, using neural networks to produce representations with the properties we want), and it\\u2019s far from a new idea either. In fact, learnable embeddings are used for essentially all language tasks nowadays. I retell it only to explicitly acknowledge the idea that how we represent data is a choice, and that to represent it in such a way that exposes valuable information to our network we sometimes must learn those representations themselves. In other words, we must be able to learn the representations we want to learn with.\\n\\nIn most cases, this would be enough. We\\u2019ve computed representations which give the network a sense of how words are related and provided it with an embedding space powerful enough to encode new words as it encounters them. But we also want a way to calculate how much attention any given word should focus on another, preferably by the same metric (closness) as it is relatively cheap and easy to compute (via the dot product).  The only problem is that we\\u2019ve already optimized the similarity between two vectors under the perspective of the similarity between the meanings of the words they represent. Furthermore, just because\"], null)], null);\npersonal_website.content.writings.blog_posts.transformer_post.post.post = new cljs.core.PersistentArrayMap(null, 7, [new cljs.core.Keyword(null,\"title\",\"title\",636505583),\"Attention Via The Transformer\",new cljs.core.Keyword(null,\"date\",\"date\",-1463434462),\"2020/8/29\",new cljs.core.Keyword(null,\"show\",\"show\",-576705889),personal_website.content.writings.blog_posts.transformer_post.post.post_preview,new cljs.core.Keyword(null,\"content\",\"content\",15833224),personal_website.content.writings.blog_posts.transformer_post.post.post_content,new cljs.core.Keyword(null,\"tags\",\"tags\",1771418977),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [\"mathematics\",\"differentiable programming\"], null),new cljs.core.Keyword(null,\"type\",\"type\",1174270348),\"blog-post\",new cljs.core.Keyword(null,\"id\",\"id\",-1388402092),\"0\"], null);\n","~:ns-info",["^ ","~:rename-macros",null,"~:renames",["^ "],"~:meta",["^ ","~:file","personal_website/content/writings/blog_posts/transformer_post/post.cljs","~:line",1,"~:column",5,"~:end-line",1,"~:end-column",71],"~:ns-aliases",["^ ","~$cljs.loader","~$shadow.loader","~$clojure.pprint","~$cljs.pprint","~$react","~$module$node_modules$react$index","~$react-dom","~$module$node_modules$react_dom$index","~$clojure.spec.alpha","~$cljs.spec.alpha","~$clojure.spec.gen.alpha","~$cljs.spec.gen.alpha"],"~:use-macros",null,"~:excludes",["~#set",[]],"~:name","~$personal-website.content.writings.blog-posts.transformer-post.post","~:op","~:ns","~:imports",null,"~:requires",["^ ","~$personal-website.content.utils","^Q","~$utils","^Q","~$cljs.core","^S","~$goog","^T"],"~:seen",["^J",["~:require"]],"~:uses",null,"~:require-macros",["^ ","^S","^S"],"~:form",["~#list",["~$ns","^L",["^Z",["^V",["^Q","~:as","^R"]]]]],"~:flags",["^ ","^V",["^J",[]]],"~:js-deps",["^ "],"~:deps",["^T","^S","^Q"]],"^N","^L","~:resource-id",["~:shadow.build.classpath/resource","personal_website/content/writings/blog_posts/transformer_post/post.cljs"],"~:compiled-at",1601920609168,"~:resource-name","personal_website/content/writings/blog_posts/transformer_post/post.cljs","~:warnings",[],"~:source","(ns personal-website.content.writings.blog-posts.transformer-post.post\n  (:require [personal-website.content.utils :as utils]))\n\n\n(def post-preview\n  \"The transformer – a neural network model desiged to replace traditional\n  seq2seq architecture – introduces a powerful abstraction to think about\n  attention.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   [:p \"The transformer — a neural network model proposed in \"\n    (utils/link \"Vsawani et. al\" \"https://arxiv.org/pdf/1706.03762.pdf\")\n    \" – has taken the NLP world by storm in the past few years, contributing to the recent\n    success of \" (utils/link \"OpenAI’s GPT-3\" \"https://arxiv.org/pdf/2005.14165.pdf\")\n    \" model among many others. However while there is an abundance of online material\n    (see further reading) describing the transformer architecture, a crucial idea\n    present in the original paper – the key, query, and value abstraction – lacks a clear accessible\n    explanation online, especially from the perspective of what motivated it and why it works.\n    The focus of this post is to hopefully provide some of that intuition\n    and demonstrate why it is so powerful.\"]\n\n\n   [:p \"The transformer’s attention abstraction is an incredibly helpful tool with which to think\n   about the concept of self-attention, and even more broadly representation learning in general.\n   Specifically, it operates on the principle that computing multiple representations of a single\n   entity which each highlight particular characteristics of that entity is more feasible than\n   trying to compute a single representation which is able to highlight those same characteristics\n   with similarly granular detail and still be manageably produced and operated over by a neural net.\n   That is a veritable mouthful, but in other words: many [smaller] representations are better than\n   [a larger] one.\"]\n\n[:p \"Before we delve further into how the transformer phrases self-attention mathematically though, it's worth clarifying what exactly “self-attention” means. In a single sentence, it is the ability for the elements of an input to selectively incorporate information from other elements of that same input when being processed by a neural net, where the degree to which they incorporate information from those other elements is determined by a learnable function.  This approach has proven to be especially useful in language tasks – although it is in no way limited to the NLP field – as it allows the mapping of informational dependencies across the entire input, whereas other approaches such as RNNs degrade the farther away elements of the input are. By allowing individual elements of the input to “attend” to any other element implicit context can be shared throughout the entire input, allowing networks to build up more robust semantic representations of the text they are fed and to work with longer, more complicated text while still being capable of similar comprehension.\"]\n\n[:p \"Again, while the concept of self-attention is not exclusive to language-based tasks, the transformer – and even the idea of attention itself – was designed with this domain in mind, and so approaching the concept from this perspective will make laying down the necessary intuition much easier. From there, we can extrapolate the concept to other problem domains.  With this in mind, let’s consider how we might program self-attention into a neural net for some language task. A key part of the attentional paradigm is being able to selectively incorporate information from other sources  – in the case of self-attention specifically, from other elements of the input. In our example, this equates to other words. But how exactly do we represent this information?\n\nA naive implementation might encode each word into a one-hot vector that considers the first thousand-or-so most common words. This would (obviously) not only be computationally inefficient, but fail to make full use of all the properties of the embedding space. For one, the space would be very sparsely populated, with only the pure unit vectors () of the space holding any real semantic meaning, and as a consequence all representations would be orthogonal to one another – in other words, equally different. This is not a faithful depiction of the words being represented though, as some are definitely more similar than others and giving the networks this information explicitly will help in making their outputs similar too. But how exactly do we make words with similar meanings “closer” and vice versa?\n\nThis is a problem the famous word2vec paper solved with learnable embeddings (in other words, using neural networks to produce representations with the properties we want), and it’s far from a new idea either. In fact, learnable embeddings are used for essentially all language tasks nowadays. I retell it only to explicitly acknowledge the idea that how we represent data is a choice, and that to represent it in such a way that exposes valuable information to our network we sometimes must learn those representations themselves. In other words, we must be able to learn the representations we want to learn with.\n\nIn most cases, this would be enough. We’ve computed representations which give the network a sense of how words are related and provided it with an embedding space powerful enough to encode new words as it encounters them. But we also want a way to calculate how much attention any given word should focus on another, preferably by the same metric (closness) as it is relatively cheap and easy to compute (via the dot product).  The only problem is that we’ve already optimized the similarity between two vectors under the perspective of the similarity between the meanings of the words they represent. Furthermore, just because\"]\n\n\n  ])\n\n(def post\n  {:title \"Attention Via The Transformer\"\n   :date \"2020/8/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :id \"0\"})\n","~:reader-features",["^J",["~:cljs"]],"~:cljc",false,"~:source-map-compact",["^ ","mappings",";AAIA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGD,AAAA,AAAA,AAACC,AAEY,AAAA,AAAA,AAACA;AA+BlB,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIH,AACGC","names",["personal-website.content.writings.blog-posts.transformer-post.post/post-preview","personal-website.content.writings.blog-posts.transformer-post.post/post-content","personal-website.content.utils/link","personal-website.content.writings.blog-posts.transformer-post.post/post"]],"~:used-vars",["^J",["~$personal-website.content.writings.blog-posts.transformer-post.post/post-preview","~$personal-website.content.writings.blog-posts.transformer-post.post/post-content","~$personal-website.content.writings.blog-posts.transformer-post.post/post","~$personal-website.content.utils/link"]]],"~:cache-keys",["~#cmap",[["^15","goog/dom/tagname.js"],["6025affb7181cd40418600864f58eed1ea80055d","~:shadow.build.compiler/resolve",["^ ","~:require-id",null,"~:deps-ids",["^J",[]],"~:deps-syms",["^T","~$goog.dom.HtmlElement"]]],["^15","goog/math/math.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","~$goog.array","~$goog.asserts"]]],["^15","goog/html/trustedtypes.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/labs/useragent/browser.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","~$goog.labs.userAgent.util","~$goog.object","~$goog.string.internal"]]],["^15","goog/html/safeurl.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M","~$goog.fs.url","~$goog.html.TrustedResourceUrl","~$goog.i18n.bidi.Dir","~$goog.i18n.bidi.DirectionalString","~$goog.string.Const","~$goog.string.TypedString","^1P"]]],["^15","goog/array/array.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M"]]],["^15","goog/debug/error.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/dom/nodetype.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/string/typedstring.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/object/object.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/dom/asserts.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M"]]],"~:SHADOW-TIMESTAMP",[1600470677000,1600470677000,1593530297000],["^15","goog/math/long.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M","~$goog.reflect"]]],["^15","personal_website/content/utils.cljs"],["b2273d23749d664b63905efe32fcbaac806f2774","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^S"]]],["^15","goog/html/trustedresourceurl.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M","~$goog.html.trustedtypes","^1S","^1T","^1U","^1V"]]],["^15","goog/string/internal.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/functions/functions.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/html/safestyle.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","^1M","~$goog.html.SafeUrl","^1U","^1V","^1P"]]],["^15","goog/dom/safe.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M","~$goog.dom.asserts","~$goog.functions","~$goog.html.SafeHtml","~$goog.html.SafeScript","~$goog.html.SafeStyle","^1Z","^1R","~$goog.html.uncheckedconversions","^1U","^1P"]]],["^15","goog/structs/map.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","~$goog.iter.Iterator","~$goog.iter.StopIteration"]]],["^15","goog/html/safehtml.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","^1M","~$goog.dom.TagName","~$goog.dom.tags","^22","^23","~$goog.html.SafeStyleSheet","^1Z","^1R","^1Y","^1S","^1T","~$goog.labs.userAgent.browser","^1O","^1U","^1V","^1P"]]],["^15","goog/dom/tags.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1O"]]],["^15","goog/asserts/asserts.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","~$goog.debug.Error","~$goog.dom.NodeType"]]],["^15","goog/uri/uri.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","^1M","~$goog.string","~$goog.structs","~$goog.structs.Map","~$goog.uri.utils","~$goog.uri.utils.ComponentIndex","~$goog.uri.utils.StandardQueryParam"]]],["^15","goog/i18n/bidi.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/fs/url.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/base.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",[]]],["^15","goog/structs/structs.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","^1O"]]],["^15","goog/string/string.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","~$goog.dom.safe","^24","^1U","^1P"]]],["^15","goog/reflect/reflect.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/labs/useragent/util.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1P"]]],["^15","goog/string/stringbuffer.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","goog/iter/iter.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","^1M","^20","~$goog.math"]]],["^15","goog/html/uncheckedconversions.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M","^21","^22","^23","^29","^1Z","^1R","^1U","^1P"]]],["^15","goog/dom/htmlelement.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T"]]],["^15","cljs/core.cljs"],["9f9729dbbf9b814c83dc189977b447d2ae92b6cd","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","~$goog.math.Long","~$goog.math.Integer","^2=","^1O","^1L","~$goog.Uri","~$goog.string.StringBuffer"]]],["^15","goog/html/safescript.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M","^1Y","^1U","^1V"]]],["^15","personal_website/content/writings/blog_posts/transformer_post/post.cljs"],["d3f576435a35f7ea568e6947e569e90c9e5e3f64","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^S","^Q"]]],["^15","goog/html/safestylesheet.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","^1M","^23","^1O","^1U","^1V","^1P"]]],["^15","goog/math/integer.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1X"]]],["^15","goog/uri/utils.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1L","^1M","^2="]]],["^15","goog/string/const.js"],["6025affb7181cd40418600864f58eed1ea80055d","^1G",["^ ","^1H",null,"^1I",["^J",[]],"^1J",["^T","^1M","^1V"]]]]],"~:clj-info",["^ ","jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/tools.reader/1.3.3/tools.reader-1.3.3.jar!/clojure/tools/reader/impl/errors.clj",1600470675000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojure/1.10.1/clojure-1.10.1.jar!/clojure/java/io.clj",1566593952000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/core.cljc",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/source_map/base64.clj",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/data.json/1.0.0/data.json-1.0.0.jar!/clojure/data/json.clj",1600470675000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/source_map/base64_vlq.clj",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/tools.reader/1.3.3/tools.reader-1.3.3.jar!/clojure/tools/reader/default_data_readers.clj",1600470675000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/tools.reader/1.3.3/tools.reader-1.3.3.jar!/clojure/tools/reader/impl/inspect.clj",1600470675000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/util.cljc",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/compiler.cljc",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojure/1.10.1/clojure-1.10.1.jar!/clojure/string.clj",1566593952000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/tagged_literals.cljc",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/tools.reader/1.3.3/tools.reader-1.3.3.jar!/clojure/tools/reader.clj",1600470675000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/source_map.clj",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojure/1.10.1/clojure-1.10.1.jar!/clojure/set.clj",1566593952000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/js_deps.cljc",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/env.cljc",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/tools.reader/1.3.3/tools.reader-1.3.3.jar!/clojure/tools/reader/reader_types.clj",1600470675000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojure/1.10.1/clojure-1.10.1.jar!/clojure/instant.clj",1566593952000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/externs.clj",1593530297000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojure/1.10.1/clojure-1.10.1.jar!/clojure/pprint.clj",1566593952000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojure/1.10.1/clojure-1.10.1.jar!/clojure/core.clj",1566593952000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojure/1.10.1/clojure-1.10.1.jar!/clojure/edn.clj",1566593952000,"jar:file:/Users/Rohan%20Mehta/.m2/repository/org/clojure/clojurescript/1.10.773/clojurescript-1.10.773.jar!/cljs/analyzer.cljc",1593530297000],"~:analyzer",["^ ","^3",null,"^4",["^ "],"^5",["^ ","^6","personal_website/content/writings/blog_posts/transformer_post/post.cljs","^7",1,"^8",5,"^9",1,"^:",71],"^;",["^ ","^<","^=","^>","^?","^@","^A","^B","^C","^D","^E","^F","^G"],"^H",null,"^I",["^J",[]],"^K","^L","^O",null,"^P",["^ ","^Q","^Q","^R","^Q","^S","^S","^T","^T"],"^U",["^J",["^V"]],"^W",null,"~:defs",["^ ","~$post-preview",["^ ","^K","^1A","^6","personal_website/content/writings/blog_posts/transformer_post/post.cljs","^7",5,"^8",1,"^9",5,"^:",18,"^5",["^ ","^6","personal_website/content/writings/blog_posts/transformer_post/post.cljs","^7",5,"^8",6,"^9",5,"^:",18],"~:tag","~$string"],"~$post-content",["^ ","^K","^1B","^6","personal_website/content/writings/blog_posts/transformer_post/post.cljs","^7",10,"^8",1,"^9",10,"^:",18,"^5",["^ ","^6","personal_website/content/writings/blog_posts/transformer_post/post.cljs","^7",10,"^8",6,"^9",10,"^:",18],"^39","~$cljs.core/IVector"],"~$post",["^ ","^K","^1C","^6","personal_website/content/writings/blog_posts/transformer_post/post.cljs","^7",46,"^8",1,"^9",46,"^:",10,"^5",["^ ","^6","personal_website/content/writings/blog_posts/transformer_post/post.cljs","^7",46,"^8",6,"^9",46,"^:",10],"^39","~$cljs.core/IMap"]],"^X",["^ ","^S","^S"],"~:cljs.analyzer/constants",["^ ","^U",["^J",["~:tags","~:date","~:content","~:type","~:title","~:div","~:id","~:p","~:show"]],"~:order",["^3E","~:p","^3D","^3A","^3G","^3B","^3@","^3C","^3F"]],"^11",["^ ","^V",["^J",[]]],"^12",["^ "],"^13",["^T","^S","^Q"]],"^N","^L","~:ns-specs",["^ "],"~:ns-spec-vars",["^J",[]],"~:compiler-options",["^1F",[["^3K","~:static-fns"],true,["^3K","~:shadow-tweaks"],null,["^3K","~:source-map-inline"],null,["^3K","~:elide-asserts"],false,["^3K","~:optimize-constants"],null,["^3K","^18"],null,["^3K","~:external-config"],null,["^3K","~:tooling-config"],null,["^3K","~:emit-constants"],null,["^3K","~:load-tests"],null,["^3K","~:form-size-threshold"],null,["^3K","~:data-readers"],null,["^3K","~:infer-externs"],"~:auto",["^3K","^1:"],null,["~:js-options","~:js-provider"],"~:shadow",["~:mode"],"~:dev",["^3K","~:fn-invoke-direct"],null,["^3K","~:source-map"],"/dev/null"]]]