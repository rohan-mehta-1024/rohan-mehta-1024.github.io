<!DOCTYPE html>
<html><head><title>The Attention Mechanism Demystified</title><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><script src="https://unpkg.com/wolfram-notebook-embedder@0.3/dist/wolfram-notebook-embedder.min.js"></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script>window.MathJax = {
                      tex:    {
                               //        inlineMath: [['$', '$'], ['\(', '\)']],
                               packages: {'[+] ': ['mhchem']}
                               },
                      loader: {load: ['[tex]/mhchem']},
                      };</script><script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script><link href="/images/favicon.ico" rel="icon" type="image/png"><link href="/css/header.css" rel="stylesheet"><link href="/css/footer.css" rel="stylesheet"><link href="/css/post.css" rel="stylesheet"><script src="/cljs-out/dev-main.js" type="text/javascript"></script></head><header><h1 id="my-name"><a href="/">Rohan Mehta</a></h1><nav><ul id="subpage-container-1"><li class="subpage-container-2"><div class="subpage-container-3"><a class="subpages" href="/about_me">About Me</a></div></li><li class="subpage-container-2"><div class="subpage-container-3"><a class="subpages" href="/projects">Projects</a></div></li><li class="subpage-container-2"><div class="subpage-container-3"><a class="subpages">Writings</a><ul class="sub-subpage-container"><a class="sub-subpage" href="/writings/poetry/index.html#myname">Poetry</a><a class="sub-subpage" href="/writings/blog_posts/index.html#myname">Blog Posts</a></ul></div></li><li class="subpage-container-2"><div class="subpage-container-3"><a class="subpages" href="/readings">Readings</a></div></li></ul></nav><div id="ham-menu"><div class="lines"></div><div class="lines"></div><div class="lines"></div></div><div id="side-nav-background"><div id="side-nav-container-1"><ul class="side-nav-container-2"><a class="side-nav-subpages" href="/about_me">About Me</a></ul><ul class="side-nav-container-2"><a class="side-nav-subpages" href="/projects">Projects</a></ul><ul class="side-nav-container-2"><a class="side-nav-subpages">Writings</a><ul class="side-nav-sub-subpage-container"><a class="side-nav-sub-subpage" href="/writings/poetry/index.html#myname">Poetry</a><a class="side-nav-sub-subpage" href="/writings/blog_posts/index.html#myname">Blog Posts</a></ul></ul><ul class="side-nav-container-2"><a class="side-nav-subpages" href="/readings">Readings</a></ul></div></div></header><div><div id="post-content-container"><h1 id="post-title">The Attention Mechanism Demystified</h1><h4 id="post-byline">1/30/2022 • Rohan Mehta | Deep Learning, NLP</h4><div class="not-poem" id="post-content"><p>The transformer – a neural net architecture proposed in <a class="colored-post-link" href="https://arxiv.org/abs/1706.03762"> Vaswani et al., 2017</a> – has taken the NLP world by storm in the past few years, contributing to the recent success of <a class="colored-post-link" href="https://arxiv.org/abs/2005.14165">OpenAI's GPT-3</a> among many others. But while there is an abundance of online material describing the transformer architecture, its crucial theoretical contribution – the key, query, and value attention mechanism – lacks a clear, accessible explanation, especially from the perspective of what motivated it and why it works. The focus of this post is to hopefully provide some of that intuition, and demonstrate why the attention mechanism is so powerful.</p><h1>Attending To Thy-Self</h1><p>Before we delve further into how the transformer phrases self-attention mathematically, it's worth clarifying what exactly “self-attention” means. Just like a standard layer of neurons, the self-attention operation is some parameterized, differentiable function we can include in our neural nets. Unlike these layers though, it works on sets of elements (not vectors), and learns some function to linearly combine each element with all the others.</p><p>You can think of it a bit like a blender: it blends all other elements into the current element, creating some new hybrid element that contains contextual information about the set as a whole. The intuition  here is that the network is learning to focus some of its “attention” on elements other than the one it is currently processing, hence the operation's name.</p><div style="text-align:center;">
<img src=../../images/attention/self_attention_1.png class=get-bigger style=width:85%;></img>
<figcaption style="text-align:left;">
Fig. 1.  Example of self-attention. The word being processed (red)
learns to attend to other words (blue) and incorporates them
in differing amounts (Source: 
<a class="colored-post-link" href= "https://arxiv.org/pdf/1601.06733.pdf">
Cheng et al., 2016</a>).
</figcaption>
</div><p>And this turns out to be incredibly helpful, especially in situations where context matters (like understanding language). What's more, it offers a distinctive advantage over other, more traditional ways of embedding context, like recurrent neural nets.</p><p>RNNs try and smush the whole context into a single vector. This compression means that some information will inevitably be lost, especially as the text being interpreted gets longer and more complex. Moreover, only a small part of this vector may actually be useful to any given element, and having to block out the noise introduces uneccessary complexity.</p><div style="text-align:center;">
<img src=../../images/attention/rnn_encoder_decoder.png class=get-bigger style=width:100%;></img>
<figcaption style="text-align:left;">
Fig. 2. RNNs have to compress their entire input text
into a single vector; only parts of each word's representation
are conserved (Source: Unknown).
</figcaption>
</div><p>Self-attention, on the other hand, allows any element of the input to directly attend to any other element. As such, each element gets to define its own unique context vector! And since it only has to account for the local  context (unlike in RNNs), this vector can be much more precise about the information it does contain.</p><div style="text-align:center;">
<img src=../../images/attention/self_attention_2.png class=get-bigger style=width:95%;margin-top:-20px;></img>
<figcaption style="text-align:left;">
Fig. 3. Self-attention allows each element to access
the entire representation of any other element, rather than some
compressed version of it (Source: 
<a class="colored-post-link" href="https://arxiv.org/pdf/1706.03762.pdf">
Vaswani. et al., 2017</a>).
</figcaption>
</div><p>But how exactly does a given element decide how much to attend to another element? And how does it use this information to build up its associated context vector? To get to the real meat of the attention mechanism, we need to start phrasing things mathematically.</p><p>Let's imagine we have a matrix \(\boldsymbol{V}\) that contains the vector representations of some input text in its columns. Suppose we also have a vector \(\boldsymbol{\vec{\alpha}}\) which contains the attention coefficients for one of these words (i.e., how much that word wants to attend to every other word).</p><p>$$\boldsymbol{V} =
           \begin{bmatrix}
           \vert & \vert & \hspace{1cm} & \vert \\
           \boldsymbol{\vec{v}_1} & \boldsymbol{\vec{v}_2} & \cdots & \boldsymbol{\vec{v}_n} \\
           \vert & \vert & \hspace{1cm} & \vert \\
           \end{bmatrix} \in \mathbb{R}^{n \times m}
           \hspace{2cm}
           \begin{align}
           \boldsymbol{\vec{\alpha}} &=
           \begin{bmatrix} 
           \alpha_{1} \\
           \alpha_{2} \\
           \vdots \\
           \alpha_{n} \\
           \end{bmatrix} \in \mathbb{R}^{n}
           \end{align}$$</p> <p>When computing the new representation for this word, we want it to incorporate other words such that it more strongly incorporates words with which it has a high attention coefficient, and more weakly incorporates those with which it has low attention coefficient.</p><p>The easiest way to do this is by a weighted sum (of vectors), \(\alpha_1 \boldsymbol{\vec{v}_1} + \alpha_2 \boldsymbol{\vec{v}_2} + \ldots + \alpha_n \boldsymbol{\vec{v}_n}.\) This is equivelant to the matrix-vector product \(\boldsymbol{V}\boldsymbol{\vec{\alpha}}.\) In other words, calculating this matrix-vector product gives us our chosen word's specific context vector.</p><p>However, we still need to find some function that is capable of computing the vector \(\boldsymbol{\vec{\alpha}}\) for each word in the first place. Given the embeddings of two words, it needs to determine how strongly one should attend to the other (and vice versa).</p><p>The simplest binary operation on vectors which gives some sense of how strongly they resonate with one another is the dot product. It  acts as a measure of similarity between two vectors: the closer in space two vectors are, the larger their dot product.</p><p>Intuitively, it makes sense that the more similar two words are, the more they would want to attend to one another. So perhaps we can just define the attention coefficient between two words as the dot product of their embeddings.</p><p><img src=../../images/attention/attention-with-similarity-1.svg class=two-images></img> <img src=../../images/attention/attention-with-similarity-2.svg class=two-images></img></p><figcaption>Fig. 4. If we inform our self-attention algorithm based on vector similarity we
might expect pronouns (e.g., &ldquo;it&rdquo;) to be close to nouns, and
linking verbs (e.g.,  &ldquo;is&rdquo;) to be close to adjectives.</figcaption><p>Unfortunately, this idea has two key flaws that stop it from working outright. To see what I mean, consider the words “hot” and “cold”. For now, we're going to make the simplifcation that word-embeddings only reflect word-meaning. In other words, the representations for “hot” and “cold” should point in opposite directions, since they are antonyms.</p><p>However, it's also likely that both words will want to attend highly to the word “temperature”, as in the phrase “The temperature is hot” or “The temperature is cold”. If both “hot” and “cold” want to attend highly to “temperature”, then their embeddings must be close to its embedding, as that's the only way they will yield a high dot product with one another,</p><p>and thus – as we've defined it – a high attention coefficient.</p><p>But by making the representations of “hot” and “cold” closer the representation of “temperature”, we are necessarily making them closer to one another, and therefore loosing the information that they are antonyms!</p><p><img src=../../images/attention/cold-hot-temp-vectors.svg class=two-images></img> <img src=../../images/attention/cold-hot-vectors.svg class=two-images></img></p><figcaption>Fig. 5. Making the representations of &ldquo;hot&rdquo; and 
&ldquo;cold&rdquo; closer to that of &ldquo;temperature&rdquo; also
causes them to be closer to another (as can be seen on the right). 
This overwrites the information that they are antoyms.</figcaption><p>In other words, it's not possible to encode data about attentional relationships into our word-embeddings without compromising the information about semantic relationships that's already there. Dot production attention requires that we choose between one or the other.</p><p>As if that wasn't bad enough, there's another, arguably even worse problem.  Attention is not a mutually reciprocal action, but similarity is.</p><p>Consider the sentence “He picked up the tennis ball and found it was wet.” We would probably expect  “it” to attend highly to “ball” (what “it” is referring to) while “ball” would probably attend highly to “tennis” and “picked up” (its type and the action performed on it), but much more weakly to  “it” (after all, what new information does it gain from this?).</p><p>However, even though it might be more advantageous for “ball” to attend weakly to “it”, so long as “it” attends strongly to “ball”, “ball” must attend to “it” with equal strength.<sup><a href=#foot1 id=head1 class="colored-post-link">1</a></sup> This is because making “it” closer to “ball” necessarily makes “ball” closer to “it” by an equal amount.</p><p>More generally if a word X attends highly to another word Y, then Y must attend as highly to X by default. Furthermore, since X is similar to Y it is also similar to those words which are also similar to Y (words which Y attends highly to) and by extension those words which are similar to those words which are similar to Y (the words the words similar to Y attend highly to) and so on and so forth. Thus, everything will end up attending to everything else!</p><div style="">
<img src=../../images/attention/reciprocal-attention.svg class=get-bigger style=width:100%;margin-left:5%;></img>
</div><div style="">
<img src=../../images/attention/reciprocal-attention-2.svg class=get-bigger style=width:100%;;margin-left:5%;></img>
<figcaption style="text-align:left;">
Fig. 6. If we use similarity  as an indicator for self-attention,
then attention becomes reciprocal. The original attention (in black)
will be automatically reciprocated (in red).
</figcaption>
</div><p>Evidently, using similarity as a metric for self-attention comes with its problems. But if we could  get it work, a dot product attention mechanism would be awfully nice. And it seems to make a great deal of intuitive sense as well. So how can we resolve its shortcomings?</p><h1>Keys, Queries, and Values</h1><p>The answer, in fact, is deceptively simple: with multiple embedding spaces. Let's consider our first problem again.  There wasn't enough room in a single word-embedding to hold information about both word meaning and attentional relationships.</p><p>The natural thing to do, then, would be to distribute this information throughout multiple representations, as opposed to trying to cram it all into just one.</p><p>So let's create two embedding spaces. In \(\boldsymbol{E_1}\), the vector representation of each word will encode the semantic information associated with that word. It is in this space that “hot” and “cold” will be oppositely positioned. Conversely, in \(\boldsymbol{E_2}\), word-embeddings will hold  attentional information – here, “hot” and “cold” will be close together along with “temperature”. But this is no longer an issue, as we have preserved the information that they are antonyms within \(\boldsymbol{E_1}\)!</p><!--  <p>More mathematically, given \(\boldsymbol{\vec{v}_{cold}},\hspace{0.15cm}\boldsymbol{\vec{v}_{hot}}
\in \boldsymbol{E_1}\)  and \(\boldsymbol{\vec{\alpha}_{cold}},\hspace{0.15cm} \boldsymbol{\vec{\alpha}_{hot}},\hspace{0.15cm}
\boldsymbol{\vec{\alpha}_{temperature}} \in \boldsymbol{E_2}\)
we would expect \(\boldsymbol{\vec{v}_{cold}} \cdot \boldsymbol{\vec{v}_{hot}}\)
to be very negative (&ldquo;hot&rdquo; and &ldquo;cold&rdquo; are antonyms) and
\(\boldsymbol{\vec{\alpha}_{cold}} \cdot \boldsymbol{\vec{\alpha}_{temperature}}\)
and \(\boldsymbol{\vec{\alpha}_{hot}} \cdot \boldsymbol{\vec{\alpha}_{temperature}}\)
to be very positive (&ldquo;hot&rdquo; and &ldquo;cold&rdquo; attend highly
to &ldquo;temperature&rdquo;).</p> --><p><img src=../../images/attention/cold-hot-temp-vectors-vspace.svg class=two-images style="border:2px solid black;"></img> <img src=../../images/attention/cold-hot-vectors-vspace.svg class=two-images style="border:2px solid black;"></img></p><figcaption>Fig. 7. So our visualization from earlier wasn't
necessarily wrong – it would just have to take place in seperate
embedding spaces. \(\boldsymbol{E_1}\) (semantic-space) is on the
left and \(\boldsymbol{E_2}\) (attention-space) is on the right.</figcaption><p>This seems to solve our first problem, but it doesn't do much to help us with our second one. By having “hot” and “cold” attend highly to “temperature”, we're still forcing it to attend highly to them, as well (which it may acutally want to do, but shouldn't have to).</p><p>Luckily for us though, this has a very similar fix: add another embedding space. More specifically, we need to split our second embedding space, \(\boldsymbol{E_2}\), into two seperate spaces.</p><p>Let's call them \(\boldsymbol{E_q}\) and \(\boldsymbol{E_k}.\) We can then define self-attention as follows. When calculating self-attention with respect to “it” (i.e., how much “it” attends to “ball”) we dot its embedding in \(\boldsymbol{E_q}\) with the embedding of “ball” in \(\boldsymbol{E_k}.\) Likewise, dotting the representation of “ball” in \(\boldsymbol{E_q}\) with that of “it” in \(\boldsymbol{E_k}\), will give us  self-attention with respect to “ball”.</p><p>As both operations deal with different embeddings (“it” in \(\boldsymbol{E_q}\) and “ball” in \(\boldsymbol{E_k}\) vs. “ball” in \(\boldsymbol{E_q}\) and “it” in \(\boldsymbol{E_k}\)) attention doesn't reciprocate. But, what is the intuition behind this operation?</p><p>Essentially, what we're doing is computing two representations for each word: one is a “key”, and the other is a “lock”. Computing self-attention with respect to some word X is a matter of seeing how similar other words' lock-representations are to its key-representation.</p><p>But just because a word Y's lock-representation is similar to X's key-representation doesn't imply the opposite – its key-representation may not be similar to X's lock representation!</p><p><img src=../../images/attention/it-ball-1.svg class=two-images style="border:2px solid black;"></img> <img src=../../images/attention/it-ball-2.svg class=two-images style="border:2px solid black;"></img></p><figcaption>Fig. 8. \(\boldsymbol{E_q}\) is on the right and \(\boldsymbol{E_k}\)
is on the left. Notice that the representation of &ldquo;it&rdquo; in
\(\boldsymbol{E_q}\) is close to the representation of &ldquo;ball&rdquo; 
in \(\boldsymbol{E_k}\) (&ldquo;it&rdquo; attends highly to &ldquo;ball&rdquo;)
but the representation of &ldquo;ball&rdquo; in \(\boldsymbol{E_q}\) is
rather far from the representation of &ldquo;it&rdquo; in \(\boldsymbol{E_k}\)
(&ldquo;ball&rdquo;  attends weakly to &ldquo;it&rdquo;).</figcaption><p>To adopt the transformer's lingo, we refer to these key- and lock-representations as queries (key-representations) and keys (lock-representations), respectively. Put simply, a word's key is the information it chooses to expose about itself (the lock), while its query is the information it looks for in other words, when determining how much to attend to them (the key).</p><p>For example, the query of the word “it” might be something like  “I attend highly to nouns – are you a noun?”, while its key might be “I am a pronoun”. Since the key of “ball” is “I am a noun”, it would strongly satisfy the query of “it”, causing “it” to attend highly to “ball”.</p><p>But crucially, since the key of  “it” doesn't answer the query of “ball” with similar strength (“I attend highly to adjectives – are you an adjective?”), “ball” will attend very weakly to “it”.</p><p>We can visualize this idea even more viscerally by overlaying the embedding spaces containing our keys and queries atop one another. When doing so, we see that the angle formed between between the key of “it” and the query of “ball” is different from the angle formed between the key of “ball” and the query of “it”. Different angles means different dot products means different attention coefficients!</p><div style="text-align:center;">
<img src=../../images/attention/it-ball-3.svg class=get-bigger style="width:75%;"></img>
<figcaption>Fig. 9. An overlay of the embedding spaces \(\boldsymbol{E_q}\) and \(\boldsymbol{E_k}.\)</figcaption>
</div><p>So what's the mathematics behind all of this, then? How do we formalize these ideas? Let's imagine that we are given some sentence of variable length and need to find \(\boldsymbol{\vec{\alpha}}\) for the \(i\)th word in this sentence. Given this word's query \(\boldsymbol{\vec{q}}\) and the matrix \(\boldsymbol{K}\) whose columns are the keys of each word in the input, we can calculate it like so:</p><p>
$$\boldsymbol{\vec{\alpha}} = \boldsymbol{K}^\top\boldsymbol{\vec{q}}$$
<p><p>This is equivelant to dotting \(\boldsymbol{\vec{q}}\) with each entry of \(\boldsymbol{K}\) and storing the results in a vector, which is exactly how we defined key-query attention. But calculating \(\boldsymbol{\vec{\alpha}}\) isn't our true goal. What we really want to compute is \(\boldsymbol{\vec{\gamma}},\) our current word's context vector. And, as we realized earlier, this is just the product of \(\boldsymbol{\vec{\alpha}}\) with the matrix containing the semantic representations of all words.</p><p>We had said that these representations lived in the emebdding space  \(\boldsymbol{E_1}\), but to be  consistent with the transformer's  jargon, we'll rename  it to \(\boldsymbol{E_v}\). Just as  key- and lock-representations became queries and keys, we will refer to a word's semantic representation as its value.</p><p>In this vocabulary, self-attention is a  sum of words' values weighted by their attention coefficients, or the dot product of their key with the current word's query. Thus:</p><p>$$\boldsymbol{\vec{\gamma}} =
        \boldsymbol{V}\boldsymbol{K}^\top\boldsymbol{\vec{q}}
        \hspace{1cm}$$<p><h1>Sprouting Multiple Heads</h1><p>Given all the complexities we encountered, it's surprisingly satisfying to see everything reduce to such a simple formula! But there's one glaring detail that we've glossed over so far. We've been assuming that our initial represention for each word is its semantic representation, or value. So where do its key and query come from?</p><p>Instead of first working with our word's value, we might imagine starting with some more general, abstract representation, that has a broad idea of word meaning, grammar, syntax, etc., but nothing too specific. We can then learn a series of weight matrices to linearly project this representation in three different ways, yielding the word's query, key, and value.</p><p>
$$\boldsymbol{Q} = \boldsymbol{X}\boldsymbol{W_Q} \hspace{1cm}
\boldsymbol{K} = \boldsymbol{X}\boldsymbol{W_K} \hspace{1cm}
\boldsymbol{V} = \boldsymbol{X}\boldsymbol{W_V}
$$
</p><p>The query weight matrix might, for example, pull out the specific parts of this representation that store the types of words our word is usually accompanied by, while the key weight matrix might project it into a space that is more aware of grammar, tense, and part-of-speech. Of course, the attention mechanism is much more complex than this, and still not entirely understood, but rationalizing it in this way helps us understand why it works so well.</p><p>And it does indeed work! This general recipe of keys, queries, and values has been proven to be applicable far beyond NLP, to fields such as computer vision and generative modeling. Admittedly, a lot of things have to go just right for this approach to be successful. For instance, why is a linear projection expressive enough to derive our keys, values, and queries?  We don't yet have extremely satisfactory answers to these types of questions.</p><p>Now that we understand how to obtain our keys, queries, and values, we can re-write our self-attention equation to operate on an entire matrix of word-embeddings. If we also sprinkle in a softmax and dimensionality constant to keep numbers from getting to large, we get:<sup><a href=#foot2 id=head2 class="colored-post-link">2</a></sup></p><p>$$\textrm{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \textrm{softmax}\left( \frac{\boldsymbol{Q} \boldsymbol{K}^T}{\sqrt{d_k}} \right)\boldsymbol{V}^T$$</p><p>The full picture, then, is as follows. The self-attention mechanism is paramaterized by three learned weight matrices, each of which projects our general word representation into a specific space, which themselves each prioritize a different aspect of the word's identity. By multiplying together a word's representations in these three spaces we can produce its new representation, or context vector.</p><div style="text-align:center;">
<img src=../../images/attention/single-head.png class=get-bigger style="width:95%;"></img>
</div>
<figcaption style="margin-top:-25px;">Fig. 10. Visual diagram of the single-head self-attention mechanism  (Source: 
<a class="colored-post-link" href= "https://arxiv.org/pdf/1601.06733.pdf">
Vaswani et al., 2017</a>).</figcaption><p>It's also worth considering how self-attention matches up with the convolution operation.<sup><a href=#foot3 id=head3 class="colored-post-link">3</a></sup> They have some striking similarities. Both are fundamentally ways of embedding context. The key difference between the two is that attention has no limiting receptive field: it can model arbitrarily long-distance interactions, while convolutions operate on a pre-determined scale.</p><p>This makes attention seem like the clear winner. But there is also a less obvious advantage to convolutions that doesn't become clear until we try to express attention as a convolutional kernel itself. For example, the attention kernel for a single word would look like this:</p><p>$$\boldsymbol{A} =
           \begin{bmatrix}
           \alpha_1 & \alpha_2 & \hspace{1cm} & \alpha_n \\
           \vdots & \vdots  & \cdots & \vdots \\
           \alpha_1 & \alpha_2 & \hspace{1cm} & \alpha_n  \\
           \end{bmatrix} \in \mathbb{R}^{n \times m}$$</p> <p>Seen in this way, it is intuitively obvious that an attention kernel is less expressive than an arbitrary convolutional one. Attention is a convolutional kernel where the values are the same in all columns; in particular, they are the attention coefficients of the word in question with respect to the word represented by that column. That's because attention is simply a weighted sum: it is just mixing together what already exists in different proportions.</p><div style="text-align:center;">
<img src=../../images/attention/single-head-viz.png class=get-bigger style="width:95%;text-align:center;"></img>
</div>
<figcaption style="margin-top:-25px;">Fig. 11. Visual diagram of single-head attention performing a weighted sym  (Source: 
<a class="colored-post-link" href= "https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf">
Vaswani et al., 2019</a>).</figcaption><p>Convolutions, on the other hand, have no such limitation. They can transform the input in a much more complex way than a weighted sum. In fact, they actually do so several linear transformations (as opposed to just one: scaling) based on the relative position of each of the elements in the input.</p><div style="text-align:center;">
<img src=../../images/attention/convolutions.png class=get-bigger style="width:95%;;margin-left:-30px;"></img>
</div>
<figcaption style="margin-top:-25px;">Fig. 12. Visual diagram of a convolutional kernel performing several linear transformations over its input (Source: 
<a class="colored-post-link" href= "https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf">
Vaswani et al., 2019</a>).</figcaption><p>This reveals a drawback of our current attention mechanism. While the weighted sum operation it is doing makes intuitive sense, it also limits the types of new representations we can create, confining us to the subspace spanned by the specific words in the input.</p><p>We can think of this problem in another way, too. Even though our idea of creating context vectors for each word was a good first step, we need to go even further. Our key-query attention scheme is overworked: it's trying to consider all of the meanings and relationships of each word with respect to the current word, and then condense this information into a single representation. Once again, we run into the issue of straining our information capacity.</p><p>We might instead imagine using multiple such attention mechanisms, where each attention mechanism focuses on a specific aspect of each word. Instead of relying on one attention mechanism to recognize that a verb should attend highly to both the subject performing it and the direct object it's being done to, we could split up this work between two attention mechanisms. In this way, each attention mechanism can specialize, and develop its own niche.</p><div style="text-align:center;">
<img src=../../images/attention/multi-head-viz.png class=get-bigger style="width:95%;;margin-left:-30px;"></img>
</div>
<figcaption style="margin-top:-25px;">Fig. 12. Visual diagram of conbolutional  performing a weighted sym  (Source: 
<a class="colored-post-link" href= "https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf">
Vaswani et al., 2019</a>).</figcaption><p>Mathematically, we can represent this idea as follows. Instead of just learning one set of three project matrices, we can learn multiple such sets – one for each attention mechanism, which we'll call attention heads. If we plug these into the attention formula we end up with multiple context vectors for each word, one per attention head.</p><p>Now we're left with the task of combining this set of context vectors into a single context vector for each word. This might seem counterintuitive; after all, wasn't the whole point of learning multiple attention heads to avoid having to cram things into a single representation?</p><p>Well, sort of. Really, it was to provide room for the network to extract this information and then whittle it down from there. The same goes for the idea of computing multiple intermediate representations for our word (i.e., key, query, value), for that matter.</p><p>Then multi-head attention is;</p><p>$$\textrm{MultiHead}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \textrm{softmax}\left( \frac{\boldsymbol{Q} \boldsymbol{K}^T}{\sqrt{d_k}} \right)\boldsymbol{V}^T$$</p><p>It's worth noting that the. Many papers of ablating, suggesting that they aren't nearly. That's another thing to be careful of too.</p><div style="text-align:center;">
<img src=../../images/attention/multi-head.png class=get-bigger style="width:35%;"></img>
</div>
<figcaption style="margin-top:-25px;">Fig. 10. Visual diagram of the single-head self-attention mechanism  (Source: 
<a class="colored-post-link" href= "https://arxiv.org/pdf/1601.06733.pdf">
Vaswani et al., 2017</a>).</figcaption><h1>Conclusion</h1><p>So there you have it: an intuitive, visual understanding to the attention mechanism. Really, it's all about grappling with some of the intrinsic properties of vectors, and providing the network with ample opportunity to spread information out across multiple sources before pooling it all together. This design seems simple in hindsight, but within it lies the engineering genius of the transformer architecture.</p><p>But we've really just dipped our toes into this idea. For one, the parallel between attention and convlutions given above was highly simplified, and there is extremely interesting work going right now. Other efforts have tried a model mechanistic, or ciruit-level view of the transformer. Attention has been even more recently linked to ideas such as Hopfield networks.</p><p>That's to say that many of these ideas are still very much in flux. These ideas, and I'd definitely like to explore them further in a future blog post. One thing's for certain though. The attention mechanism and will play an increasingly important role in the future of deep learning.</p><h1>Resources</h1><h1>Footnotes</h1><p><span id=foot1><strong>1.</strong></span> This leads to a certain Newton's Third Law of Attention: each act of attention induces an equal and opposite act of attention. Of course, we don't want this property! <a href=#head1 class=colored-post-link>↩</a></p></div></div><div class="commentbox"></div><script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script><script>commentBox('5711441948573696-proj')</script></div><footer><div id="footer-part-1"><a href="https://github.com/rohan-mehta-1024"><img class="icons" src="/images//github.svg"></a><a href=""><img class="icons" src="/images//twitter.svg"></a><a href=""><img class="icons" src="/images/linkedin.svg"></a><a href=""><img class="icons" src="/images/rss.svg"></a></div><div id="footer-part-2">© 2021 Rohan Mehta. All Rights Reserved. Built with
<a href="https://github.com/magnars/stasis" id="stasis" target="blank_">Stasis</a>.</div></footer></html>