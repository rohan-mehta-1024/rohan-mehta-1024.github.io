goog.provide('personal_website.content.writings.blog_posts.attention_via_the_transformer');
personal_website.content.writings.blog_posts.attention_via_the_transformer.post_preview = "The transformer neural net architecture introduced the modern attention mechanism, and to great success.\n   In this post, we will develop an intution for how it works, examine the\n   deeper ideas at play behind it, and take a look at where it's heading in the future.";
personal_website.content.writings.blog_posts.attention_via_the_transformer.post_content = cljs.core.PersistentVector.fromArray([new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentVector(null, 6, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The transformer \u2014 a neural net architecture proposed in ",personal_website.utils.link("Vsawani et. al., 2017","https://arxiv.org/abs/1706.03762")," \u2013 has taken the NLP world by storm in the past few years, contributing\n        to the recent success of ",personal_website.utils.link("OpenAI\u2019s GPT-3","https://arxiv.org/abs/2005.14165")," model among many others. While there is an abundance of online\n       material (see further reading) describing the transformer architecture though,\n       a crucial idea present in the original paper \u2013 the key, query, and value\n       abstraction \u2013 lacks a clear, accessible explanation online, especially from\n       the perspective of what motivated it and why it works. The focus of this post\n       is to hopefully provide some of that intuition and demonstrate why it is\n       so powerful."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Beyond that, though, I also want to take a look at how\n        deeply embedded the idea of attention seems to be in the field,\n        and its significance beyond the sub-field of natural language processing.\n        Will it overtake convolutions? Is recurrence still useful?\n        Are transformers a sign that inductive biases always fall\n        to a more general architectures with enough compute?\n        An intuitive understanding of the mechanics behind attention\n        will let us ponder these \u2013 and many more \u2013 interesting questions\n        in greater detail, so let's begin!"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Introduction"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Before we delve further into how exactly the transformer phrases\n        self-attention mathematically, it's worth clarifying what exactly\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"self-attention"], null)," means. Just like a standard layer of neurons,\n        the self-attention operation is some parameterized, differentiable function we can\n        include in our neural nets. Unlike these layers though,\n        it works on sets of elements (not vectors), and learns some\n        function to linearly combine each element with all the others."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In other words, it selectively incoporates the ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"information"], null),"\n         of all other elements into the current element, providing the network with some\n         contextual information about the set as a whole. Intuitively, this means that\n         the network is learning to focus some of its attention on elements other than\n         the one it is currently processing, hence the operation's name."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/self_attention_1.png",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"75%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 7, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 1. An example of self-attention. The word being processed (red)\n      learns to attend to other words (blue) and incorporates\n      their ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"information"], null)," in differing amounts (Source: ",personal_website.utils.link("Cheng et al., 2016","https://arxiv.org/pdf/1601.06733.pdf"),")."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In particular, this approach has proven to be especially useful for language\n      tasks, as it allows the mapping of informational dependencies\n      across the entire input, whereas conventional approaches such\n      as RNNs (and their more complex cousins, LSTMs and GRUs) degrade\n      the further spread out those dependencies are."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Self-attention on\n      the other hand allows any element of the input to directly attend to any\n      other element, making it much easier for models to build up more\n      robust representations of the text they are fed, even when that\n      text is especially long or complex."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/rnn_encoder_decoder.jpg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"85%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/self_attention_2.png",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"85%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 2. Top: RNNs have to compress their entire input text\n     into a single vector; only parts of each word's representation\n     are conserved; directly access any other words' entire representation\n     (Source: ",personal_website.utils.link("Vsawni. et al., 2017","https://arxiv.org/pdf/1706.03762.pdf"),")."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"While it has proved incredibly helpful to the NLP field,\n        the concept of self-attention (and more broadly, attention in\n        general) is not exclusive to language-based tasks. That said,\n        the transformer \u2013 and even the idea of attention itself \u2013 was\n        designed with this domain in mind, and so approaching the concept\n        from this perspective will make laying down the necessary intuition\n        much easier. Thus, as we consider how we might implement self-attention\n        ourselves we will first address it in its canonical NLP context.\n        From there, we can extrapolate the concept to other problem domains."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Learnable Representations"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So how might we go about implementing self-attention for some language\n        task? Well I've been usinf the term information somewhat flippantly."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"For us, this equates to words.\n        But whereas the constituent units of other types of data (e.g.,\n        the pixels in an image) have self-explanatory digital representations\n        (the same as those computers use), representing words is not so straightforward."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"A naive implementation might encode each word into a one-hot vector that\n        considers the first thousand-or-so most common words. This would\n        not only be computationally inefficient, but also fail to make\n        full use of all the properties of the underlying embedding space\n        (the vector space used to hold the learned representations of our data)."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"For one, the space would be very sparsely populated, with only its\n        basis vectors holding any real semantic\n        meaning; consequently, all representations would be orthogonal to one\n        another \u2013 which is to say, equally different. This is not a faithful\n        depiction our data though, as some words are definitely more similar\n        than others, and giving our network this information explicitly\n        will help in making sure it recognizes this. But how would we even\n        do something like this?"], null),new cljs.core.PersistentVector(null, 8, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The answer is to let a neural network worry it that for us,\n         by learning embeddings with the properties we want. In particular,\n         we can make the representations of words with similar meanings ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"closer together"], null)," and those with disimilar meanings ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"further apart"], null),", where we use the dot product as a measure\n         of closness. ",personal_website.utils.link("Mikolov et. al., 2013's","https://arxiv.org/pdf/1301.3781.pdf")," seminal word2vec algorithm first demonstrated this idea, and (as you can\n         tell from the date) it\u2019s far from a new one either."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In fact, learnable  embeddings are used for essentially all language\n         tasks nowadays. I bring it up (at the risk of being pedantic) only to\n         explicitly acknowledge the fact that how we represent data is a choice,\n         and to do so such that it exposes valuable information to our network\n         we often must learn these representations themselves. ",personal_website.utils.bold("In other words, we must learn the representations we want\n         our network to learn with.")], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/learned_embeddings.png",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"75%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 3. A lower dimensional visualization of a learned embedding space\n       using PCA. Words with similar meanings, connotations, or that are used\n       conjointly are closer (Source: ",personal_website.utils.link("Tensorflow Embedding Projector","https://projector.tensorflow.org/"),")."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But even though learnable embeddings provide a substantial power-up\n         to our models' performance, they still only consider a single type of\n         information in their predictions: word-meaning. Language however, cannot\n        be neatly described as a one-to-one mapping of meanings to words. Words\n        can have different meanings in different contexts, and some words (e.g.,\n        it, is, the, a, an) don't really have well-defined meanings at all, but\n        are instead only useful in modifying other words."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The re-occuring idea here is that we need some form of contextual\n          information in addition to raw information about the words themselves,\n          thus our use of self-attention. More mathematically-speaking, consider\n          the following, where the columns of matrix \\(\\boldsymbol{V}\\) are\n          the embeddings of each word in some input text (with similar words closer\n          together), and the elements of vector \\(\\boldsymbol{\\vec{\\alpha}}\\)\n          are the attention coefficients for a given word (i.e., how much that word\n          should pay attention to all other words):"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"id","id",-1388402092),"math-1"], null),"$$\\boldsymbol{V} =\n           \\begin{bmatrix}\n           \\vert & \\vert & \\hspace{1cm} & \\vert \\\\\n           \\boldsymbol{\\vec{v}_1} & \\boldsymbol{\\vec{v}_2} & \\cdots & \\boldsymbol{\\vec{v}_n} \\\\\n           \\vert & \\vert & \\hspace{1cm} & \\vert \\\\\n           \\end{bmatrix}\n           \\hspace{2cm}\n           \\begin{align}\n           \\boldsymbol{\\vec{\\alpha}} &=\n           \\begin{bmatrix}\n           \\alpha_{1} \\\\\n           \\alpha_{2} \\\\\n           \\vdots \\\\\n           \\alpha_{n} \\\\\n           \\end{bmatrix}\n           \\end{align}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Then, the self-attention for the word in question becomes nothing more\n         than computing  the matrix-vector product \\(\\boldsymbol{V}\n         \\boldsymbol{\\vec{\\alpha}} \\), which will yield the linear combination\n         of all vector representations with their respective attention coefficients\n         \\((\\alpha_1 \\boldsymbol{\\vec{v}_1} + \\alpha_2 \\boldsymbol{\\vec{v}_2} +\n         \\ldots + \\alpha_n \\boldsymbol{\\vec{v}_n} \\))."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The intuitive interpretation of this operation is that each words'\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"information"], null)," is being scaled by whatever importance the\n        processed word assigns it (see Figs. 1 and 2), before being summed\n        into a single vector that represents the context that word is acting\n        in. This stands out in stark contrast to RNNs, as they only ever\n        compute a single, global context vector and reuse it for each word."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This is all fine and good, but we still need to find some function that\n        is capable of computing the vector \\(\\boldsymbol{\\vec{\\alpha}}\\) for\n        each word in the first place. Obviously this function will need to be\n        learned, and it seems intuitively plausible that we could learn it\n        similarly to how we learned our other embeddings, such that the\n        representations of words that attend highly to one another are close\n        together. Then computing the dot product between the processed word\n        and all other words yield \\(\\boldsymbol{\\vec{\\alpha}} \\) (for that\n        word)."], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/attention-with-similarity-1.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/attention-with-similarity-2.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 7, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 4. If we inform our self-attention algorithm based on closness we\n     might expect pronouns (e.g.,  ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null),") to be  close to nouns, and\n     linking verbs (e.g., ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"is"], null),") to be close to adjectives. These words\n     don't carry much information in and of themselves  and thus will need to attend\n     highly to the words they're referring to."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This is an especially attractive propsect since computing the dot product\n       is computationally trivial (as opposed to having to train a whole neural net).\n       Unfortunately, there are two key flaws with this approach that stop it from\n       working outright."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Firstly, the similarity between our vector representations has\n       already already been optimized such that words with similar\n       meanings have similar representations (and are thus closer).\n       As a result, it may not be possible to encode data about\n       attentional relationships into our representations without\n       compromising the information about semantic relationships\n       that is already there."], null),new cljs.core.PersistentVector(null, 12, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"For instance, consider the case of two antonyms, like the\n       words ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null),". Their learned embeddings\n       would point in essentially opposite directions (they have dissimilar\n       meanings and thus are far apart). However both words may want to\n       attend highly to the word ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"temperature"], null),", as in the sentence\n       ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"The temperature is hot"], null)," or ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"The temperature is cold"], null),", where the relation being learned is that X can describe Y (i.e.,\n       hot and cold paramterize a temperature's possible states)."], null),new cljs.core.PersistentVector(null, 12, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"However due to the commutative nature of similarity we cannot make\n        two different things (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null),") similar to\n        a single thing (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"temperature"], null),") without making them similar\n        to eachother. Yet making the representations of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null)," more similar leads our network to believe they are\n        synonyms! Thus, adding contetxual information to our words'\n        embeddings overwrites what is already there."], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/cold-hot-temp-vectors.svg",new cljs.core.Keyword(null,"id","id",-1388402092),"hot-cold-2",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/cold-hot-vectors.svg",new cljs.core.Keyword(null,"id","id",-1388402092),"hot-cold-1",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 9, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 5. Making the representations of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null)," closer to that of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"temperature"], null)," necessarily makes them closer to\n    eachother because they are converging on a single point (as you can see on\n    the right). However this causes us to loose the information that they are\n    antonyms."], null)], null),new cljs.core.PersistentVector(null, 19, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"padding-bottom","padding-bottom",-1899795591),"10px"], null)], null),"But there's another problem too! Attention is not a mutually reciprocal\n       action, but similarity is. Consider the sentence ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"He\n       picked  up the tennis  ball and found it was wet."], null)," We would probably\n       expect the pronoun ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," to attend highly to the word ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," (what ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," is referring to) while the word ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null),"\n       would probably attend highly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"tennis"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"picked up"], null)," (its type and action performed on it) but weakly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," (after all, what useful information does it get from this?)."], null),new cljs.core.PersistentVector(null, 22, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"However even though it might be more advantageous for ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," not\n        to attend highly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null),", so long as ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," attends highly to\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null),", ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," must attend to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," with equal\n        strength. This is beacuase making ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," closer to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null),", necessarily makes ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," closer to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," by an equal amount."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"margin-bottom","margin-bottom",388334941),"35px"], null)], null),"More generally if a word X highly attends to another word Y, then Y must\n       attend as highly to X by default. Furthermore, since X is similar to Y it\n       is also similar to those words which are also similar to Y (words which\n       Y attends highly to) and by extension those words which are similar to those\n       words which are similar to Y (the words the words similar to Y attend highly to)\n       and so on and so forth. Thus, everything will end up attending to everything else!"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto"], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/reciprocal-attention.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin-left","margin-left",2015598377),"6%"], null)], null)], null)], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/reciprocal-attention-2.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin-left","margin-left",2015598377),"6%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 6. If we use closeness as an indicator for self-attention,\n        then attention becomes reciprocal. The original attention (in black)\n        will be automatically reciprocated (in red) even if this is not\n        neccessarily advantageous. The result is that everything ends up attending to\n        everything else, making the idea of attention useless."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Evidently, using similarity as a metric for self-attention\n        comes with its problems. But being able to use the dot product\n        for this purpose would be awfully nice, and seems to (at least\n        intuitively) make a great deal of sense as well.\n        So how can we overcome these shortcomings?"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Keys, Queries, and Values"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The answer, in fact, is deceptively simple: with multiple embedding\n        spaces. Let's consider our first problem again.  There wasn't enough\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"room"], null)," in a single representation to hold information about\n        word meaning and attentional relationships. The natural thing to do\n        would be to distribute this information throughout multiple\n        representations, as opposed to trying to cram everything into just one."], null),new cljs.core.PersistentVector(null, 12, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So let's create two embedding spaces. In \\(\\boldsymbol{E_1}\\), the\n        vector representation of each word will encode the semantic information\n        associated with that word. It would be in this space that ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and   ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null)," are oppositely positioned. Similarly, in\n        \\(\\boldsymbol{E_2}\\), each words' embedding will encode their attentional\n        information \u2013 here, ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null)," would be close together\n        along with ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"temperature"], null),". But this is no longer a problem as we have\n        preserved the information that they are antonyms within \\(\\boldsymbol{E_1}\\)."], null),new cljs.core.PersistentVector(null, 12, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"More mathematically, given \\(\\boldsymbol{\\vec{v}_{cold}},\n        \\boldsymbol{\\vec{v}_{hot}} \\in \\boldsymbol{E_1}\\) and\n        \\(\\boldsymbol{\\vec{\\alpha}_{cold}}, \\boldsymbol{\\vec{\\alpha}_{hot}},\n        \\boldsymbol{\\vec{\\alpha}_{temperature}} \\in \\boldsymbol{E_2}\\)\n        we would expect \\(\\boldsymbol{\\vec{v}_{cold}} \\cdot \\boldsymbol{\\vec{v}_{hot}}\\)\n        to be negative (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null)," are antonyms)\n        and \\(\\boldsymbol{\\vec{\\alpha}_{cold}} \\cdot \\boldsymbol{\\vec{\\alpha}_{temperature}}\\)\n        and \\(\\boldsymbol{\\vec{\\alpha}_{hot}} \\cdot \\boldsymbol{\\vec{\\alpha}_{temperature}}\\)\n        to be very positive (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and   ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null)," attend highly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"temperature"], null),")."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/cold-hot-temp-vectors-vspace.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/cold-hot-vectors-vspace.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 7. So our visualization from earlier wasn't\n   necessarily wrong \u2013 it would just have to take place in seperate\n   embedding spaces (\\(\\boldsymbol{E_1}\\) is on the\n   left, \\(\\boldsymbol{E_2}\\) is on the right)."], null),new cljs.core.PersistentVector(null, 8, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This seems to solve our first problem, but it doesn't do much\n        to help us with our second one. By having ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"hot"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"cold"], null)," attend highly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"temperature"], null),", we're\n        forcing it to attend highly to them as well. Luckily for us though,\n        this has a very similar fix:\u00A0add another embedding space. More\n        specifically, we need to split our embedding space \\(\\boldsymbol{E_2}\\),\n        into two seperate spaces."], null),new cljs.core.PersistentVector(null, 20, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"To understand why, let's play along. Say we did have two different\n        embedding spaces, \\(\\boldsymbol{E_q}\\) and \\(\\boldsymbol{E_k}\\),\n        and that we define the self-attention operation like so. When\n        calculating self-attention with respect to\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," (i.e., how much ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," attends to other words) we\n        take the dot product of its embedding in \\(\\boldsymbol{E_q}\\) and the embedding\n        of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," in \\(\\boldsymbol{E_k}\\). Likewise, self-attention with respect to\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," is computed by dotting its embedding in\n        \\(\\boldsymbol{E_q}\\) and the embedding of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," in\n        \\(\\boldsymbol{E_k}\\). As both operations deal with different embeddings\n        (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," in \\(\\boldsymbol{E_q}\\) and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," in \\(\\boldsymbol{E_k}\\) vs.\n         ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," in \\(\\boldsymbol{E_q}\\) and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," in \\(\\boldsymbol{E_k}\\)),\n         attention doesn't reciprocate. "], null),new cljs.core.PersistentVector(null, 7, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But what is the intuition behind this operation? Essentially we're\n        computing two representations for each word: one is a\n        ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"key"], null),", and the other is a ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"lock"], null),".\n        Computing self-attention with respect to some word X is a matter of\n        seeing how similar other words' lock-representations are to its key-representation. ",personal_website.utils.bold("But, just because a word Y's lock-representation is similar to\n        X's key-representation doesn't imply the opposite \u2013 its key-representation\n        may not be similar to X's lock representation!")], null),new cljs.core.PersistentVector(null, 6, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"To adopt the transformer's lingo, we can refer\n        to these key- and lock-representations as queries (key-representations)\n        and keys (lock-representations). By using different keys and\n        queries when computing self-attention with respect to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," vs. ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null),", self-attention becomes non-commutative."], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/it-ball-1.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/it-ball-2.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 19, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 8. \\(\\boldsymbol{E_q}\\) is on the right and \\(\\boldsymbol{E_k}\\)\n     is on the left. Notice that the representation of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," in\n     \\(\\boldsymbol{E_q}\\) is close to the representation of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null),"\n     in \\(\\boldsymbol{E_k}\\) (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," attends highly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null),")\n     but the representation of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," in \\(\\boldsymbol{E_q}\\) is\n     rather far from the representation of ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," in \\(\\boldsymbol{E_k}\\)\n     (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," attends weakly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null),")."], null)], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto",new cljs.core.Keyword(null,"margin-top","margin-top",392161226),"25px"], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/it-ball-3.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"70%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 11, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 9. A juxtaposition of the two embedding spaces \\(\\boldsymbol{E_q}\\) and\n      \\(\\boldsymbol{E_k}\\). This makes clear  that ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," attends to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," (small angle) without ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," attending as highly to ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," (large angle)."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So what does this all mean mathematically? Let's imagine we are given some sentence of\n        variable length and need to find \\(\\boldsymbol{\\vec{\\alpha}}\\) for the ith word\n        in this sentence. Given this word's query \\(\\boldsymbol{\\vec{q}}\\) and the matrix\n        \\(\\boldsymbol{K}\\) whose columns are the keys of each word n the input, we can\n        calculate it like so: "], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"id","id",-1388402092),"math-2"], null),"$$\n          \\boldsymbol{K} =\n          \\begin{bmatrix}\n          \\vert & \\vert & \\hspace{1cm} & \\vert \\\\\n          \\boldsymbol{\\vec{k}_1} & \\boldsymbol{\\vec{k}_2} & \\cdots & \\boldsymbol{\\vec{k}_n} \\\\\n          \\vert & \\vert & \\hspace{1cm} & \\vert \\\\\n          \\end{bmatrix}\n          \\hspace{1cm}\n          \\boldsymbol{\\vec{q}} \\in \\boldsymbol{E_{q}}\n          \\hspace{1cm}\n          \\boldsymbol{\\vec{\\alpha}} = \\boldsymbol{K}^\\top\\boldsymbol{\\vec{q}}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This is equivelant to dotting \\(\\boldsymbol{\\vec{q}}\\) with each entry of\n        \\(\\boldsymbol{K}\\) and storing the results in a vector, which is exactly how\n        we defined key-query attention. Furthermore, if we recall, the matrix\n        \\(\\boldsymbol{V}\\) contains the semantic embeddings of each\n        word \u2013 or the values of each word, in the transformer's lingo. Thus we\n        can compute the context vector for the word in question,\n        \\(\\boldsymbol{\\vec{\\gamma}}\\), to be the\n        \\(\\boldsymbol{V}\\boldsymbol{\\vec{\\alpha}}\\), or more generally:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\boldsymbol{\\vec{\\gamma}} =\n        \\boldsymbol{V}\\boldsymbol{K}^\\top\\boldsymbol{\\vec{q}}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This vector \\(\\boldsymbol{\\vec{\\gamma}}\\) can then be fed into our neural\n        net and should theoretically contain sufficient information\n        for our model to exploit the relationships between words, ideally yielding\n        performance far better than when we fashioned our word-embeddings on\n        semantic meaning alone."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Practical Self-Attention"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So, are we done then? Not quite. We have derived the theoretical foundation of\n       the transformer's attention mechanism, yes, but have yet to arrive\n       at its full formula for self-attention or understand how this\n       self-attention operation is composed, which is what gives the\n       architecture its true power."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Well, what is the full formula for self-attention then? Given a set of matrices\n        \\(\\boldsymbol{K}\\), \\(\\boldsymbol{Q}\\), and \\(\\boldsymbol{V}\\) whose\n        columns contain the key, query, and value representations for the words in some given input text,\n        the full formula for the transformer's attention mechanism\n        is given by the function shown below: ",personal_website.utils.make_footnote("1","first-footnote-a","first-footnote-b")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$Attention(\\boldsymbol{K}, \\boldsymbol{Q}, \\boldsymbol{V})  =\n      softmax(\\frac{\\boldsymbol{K}^\\top \\boldsymbol{Q}}{\\sqrt{d_k}})\\boldsymbol{V}^\\top$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Let's break it down. Firstly,\n       instead of computing \\(\\boldsymbol{K}^\\top\\boldsymbol{\\vec{q}}\\)\n       we are computing \\(\\boldsymbol{K}^\\top\\boldsymbol{Q}\\). Wheras the\n       former generates \\(\\boldsymbol{\\vec{\\alpha}}\\)\n       for the word whose query is \\(\\boldsymbol{\\vec{q}}\\) the latter\n       generates \\(\\boldsymbol{\\vec{\\alpha}}\\) for every word in the input, and stores\n       the results in the columns of a matrix. In other words, this is just a batch operation."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309)," But what's with the   \\(\\sqrt{d_k}\\) term? Well, the nature of the dot\n         product is such that higher dimensional vectors tend to have larger\n         dot products. In a space with possibly hundreds of dimensions,\n         this means dot products are going to be on the big side. Thus the transformer\n        incorporates a scaling factor of \\(\\frac{1}{\\sqrt{d_k}}\\) to account for this,\n        where \\(d_k\\) is the dimensionality of the keys' embeddings."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Finally, we normalize the columns of this scaled matrix\n      (which contain every words' \\(\\boldsymbol{\\vec{\\alpha}}\\) vector)\n      via a softmax, so that the attention coefficients  of each word's\n      \\(\\boldsymbol{\\vec{\\alpha}}\\) vector sum to one (this also prevents\n      things from getting too big). Multiplying with\n      \\(\\boldsymbol{V}^\\top\\) then yields a matrix whose rows\n      are the context vectors \\(\\boldsymbol{\\vec{\\gamma}}\\) for each word in the input,\n      the things that we're going to send through our neural net."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/scaled-dot-prod-atten.png",new cljs.core.Keyword(null,"id","id",-1388402092),"scaled-dot",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"30%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 7, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 10. A visual illustration of the the above self-attention\n         formula. The step labeled ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"Masking"], null)," has to do specfically\n         with language-based tasks, and we won't get into it in this post. (Source: ",personal_website.utils.link("Vsawni. et al., 2017","https://arxiv.org/pdf/1706.03762.pdf"),")."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And there you have it: the full formula for the transformer's\n            self-attention mechanism! But thus far we've sort of glossed over how\n            exactly we generate the key, query, and value representations for\n            each word \u2013\u00A0and this is more important than it may seem. We've acknowledged\n            that they're learned, but does that necessarily mean we need a\n            different neural net to compute each representation?"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The answer is no, and thankfully so, as this would just add to our\n           computational load. Indeed, it turns out that we can get away with\n           using only a single neural net, by training it to compute an\n           asbtract representation of each word from which all other\n           representations can be derived."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This abstract representation lives in embedding space \\(\\boldsymbol{E}\\),\n            and we can think of it as having all the necessary information about a word\n            to calculate its key, query, and value without actually being any of the\n            three itself. As such, this representation can be transformed into\n            the key, query, or value for its respective word via a set of learned\n            linear maps, represented by the matrices\n            \\(\\boldsymbol{W^K},\n           \\boldsymbol{W^Q},\\) and \\(\\boldsymbol{W^V}\\)."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),personal_website.utils.bold("In other words, this representation is abstract precisely because it\n           can be concretized into any of the network's three preceptions of a\n           given word!")," \\(\\boldsymbol{K},\n          \\boldsymbol{Q},\\) and \\(\\boldsymbol{V}\\) can be calcuated\n          by transforming the matrix \\(\\boldsymbol{X}\\) whose columns\n          are the abstract representations for every word in some input:"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"margin-bottom","margin-bottom",388334941),"55px"], null)], null),"$$\\boldsymbol{K} = (\\boldsymbol{W^K}\\boldsymbol{X})^\\top \\hspace{2cm}\n                 \\boldsymbol{Q} = (\\boldsymbol{W^Q}\\boldsymbol{X})^\\top\\hspace{2cm}\n                 \\boldsymbol{V} = (\\boldsymbol{W^K}\\boldsymbol{X})^\\top$$"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"width","width",-384071477),"100%",new cljs.core.Keyword(null,"margin","margin",-995903681),"auto",new cljs.core.Keyword(null,"margin-top","margin-top",392161226),"25px"], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/abstract_embedding-2.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"40%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"span","span",1394872991),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"width","width",-384071477),"5%",new cljs.core.Keyword(null,"display","display",242065432),"inline-block"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/abstract_embedding.svg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"40%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 11, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"margin-top","margin-top",392161226),"25px"], null)], null),"Fig. 11. An example of what representations for ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null)," and ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," might\n       look like (",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"it"], null),"  is on the left, ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"ball"], null)," is on the right). A\n       word's abstract representation (top-right of each diagram) is used to compute all the others."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This idea is very powerful, and while we'll expound more on it later,\n          it's worth reiterating. Rather than trying to compute a single,\n          highly-detailed representation of each word, the transformer knowingly\n          computes a general, albeit incomplete representation, and instead learns how\n          to transform this representation into several more specialized\n          ones (e.g., a key, query, or value)."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Multi-Head Self-Attention"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But the transformer takes this one step further! Instead of just\n        learning one key, query, and value for a single word, it learns multiple.\n        Why might we want to do this though? Well, one way to motivate this idea is to\n        compare and contrast the self-attention operation to convolutions."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"If you haven't noticed already these two operations are strikingly\n        similar \u2013 they're both weighted averages of some kind. However while\n        the weights of a convolution remain constant throughout the whole input,\n        weights (attention coefficients) change from element to element in\n        self-attention."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"It would be unwise to assume that convolutions, as a rule,\n        are always less powerful then self-attention, though\n        (at least in the form we've defined it so far). In fact, there\n        is a desirable property of the convolution operator\n        that we would like to extend self-attention operation to be applied\n        based on relative position . "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Self-attention is just a linear combination, or in other words\n        a weighted average. It would be like a convolution filter in\n        which all columns are only one number. It fundamentally limits\n        the ability to compress informagio  In other words we can think\n        of it as limting what our context vector could be because we can only\n        scale and add the original in our\n        sentence thet limit what we can acheive our span is limited ye s\n        "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But a self-attention alone isn't as powerful as\n        After all, self attention is just a linear combination\n        of its\n\n"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But unlike self attention\n        There is no rotating of vectors only scaling"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/conv-vs-atten.png",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"80%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Because its weights are global, a doesn't only convovle one filter\n        around . These give it representations containing specific pieces of\n        information. This information can then be "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In the transformer's lingo, we call these attentional units heads, where each\n        head is associated with different \\(\\boldsymbol{W^K},\n       \\boldsymbol{W^Q},\\) and \\(\\boldsymbol{W^V}\\) matrices. It then\n       computes the matrix of context vectors like so:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\textrm{head}_\\textrm{i} = Attention\\left((\\boldsymbol{W^K_i}\\boldsymbol{X})^\\top,\n        (\\boldsymbol{W^Q_i}\\boldsymbol{X})^\\top, (\\boldsymbol{W^V_i}\\boldsymbol{X})^\\top\\right)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In other words, each attention head learns a different set of linear maps\n        that transforms the same abstract representation into a different key,\n        query, and value. This means that given \\(n\\) attention heads\n        we wind up with \\(n\\) matrices containing a different \\(\\boldsymbol{\\vec{\\gamma}}\\)\n        for each word. The transformer aggregates\n        this information by concatenating these matrices together and then multiplying\n        by some other learned matrix \\(\\boldsymbol{W^O}\\), such\n        that each word ends up with only a single context vector:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\textrm{Concat}(\\textrm{head}_1, \\ldots, \\textrm{head}_n) \\boldsymbol{W^O}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This final matrix should then hold the informtion of all the indivdual attention heads"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So the operation itself is simple enough \u2013 but why are we doing it to begin with? The\n        transformer paper's answer's are somewhat cryptic here, but we will do\n        our bst to decipher them. The first reason is that f reduced effective resolution due\n          to averaging attention-weighted positions In other words compared to RNNs which\n          use a non-linearity here\n          also\n\n          Multi-head attention allows the model to jointly attend to information from different representation\n          subspaces at different positions. With a single attention head, averaging inhibits this."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"What Does It All Mean?"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Alternative "], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Multi-Representation Learning"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"An Ode To The Transformer"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"noitatumerP Invariant?"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"But...RNNs Aren't Dead"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"References"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header",new cljs.core.Keyword(null,"id","id",-1388402092),"further-reading"], null),"Further Reading"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Footnotes"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"1. Technically this isn't the exact formula given in the actual transformer paper,\n        but is equivelant. That's because the paper regards words' representations as being\n        stored in the rows of the \\(\\boldsymbol{K}\\), \\(\\boldsymbol{Q}\\), and \\(\\boldsymbol{V}\\)\n        matrices where as in this post we regard them as being stored in their columns.",personal_website.utils.make_footnote("\u21A9","first-footnote-b","first-footnote-a")], null)], true);
personal_website.content.writings.blog_posts.attention_via_the_transformer.media_query_1 = (function personal_website$content$writings$blog_posts$attention_via_the_transformer$media_query_1(){
return garden.stylesheet.at_media.cljs$core$IFn$_invoke$arity$variadic(new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"max-width","max-width",-1939924051),"600px"], null),cljs.core.prim_seq.cljs$core$IFn$_invoke$arity$2([new cljs.core.PersistentVector(null, 1, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"#scaled-dot","#scaled-dot",1513997201),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"70%"], null)], null)], null)], 0));
});
personal_website.content.writings.blog_posts.attention_via_the_transformer.post = cljs.core.PersistentHashMap.fromArrays([new cljs.core.Keyword(null,"tags","tags",1771418977),new cljs.core.Keyword(null,"overarching","overarching",696949346),new cljs.core.Keyword(null,"date","date",-1463434462),new cljs.core.Keyword(null,"content","content",15833224),new cljs.core.Keyword(null,"css","css",1135045163),new cljs.core.Keyword(null,"type","type",1174270348),new cljs.core.Keyword(null,"title","title",636505583),new cljs.core.Keyword(null,"id","id",-1388402092),new cljs.core.Keyword(null,"show","show",-576705889)],[new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, ["mathematics","differentiable programming"], null),"writing","1/12/2021",personal_website.content.writings.blog_posts.attention_via_the_transformer.post_content,personal_website.content.writings.blog_posts.attention_via_the_transformer.media_query_1(),"blog-post","Intuitive Attention Via The Transformer","0",personal_website.content.writings.blog_posts.attention_via_the_transformer.post_preview]);

//# sourceMappingURL=personal_website.content.writings.blog_posts.attention_via_the_transformer.js.map
