{"version":3,"sources":["personal_website/content/writings/blog_posts/attention_via_the_transformer.cljs"],"mappings":";AAIA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGD,AAAA,AAAA,AAACC,AAEY,AAAA,AAAA,AAACA,AAiCsB,AAAA,AAAA,AAACA,AAkBnC,AAAA,AAAA,AAACA,AA4BsB,AAAA,AAAA,AAACA,AAgBX,AAAA,AAAA,AAACA;AAyBpB,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIH,AACGC","names":["personal-website.content.writings.blog-posts.attention-via-the-transformer/post-preview","personal-website.content.writings.blog-posts.attention-via-the-transformer/post-content","personal-website.utils/link","personal-website.content.writings.blog-posts.attention-via-the-transformer/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.attention-via-the-transformer\n  (:require [personal-website.utils :as utils]))\n\n\n(def post-preview\n  \"The transformer \u2013 a neural network model desiged to replace traditional\n  seq2seq architecture \u2013 introduces a powerful abstraction to think about\n  attention.\")\n\n(def post-content\n  [:div\n   [:p \"The transformer \u2014 a neural network model proposed in \"\n    (utils/link \"Vsawani et. al., 2017\" \"https://arxiv.org/pdf/1706.03762.pdf\")\n    \" \u2013 has taken the NLP world by storm in the past few years, contributing to the recent\n    success of \" (utils/link \"OpenAI\u2019s GPT-3\" \"https://arxiv.org/pdf/2005.14165.pdf\")\n    \" model among many others. However while there is an abundance of online material\n    (see further reading) describing the transformer architecture, a crucial idea\n    present in the original paper \u2013 the key, query, and value abstraction \u2013 lacks a clear accessible\n    explanation online, especially from the perspective of what motivated it and why it works.\n    The focus of this post is to hopefully provide some of that intuition\n    and demonstrate why it is so powerful.\"]\n\n\n   [:p \"On that note, the transformer\u2019s attention abstraction is an incredibly helpful tool with which to think\n   about the concept of self-attention, and even more broadly representation learning in general.\n   Specifically, it operates on the principle that computing multiple representations of a single\n   entity which each highlight particular characteristics of that entity is more feasible than\n   trying to compute a single representation which is able to highlight those same characteristics\n   with similarly granular detail and still be manageably produced and operated over by a neural net.\n   That is a veritable mouthful, but in other words: \" [:span {:style {:font-style \"italic\" :font-family \"WorkSansBold\"}}\n   \"many [smaller] representations are better than [a larger] one.\"]]\n\n   [:h1 {:class \"post-section-header\"} \"Introduction\"]\n\n   [:p \"Before we delve further into how exactly the transformer phrases self-attention\n        mathematically though, it's worth clarifying what exactly \u201cself-attention\u201d means.\n        In a single sentence, it is the ability for the elements of an input to selectively\n        incorporate information from other elements of that same input when being processed by a\n        neural net, where the degree to which they incorporate information from those other\n        elements is determined by a learnable function.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/self_attention_1.png\" :class \"post-img\" :style {:width \"75%\"}}]]\n      [:figcaption {:class \"post-caption\"}\n      \"Fig. 1. An example of self-attention. The word being processed (in red)\n      learns to attend to other words in the sentence (in blue) and incorporate their \" [:q \"information\"]\n      \" in differing amounts (Source: \" (utils/link \"Cheng et al., 2016\" \"https://arxiv.org/pdf/1601.06733.pdf\") \").\"]]\n\n  [:p \"This approach has proven to be especially useful in language tasks \u2013 although it is\n       in no way limited to the NLP field \u2013 as it allows the mapping of informational\n       dependencies across the entire input, whereas other approaches such as RNNs degrade the\n       farther away elements of the input are. By allowing individual elements of the input to \u201cattend\u201d\n       to any other element implicit context can be shared throughout the entire input, allowing networks\n       to build up more robust semantic representations of the text they are fed, even when\n       that text is especially long or complex.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/rnn_encoder_decoder.jpg\" :class \"post-img\" :style {:width \"85%\"}}]\n      [:img {:src \"/self_attention_2.png\" :class \"post-img\" :style {:width \"85%\"}}]]\n    [:figcaption {:class \"post-caption\"}\n     \"Fig. 2. Top: RNNs have to compress their entire input text into a single vector; only parts of\n      each word's representation are conserved. Bottom: In contrast, self-attention allows words to\n      directly access any other words' entire representation (Source: \"\n      (utils/link \"Vsawni. et al., 2017\" \"https://arxiv.org/pdf/1706.03762.pdf\") \").\"]]\n\n\n   [:p \"Again, while the concept of self-attention is not exclusive to language-based tasks,\n        the transformer \u2013 and even the idea of attention itself \u2013 was designed with this\n        domain in mind, and so approaching the concept from this perspective will make laying down\n        the necessary intuition much easier. From there, we can extrapolate the concept to\n        other problem domains.\"]\n\n   [:h1 {:class \"post-section-header\"} \"Learnable Representations\" ]\n\n   [:p \"With this in mind, let\u2019s consider how we might program self-attention into\n        a neural net for some language task. A key part of the attentional paradigm is\n        being able to selectively incorporate information from other sources  \u2013 in the case\n        of self-attention specifically, from other elements of the input. In our example,\n        this equates to other words. But how exactly do we represent this information?\"]\n\n   [:p \"A naive implementation might encode each word into a one-hot vector that considers\n        the first thousand-or-so most common words. This would (obviously) not only be computationally\n        inefficient, but fail to make full use of all the properties of the embedding space. For one,\n        the space would be very sparsely populated, with only the \" [:q \"pure\" ] \" unit vectors of\n        the space holding any real semantic meaning; as a consequence all representations would be\n        orthogonal to one another \u2013 in other words, equally different. This is not a faithful depiction\n        of the data being represented though, as some words are definitely more similar than others, and\n        giving the networks this information explicitly will help in making sure that their computed\n        representations are similar too. But how exactly do we make words with similar meanings\n        \u201ccloser\u201d and vice versa?\"]\n\n    [:p \"This is a problem \" (utils/link \"Mikolov et. al., 2013's\" \"https://arxiv.org/pdf/1301.3781.pdf\")\n        \" famous word2vec algorithm solved with learnable embeddings (in other words, using neural\n          networks to produce representations with the properties we want), and it\u2019s far from a new\n          idea either. In fact, learnable embeddings are used for essentially all language tasks nowadays.\n          I retell it (at the risk of being pedantic) only to explicitly acknowledge the idea that how we\n          represent data is a choice, and that to represent it such that exposes valuable information to our\n          network we often must learn the representations in question. In other words, we must learn the\n          representations we want to learn with.\"]\n\n    [:figure {:class \"img-container\"}\n     [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/learned_embeddings.png\" :class \"post-img\" :style {:width \"75%\"}}]]\n\n     [:figcaption {:class \"post-caption\"}\n      \"Fig. 3. A lower dimensional visualization of a learned embedding space using PCA. Words\n      with similar meanings, connotations, or that are used conjointly are \" [:q \"closer\"]\n      \" (Source: \" (utils/link \"Tensorflow Embedding Projector\" \"https://projector.tensorflow.org/\") \").\"]]\n\n\n    [:p \"While the use of learnable embeddings provides a substantial power-up to our models',\n        only a single type of information is being considered in their prediction:\n        word-meaning. The meaning of a word alone however is not enough to understand language.\n        Words can have different meanings in different contexts, and some words (it, is, the, a, an)\n        don't really have well defined meanings at all, but are instead only useful depending on how\n        they are used respective to other words. The re-occuring theme here is that we need some\n        form of contextual information in addition to raw information about the words themselves,\n        and this exactly what self-attention provides us with. To concretize this a bit more mathematically\n        consider the following: \n         When \\\\(a \\\\ne 0\\\\), there are two solutions to\n        \\\\(ax^2 + bx + c = 0\\\\) and they are $$x = {-b \\\\pm \\\\sqrt{b^2-4ac} \\\\over 2a}.$$\"]\n;[:p \"In most cases, this would be enough. We\u2019ve computed representations which give the network a sense of how words are related and provided it with an embedding space powerful enough to encode new words as it encounters them. But we also want a way to calculate how much attention any given word should focus on another, preferably by the same metric (closness) as it is relatively cheap and easy to compute (via the dot product).  The only problem is that we\u2019ve already optimized the similarity between two vectors under the perspective of the similarity between the meanings of the words they represent. Furthermore, just because\"]\n\n\n[:h1 {:class \"post-section-header\"} \"Keys, Queries, and Values\"]\n[:h1 {:class \"post-section-header\"} \"Self-Attention Beyond NLP\"]\n[:h1 {:class \"post-section-header\"} \"Multi-Representation Learning\"]\n\n\n[:h1 {:class \"post-section-header\"} \"References\"]\n[:h1 {:class \"post-section-header\"} \"Further Reading\"]])\n\n(def post\n  {:title \"Attention Via The Transformer\"\n   :date \"2020/08/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :overarching \"writing\"\n   :id \"0\"})\n"]}