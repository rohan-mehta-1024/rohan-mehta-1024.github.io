{"version":3,"sources":["personal_website/content/writings/blog_posts/attention_via_the_transformer.cljs"],"mappings":";AAIA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGD,AAAA,AAAA,AAACC,AAEY,AAAA,AAAA,AAACA,AAgCsB,AAAA,AAAA,AAACA,AAgBxB,AAAA,AAAA,AAACA,AAqCC,AAAA,AAAA,AAACA;AAUpB,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIH,AACGC","names":["personal-website.content.writings.blog-posts.attention-via-the-transformer/post-preview","personal-website.content.writings.blog-posts.attention-via-the-transformer/post-content","personal-website.utils/link","personal-website.content.writings.blog-posts.attention-via-the-transformer/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.attention-via-the-transformer\n  (:require [personal-website.utils :as utils]))\n\n\n(def post-preview\n  \"The transformer \u2013 a neural network model desiged to replace traditional\n  seq2seq architecture \u2013 introduces a powerful abstraction to think about\n  attention.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   [:p \"The transformer \u2014 a neural network model proposed in \"\n    (utils/link \"Vsawani et. al., 2017\" \"https://arxiv.org/pdf/1706.03762.pdf\")\n    \" \u2013 has taken the NLP world by storm in the past few years, contributing to the recent\n    success of \" (utils/link \"OpenAI\u2019s GPT-3\" \"https://arxiv.org/pdf/2005.14165.pdf\")\n    \" model among many others. However while there is an abundance of online material\n    (see further reading) describing the transformer architecture, a crucial idea\n    present in the original paper \u2013 the key, query, and value abstraction \u2013 lacks a clear accessible\n    explanation online, especially from the perspective of what motivated it and why it works.\n    The focus of this post is to hopefully provide some of that intuition\n    and demonstrate why it is so powerful.\"]\n\n\n   [:p \"On that note, the transformer\u2019s attention abstraction is an incredibly helpful tool with which to think\n   about the concept of self-attention, and even more broadly representation learning in general.\n   Specifically, it operates on the principle that computing multiple representations of a single\n   entity which each highlight particular characteristics of that entity is more feasible than\n   trying to compute a single representation which is able to highlight those same characteristics\n   with similarly granular detail and still be manageably produced and operated over by a neural net.\n   That is a veritable mouthful, but in other words: \" [:span {:style {:font-style \"italic\" :font-family \"WorkSansBold\"}}\n   \"many [smaller] representations are better than [a larger] one.\"]]\n\n   [:h1 {:class \"post-section-header\"} \"Introduction\"]\n\n   [:p \"Before we delve further into how exactly the transformer phrases self-attention mathematically though,\n        it's worth clarifying what exactly \u201cself-attention\u201d means. In a single sentence, it is\n        the ability for the elements of an input to selectively incorporate information from other elements\n        of that same input when being processed by a neural net, where the degree to which they incorporate information\n        from those other elements is determined by a learnable function.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/self_attention_1.png\" :class \"post-img\" :style {:width \"75%\"}}]]\n      [:figcaption {:class \"post-caption\"}\n      \"Fig. 1. An example of self-attention. The word being processed (in red)\n      learns to attend to other words in the sentence (in blue) and incorporate their \" [:q \"information\"]\n      \" in differing amounts (Source: \" (utils/link \"Cheng et al., 2016\" \"https://arxiv.org/pdf/1601.06733.pdf\") \").\"]]\n\n  [:p \"This approach has proven to be especially useful in language tasks \u2013 although it is in no way\n       limited to the NLP field \u2013 as it allows the mapping of informational dependencies across the\n       entire input, whereas other approaches such as RNNs degrade the farther away elements of the input are.\n       By allowing individual elements of the input to \u201cattend\u201d to any other element implicit context can be shared throughout\n       the entire input, allowing networks to build up more robust semantic representations of the text they are fed, even when\n       that text is especially long or complex.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/rnn_encoder_decoder.jpg\" :class \"post-img\" :style {:width \"85%\"}}]\n      [:img {:src \"/self_attention_2.png\" :class \"post-img\" :style {:width \"85%\"}}]]\n    [:figcaption {:class \"post-caption\"}\n     \"Fig. 2. Top: RNNs have to compress their entire input text into a single vector; only parts of each word's representation are conserved.\n      Bottom: In contrast, self-attention allows words to directly access any other words' entire representation\n      (Source: \" (utils/link \"Vsawni. et al., 2017\" \"https://arxiv.org/pdf/1706.03762.pdf\") \").\"]]\n\n\n   [:p \"Again, while the concept of self-attention is not exclusive to language-based tasks, the transformer\n        \u2013 and even the idea of attention itself \u2013 was designed with this domain in mind, and so approaching the\n        concept from this perspective will make laying down the necessary intuition much easier. From there, we can extrapolate\n        the concept to other problem domains.\"]\n\n   [:h1 {:class \"post-section-header\"} \"I Don't Know Yet\" ]\n\n   [:p \"With this in mind, let\u2019s consider how we might program self-attention into\n        a neural net for some language task. A key part of the attentional paradigm is being able to selectively\n        incorporate information from other sources  \u2013 in the case of self-attention specifically, from other elements of\n        the input. In our example, this equates to other words. But how exactly do we represent this information?\"]\n\n   [:p \"A naive implementation might encode each word into a one-hot vector that considers the first thousand-or-so most\n        common words. This would (obviously) not only be computationally inefficient, but fail to make full use of all\n        the properties of the embedding space. For one, the space would be very sparsely populated, with only\n        the \" [:q \"pure\" ] \" unit vectors of the space holding any real semantic meaning, and as a consequence all\n        representations would be orthogonal to one another \u2013 in other words, equally different. This is not a faithful depiction\n        of the data being represented though, as some words are definitely more similar than others, and giving the networks\n        this information explicitly will help in making their outputs similar too. But how exactly do we make words with similar meanings \u201ccloser\u201d and vice versa?\"]\n\n    [:p \"This is a problem the famous word2vec paper solved with learnable embeddings (in other words, using\n        neural networks to produce representations with the properties we want), and it\u2019s far from a new idea either.\n        In fact, learnable embeddings are used for essentially all language tasks nowadays. I retell it only to explicitly\n        acknowledge the idea that how we represent data is a choice, and that to represent it in such a way that exposes\n        valuable information to our network we sometimes must learn those representations themselves. In other words, we must\n        be able to learn the representations we want to learn with.\"]\n\n    [:figure {:class \"img-container\"}\n     [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/learned_embeddings.png\" :class \"post-img\" :style {:width \"75%\"}}]]\n\n     [:figcaption {:class \"post-caption\"}\n      \"Fig. 3. A lower dimensional visualization of a learned embedding space using tSNE. Words\n      with similar meanings, connotations, or that are used conjointly are \" [:q \"closer\"]\n      \" (Source: \" (utils/link \"Tensorflow Embedding Projector\" \"https://projector.tensorflow.org/\") \").\"]]\n\n\n[:p \"In most cases, this would be enough. We\u2019ve computed representations which give the network a sense of how words are related and provided it with an embedding space powerful enough to encode new words as it encounters them. But we also want a way to calculate how much attention any given word should focus on another, preferably by the same metric (closness) as it is relatively cheap and easy to compute (via the dot product).  The only problem is that we\u2019ve already optimized the similarity between two vectors under the perspective of the similarity between the meanings of the words they represent. Furthermore, just because\"]\n\n[:h1 {:class \"post-section-header\"} \"References\"]\n[:h1 {:class \"post-section-header\"} \"Further Reading\"]\n   ]\n )\n\n(def post\n  {:title \"Attention Via The Transformer\"\n   :date \"2020/08/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :id \"0\"})\n"]}