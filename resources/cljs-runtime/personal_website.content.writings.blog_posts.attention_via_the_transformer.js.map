{"version":3,"sources":["personal_website/content/writings/blog_posts/attention_via_the_transformer.cljs"],"mappings":";AAKA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGG,AAAA,AAAA,AAACC,AAGF,AAAA,AAAA,AAACA,AAqBA,AAAA,AAACC,AAoCH,AAAA,AAAA,AAACD,AAoBS,AAAA,AAAA,AAACA,AA2CO,AAAA,AAAA,AAACA,AAShB,AAAA,AAACC,AAUH,AAAA,AAAA,AAACD,AA6NA,AAAA,AAACC,AAkFuC,AAAA,AAAA,AAAA,AAACC,AAoCxC,AAAA,AAAA,AAACF,AA6BA,AAAA,AAACC,AAgIT,AAAA,AAAA,AAAA,AAACC;AAQH,AAAA,AAAMC;AAAN,AACE,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAACC;;AASH,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIP,AACGC,AAKJ,AAACI","names":["personal-website.content.writings.blog-posts.attention-via-the-transformer/post-preview","personal-website.content.writings.blog-posts.attention-via-the-transformer/post-content","personal-website.utils/link","personal-website.utils/bold","personal-website.utils/make-footnote","personal-website.content.writings.blog-posts.attention-via-the-transformer/media-query-1","garden.stylesheet.at_media","personal-website.content.writings.blog-posts.attention-via-the-transformer/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.attention-via-the-transformer\n  (:require [personal-website.utils :as utils]\n            [garden.stylesheet :refer [at-media]]))\n\n\n(def post-preview\n  \"The transformer NN architecture introduced the modern attention mechanism.\n   In this post we will develop an intution for how it works and the\n   deeper ideas at play behind it.\")\n\n(def post-content\n  [:div\n   [:p \"The transformer \u2014 a neural network architecture proposed in \"\n        (utils/link \"Vsawani et. al., 2017\" \"https://arxiv.org/pdf/1706.03762.pdf\")\n       \" \u2013 has taken the NLP world by storm in the past few years, contributing\n        to the recent success of \"\n       (utils/link \"OpenAI\u2019s GPT-3\" \"https://arxiv.org/pdf/2005.14165.pdf\")\n       \" model among many others. While there is an abundance of online\n       material (see further reading) describing the transformer architecture though,\n       a crucial idea present in the original paper \u2013 the key, query, and value\n       abstraction \u2013 lacks a clear, accessible explanation online, especially from\n       the perspective of what motivated it and why it works. The focus of this post\n       is to hopefully provide some of that intuition and demonstrate why it is\n       so powerful.\"]\n\n\n   [:p \"Before we lay out this idea from the bottom-up however,\n        it's worth taking a few moments to get a bird's-eye-view of\n        the whole concept, even if right now it only gives us a vague\n        sense of what's really going on. On that note, the core\n        idea behind the transformer can be extrapolated\n        even beyond the scope of attention to representation\n        learning itself. Specifically, it operates on the principle\n        that computing multiple representations of a single entity is\n        more feasible than trying to compute only a single representation\n        [of that entity] without comprimsing it's quality. That is\n        a veritable mouthful, but in other words: \"\n        (utils/bold \"many [smaller] representations are better than\n        [a larger] one. \")]\n\n        [:p \"And it is this idea \u2013 more so than that of attention itself \u2013\n         that has led to the explosion of successes in the NLP field so recently.\n         With that in mind, this post also seeks to (ironically enough) bring\n         attention to this somewhat untalked about idea which silently\n         supports the transformer's (and most modern) attention mechanisms\n         from the shadows, and examine how it might be more generally applied to the\n         field of deep learning as whole (my ulterior motive, if you will). With that\n         out of the way though, let's begin!\"]\n\n   [:h1 {:class \"post-section-header\"} \"Introduction\"]\n\n   [:p \"Before we delve further into how exactly the transformer phrases\n        self-attention mathematically, it's worth clarifying what exactly\n        \" [:q \"self-attention\"] \" means. In a single sentence, it is the\n        ability for the elements of an input to selectively incorporate\n        information from other elements of that same input when being\n        processed by a neural net, where the degree to which they\n        incorporate this information is determined by some learnable\n        function.\"]\n\n    [:p \"In a more intuitive sense, it allows the network to \" [:q \"focus\"]\n        \" on certain relevant parts of the input other than the one its\n        currently processing so that it can take into account contextual\n        (although not neccessarily local) information when making a\n        prediction.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/self_attention_1.png\" :class \"post-img\" :style {:width \"75%\"}}]]\n      [:figcaption {:class \"post-caption\"}\n      \"Fig. 1. An example of self-attention. The word being processed (in red)\n      learns to attend to other words in the sentence (in blue) and incorporate\n      their \" [:q \"information\"] \" in differing amounts (Source: \"\n      (utils/link \"Cheng et al., 2016\" \"https://arxiv.org/pdf/1601.06733.pdf\") \").\"]]\n\n  [:p \"This approach has proven to be especially useful for language\n      tasks as it allows the mapping of informational dependencies\n      across the entire input, whereas conventional approaches such\n      as RNNs (and their more complex cousins, LSTMs and GRUs) degrade\n      the further spread out those dependencies are. Self-attention on\n      the other hand allows any element of the input to attend to any\n      other element, making it much easier for models to build up more\n      robust representations of the text they are fed, even when that\n      text is especially long or complex.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/rnn_encoder_decoder.jpg\" :class \"post-img\" :style {:width \"85%\"}}]\n      [:img {:src \"/self_attention_2.png\" :class \"post-img\" :style {:width \"85%\"}}]]\n    [:figcaption {:class \"post-caption\"}\n     \"Fig. 2. Top: RNNs have to compress their entire input text\n     into a single vector; only parts of each word's representation\n     are con directly access any other words' entire representation\n     (Source: \" (utils/link \"Vsawni. et al., 2017\" \"https://arxiv.org/pdf/1706.03762.pdf\") \").\"]]\n\n\n   [:p \"While it has proved incredibly helpful to the NLP field though,\n        the concept of self-attention (and more broadly, attention in\n        general) is not exclusive to language-based tasks. That said,\n        the transformer \u2013 and even the idea of attention itself \u2013 was\n        designed with this domain in mind, and so approaching the concept\n        from this perspective will make laying down the necessary intuition\n        much easier. Thus, as we consider how we might implement self-attention\n        ourselves we will first address it in its canonical NLP context.\n        From there, we can extrapolate the concept to other problem domains.\"]\n\n   [:h1 {:class \"post-section-header\"} \"Learnable Representations\" ]\n\n   [:p \"So how might we go about implementing self-attention for some language\n        task? A key part of the attentional paradigm is being able to selectively\n        incorporate information from other sources \u2013 in the case of self-attention\n        from other elements of the input. For us, this equates to words.\n        But whereas the constituent units of other types of data (e.g.,\n        the pixels in an image) have self-explanatory digital representations\n        (the same as those computers use), how exactly do we represent words?\"]\n\n   [:p \"A naive implementation might encode each word into a one-hot vector that\n        considers the first thousand-or-so most common words. This would\n        (obviously) not only be computationally inefficient, but fail to make\n        full use of all the properties of the underlying embedding space\n        (the vector space used to hold the learned representations of our data).\"]\n\n   [:p \"For one, the space would be very sparsely populated, with only its\n        basis vectors (assuming standard basis) holding any real semantic\n        meaning; consequently, all representations would be orthogonal to one\n        another \u2013 which is to say, equally different. This is not a faithful\n        depiction our data though, as some words are definitely more similar\n        than others, and giving our network this information explicitly\n        will help in making sure it recognizes this. But how would we even\n        do something like this?\"]\n\n    [:p \"The answer is to let a neural network worry it that for us,\n         by learning embeddings with the properties we want. In particular,\n         we can make the representations of words with similar meanings \"\n         [:q \"closer together\"] \" and those with disimilar meanings \"\n         [:q \"further apart\"] \", where we use the dot product as a measure\n         of closness. \" (utils/link \"Mikolov et. al., 2013's\" \"https://arxiv.org/pdf/1301.3781.pdf\")\n         \" seminal word2vec algorithm first demonstrated this idea, and (as you can\n         tell from the date) it\u2019s far from a new one either.\"]\n\n    [:p \"In fact, learnable  embeddings are used for essentially all language\n         tasks nowadays. I bring it up (at the risk of being pedantic) only to\n         explicitly acknowledge the fact that how we represent data is a choice,\n         and to do so such that it exposes valuable information to our network\n         we often must learn these representations themselves. \"\n         (utils/bold \"In other words, we must learn the representations we want\n         our network to learn with.\")]\n\n    [:figure {:class \"img-container\"}\n     [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/learned_embeddings.png\" :class \"post-img\" :style {:width \"75%\"}}]]\n     [:figcaption {:class \"post-caption\"}\n      \"Fig. 3. A lower dimensional visualization of a learned embedding space\n       using PCA. Words with similar meanings, connotations, or that are used\n       conjointly are closer (Source: \"\n       (utils/link \"Tensorflow Embedding Projector\" \"https://projector.tensorflow.org/\") \").\"]]\n\n\n    [:p \"But even though learnable embeddings provide a substantial power-up\n         to our models' performance, they still only consider a single type of\n         information in their predictions: word-meaning. Language however, cannot\n        be neatly described as a one-to-one mapping of meanings to words. Words\n        can have different meanings in different contexts, and some words (e.g.,\n        it, is, the, a, an) don't really have well-defined meanings at all, but\n        are instead only useful in modifying other words.\"]\n\n\n     [:p \"The re-occuring idea here is that we need some form of contextual\n          information in addition to raw information about the words themselves,\n          thus our use of self-attention. More mathematically-speaking, consider\n          the following, where the columns of matrix \\\\(\\\\boldsymbol{V}\\\\) are\n          the embeddings of each word in some input text (with similar words closer\n          together), and the elements of vector \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\)\n          are the attention coefficients for a given word (i.e., how much that word\n          should pay attention to all other words):\"]\n\n    [:p {:id \"math-1\"} \"$$\\\\boldsymbol{V} =\n           \\\\begin{bmatrix}\n           \\\\vert & \\\\vert & \\\\hspace{1cm} & \\\\vert \\\\\\\\\n           \\\\boldsymbol{\\\\vec{v}_1} & \\\\boldsymbol{\\\\vec{v}_2} & \\\\cdots & \\\\boldsymbol{\\\\vec{v}_n} \\\\\\\\\n           \\\\vert & \\\\vert & \\\\hspace{1cm} & \\\\vert \\\\\\\\\n           \\\\end{bmatrix}\n           \\\\hspace{2cm}\n           \\\\begin{align}\n           \\\\boldsymbol{\\\\vec{\\\\alpha}} &=\n           \\\\begin{bmatrix}\n           \\\\alpha_{1} \\\\\\\\\n           \\\\alpha_{2} \\\\\\\\\n           \\\\vdots \\\\\\\\\n           \\\\alpha_{n} \\\\\\\\\n           \\\\end{bmatrix}\n           \\\\end{align}$$\"]\n\n   [:p  \"Then, the self-attention for the word in question becomes nothing more\n         than computing  the matrix-vector product \\\\(\\\\boldsymbol{V}\n         \\\\boldsymbol{\\\\vec{\\\\alpha}} \\\\), which will yield the linear combination\n         of all vector representations with their respective attention coefficients\n         \\\\((\\\\alpha_1 \\\\boldsymbol{\\\\vec{v}_1} + \\\\alpha_2 \\\\boldsymbol{\\\\vec{v}_2} +\n         \\\\ldots + \\\\alpha_n \\\\boldsymbol{\\\\vec{v}_n} \\\\)).\"]\n\n   [:p \"The intuitive interpretation of this operation is that each words'\n        \" [:q \"information\"] \" is being scaled by whatever importance the\n        processed word assigns it (see Figs. 1 and 2), before being summed\n        into a single vector that represents the context that word is acting\n        in. This stands out in stark contrast to RNNs, as they only ever\n        compute a single, global context vector and reuse it for each word.\"]\n\n   [:p \"This is all fine and good, but we still need to find some function that\n        is capable of computing the vector \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\) for\n        each word in the first place. Obviously this function will need to be\n        learned, and it seems intuitively plausible that we could learn it\n        similarly to how we learned our other embeddings, such that the\n        representations of words that attend highly to one another are close\n        together. Then computing the dot product between the processed word\n        and all other words yield \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}} \\\\) (for that\n        word).\"]\n\n  [:figure {:style {:width \"100%\" :margin \"auto\"}}\n   [:img {:src \"/attention-with-similarity-1.svg\" :style {:width \"50%\"}}]\n   [:img {:src \"/attention-with-similarity-2.svg\" :style {:width \"50%\"}}]\n   [:figcaption {:class \"post-caption\"}\n    \"Fig. 4. If we inform our self-attention algorithm based on closness we\n     might expect pronouns (e.g.,  \" [:q \"it\"] \") to be  close to nouns, and\n     linking verbs (e.g., \" [:q \"is\"] \") to be close to adjectives. These words\n     don't carry much information in and of themselves  and thus will need to attend\n     highly to the words they're referring to.\"]]\n\n  [:p \"This is an especially attractive propsect since computing the dot product\n       is computationally trivial (as opposed to having to train a whole neural net).\n       Unfortunately, there are two key flaws with this approach that stop it from\n       working outright.\"]\n\n  [:p \"Firstly, the similarity between our vector representations has\n       already already been optimized such that words with similar\n       meanings have similar representations (and are thus closer).\n       As a result, it may not be possible to encode data about\n       attentional relationships into our representations without\n       compromising the information about semantic relationships\n       that is already there.\"]\n\n  [:p \"For instance, consider the case of two antonyms, like the\n       words \" [:q \"hot\"] \" and \" [:q \"cold\"]\". Their learned embeddings\n       would point in essentially opposite directions (they have dissimilar\n       meanings and thus are far apart). However both words may want to\n       attend highly to the word \" [:q \"temperature\"] \", as in the sentence\n       \" [:q \"The temperature is hot\"] \" or \" [:q \"The temperature is cold\"]\n       \", where the relation being learned is that X can describe Y (i.e.,\n       hot and cold paramterize a temperature's possible states).\"]\n\n\n   [:p \"However due to the commutative nature of similarity we cannot make\n        two different things (\"[:q \"hot\"] \" and \" [:q \"cold\"] \") similar to\n        a single thing (\" [:q \"temperature\"] \") without making them similar\n        to eachother. Yet making the representations of \" [:q \"hot\"] \" and\n        \" [:q \"cold\"] \" more similar leads our network to believe they are\n        synonyms! Thus, adding contetxual information to our words'\n        embeddings overwrites what is already there.\" ]\n\n  [:figure {:style {:width \"100%\" :margin \"auto\"}}\n   [:img {:src \"/cold-hot-temp-vectors.svg\" :id \"hot-cold-2\" :style {:width \"50%\" }}]\n   [:img { :src \"/cold-hot-vectors.svg\" :id \"hot-cold-1\" :style {:width \"50%\" }}]\n   [:figcaption {:class \"post-caption\"}\n    \"Fig. 5. Making the representations of \" [:q \"hot\"] \" and \" [:q \"cold\"]\n    \" closer to that of \" [:q \"temperature\"] \" necessarily makes them closer to\n    eachother because they are converging on a single point (as you can see on\n    the right). However this causes us to loose the information that they are\n    antonyms.\"]]\n\n  [:p {:style {:padding-bottom \"10px\"}}\n      \"But there's another problem too! Attention is not a mutually reciprocal\n       action, but similarity is. Consider the sentence \" [:q \"He\n       picked  up the tennis  ball and found it was wet.\"] \" We would probably\n       expect the pronoun \" [:q \"it\"] \" to attend highly to the word \" [:q \"ball\"]\n       \" (what \" [:q \"it\"] \" is referring to) while the word \" [:q \"ball\"]\"\n       would probably attend highly to \" [:q \"tennis\"] \" and \" [:q \"picked up\"]\n       \" (its type and action performed on it) but weakly to \"[:q \"it\"]\n       \" (after all, what useful information does it get from this?).\"]\n\n   [:p \"However even though it might be more advantageous for \" [:q \"ball\"] \" not\n        to attend highly to \" [:q \"it\"]\", so long as \" [:q \"it\"] \" attends highly to\n        \" [:q \"ball\"] \", \" [:q \"ball\"] \" must attend to \" [:q \"it\"] \" with equal\n        strength. This is beacuase making \" [:q \"it\"] \" closer to \" [:q \"ball\"]\n        \", necessarily makes \" [:q \"ball\"] \" closer to \" [:q \"it\"] \" by an equal amount.\"]\n\n   [:p {:style {:margin-bottom \"35px\"}}\n      \"More generally if a word X highly attends to another word Y, then Y must\n       attend as highly to X by default. Furthermore, since X is similar to Y it\n       is also similar to those words which are also similar to Y (words which\n       Y attends highly to) and by extension those words which are similar to those\n       words which are similar to Y (the words the words similar to Y attend highly to)\n       and so on and so forth. Thus, everything will end up attending to everything else!\"]\n\n    [:figure {:class \"img-container\" :style {:width \"100%\" :margin \"auto\"}}\n     [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/reciprocal-attention.svg\" :style {:width \"100%\" :margin-left \"6%\"}}]  ]]\n\n    [:figure {:class \"img-container\" :style {:width \"100%\" :margin \"auto\"}}\n      [:img {:src \"/reciprocal-attention-2.svg\" :style {:width \"100%\" :margin-left \"6%\"}}]]\n\n      [:figcaption {:class \"post-caption\"}\n       \"Fig. 6. If we use closeness as an indicator for self-attention,\n        then attention becomes reciprocal. The original attention (in black)\n        will be automatically reciprocated (in red) even if this is not\n        neccessarily advantageous. The result is that everything ends up attending to\n        everything else, making the idea of attention useless.\"]\n\n   [:p \"Evidently, using similarity as a metric for self-attention\n        comes with its problems. But being able to use the dot product\n        for this purpose would be awfully nice, and seems to (at least\n        intuitively) make a great deal of sense as well.\n        So how can we overcome these shortcomings?\"]\n\n   [:h1 {:class \"post-section-header\"} \"Keys, Queries, and Values\"]\n\n   [:p \"The answer, in fact, is deceptively simple: with multiple embedding\n        spaces. Let's consider our first problem again.  There wasn't enough\n        \" [:q \"room\"] \" in a single representation to hold information about\n        word meaning and attentional relationships. The natural thing to do\n        would be to distribute this information throughout multiple\n        representations, as opposed to trying to cram everything into just one.\"]\n\n   [:p \"So let's create two embedding spaces. In \\\\(\\\\boldsymbol{E_1}\\\\), the\n        vector representation of each word will encode the semantic information\n        associated with that word. It would be in this space that \" [:q \"hot\"]\n        \" and   \" [:q \"cold\"] \" are oppositely positioned. Similarly, in\n        \\\\(\\\\boldsymbol{E_2}\\\\), each words' embedding will encode their attentional\n        information \u2013 here, \" [:q \"hot\"] \" and \" [:q \"cold\"] \" would be close together\n        along with \" [:q \"temperature\"] \". But this is no longer a problem as we have\n        preserved the information that they are antonyms within \\\\(\\\\boldsymbol{E_1}\\\\).\"]\n\n   [:p \"More mathematically, given \\\\(\\\\boldsymbol{\\\\vec{v}_{cold}},\n        \\\\boldsymbol{\\\\vec{v}_{hot}} \\\\in \\\\boldsymbol{E_1}\\\\) and\n        \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}_{cold}}, \\\\boldsymbol{\\\\vec{\\\\alpha}_{hot}},\n        \\\\boldsymbol{\\\\vec{\\\\alpha}_{temperature}} \\\\in \\\\boldsymbol{E_2}\\\\)\n        we would expect \\\\(\\\\boldsymbol{\\\\vec{v}_{cold}} \\\\cdot \\\\boldsymbol{\\\\vec{v}_{hot}}\\\\)\n        to be negative (\" [:q \"hot\"] \" and \" [:q \"cold\"] \" are antonyms)\n        and \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}_{cold}} \\\\cdot \\\\boldsymbol{\\\\vec{\\\\alpha}_{temperature}}\\\\)\n        and \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}_{hot}} \\\\cdot \\\\boldsymbol{\\\\vec{\\\\alpha}_{temperature}}\\\\)\n        to be very positive (\" [:q \"hot\"] \" and   \" [:q \"cold\"] \" attend highly to \" [:q \"temperature\"] \").\"]\n\n  [:figure {:style {:width \"100%\" :margin \"auto\"}}\n    [:img {:src \"/cold-hot-temp-vectors-vspace.svg\" :style {:width \"50%\"}}]\n    [:img {:src \"/cold-hot-vectors-vspace.svg\" :style {:width \"50%\"}}]]\n  [:figcaption {:class \"post-caption\"}\n   \"Fig. 7. So our visualization from earlier wasn't\n   necessarily wrong \u2013 it would just have to take place in seperate\n   embedding spaces (\\\\(\\\\boldsymbol{E_1}\\\\) is on the\n   left, \\\\(\\\\boldsymbol{E_2}\\\\) is on the right).\"]\n\n   [:p \"This seems to solve our first problem, but it doesn't do much\n        to help us with our second one. By having \" [:q \"hot\"] \" and \"\n        [:q \"cold\"] \" attend highly to \" [:q \"temperature\" ] \", we're\n        forcing it to attend highly to them as well. Luckily for us though,\n        this has a very similar fix:\u00a0add another embedding space. More\n        specifically, we need to split our embedding space \\\\(\\\\boldsymbol{E_2}\\\\),\n        into two seperate spaces.\"]\n\n   [:p \"To understand why, let's play along. Say we did have two different\n        embedding spaces, \\\\(\\\\boldsymbol{E_q}\\\\) and \\\\(\\\\boldsymbol{E_k}\\\\),\n        and that we define the self-attention operation like so. When\n        calculating self-attention with respect to\n        \" [:q \"it\"] \" (i.e., how much \" [:q \"it\"] \" attends to other words) we\n        take the dot product of its embedding in \\\\(\\\\boldsymbol{E_q}\\\\) and the embedding\n        of \" [:q \"ball\"] \" in \\\\(\\\\boldsymbol{E_k}\\\\). Likewise, self-attention with respect to\n        \" [:q \"ball\"] \" is computed by dotting its embedding in\n        \\\\(\\\\boldsymbol{E_q}\\\\) and the embedding of \" [:q \"it\"] \" in\n        \\\\(\\\\boldsymbol{E_k}\\\\). As both operations deal with different embeddings\n        (\" [:q \"it\"] \" in \\\\(\\\\boldsymbol{E_q}\\\\) and \" [:q \"ball\"] \" in \\\\(\\\\boldsymbol{E_k}\\\\) vs.\n         \" [:q \"ball\"] \" in \\\\(\\\\boldsymbol{E_q}\\\\) and \" [:q \"it\"] \" in \\\\(\\\\boldsymbol{E_k}\\\\)),\n         attention doesn't reciprocate. \"]\n\n   [:p \"But what is the intuition behind this operation? Essentially we're\n        computing two representations for each word: one is a\n        \" [:q \"key\"] \", and the other is a \" [:q \"lock\"] \".\n        Computing self-attention with respect to some word X is a matter of\n        seeing how similar other words' lock-representations are to its key-representation. \"\n        (utils/bold \"But, just because a word Y's lock-representation is similar to\n        X's key-representation doesn't imply the opposite \u2013 its key-representation\n        may not be similar to X's lock representation!\")]\n\n   [:p \"To adopt the transformer's lingo, we can refer\n        to these key- and lock-representations as queries (key-representations)\n        and keys (lock-representations). By using different keys and\n        queries when computing self-attention with respect to \" [:q \"it\"]\n        \" vs. \" [:q \"ball\"] \", self-attention becomes non-commutative.\"]\n\n  [:figure {:style {:width \"100%\" :margin \"auto\"}}\n   [:img {:src \"/it-ball-1.svg\" :style {:width \"50%\"}}]\n   [:img {:src \"/it-ball-2.svg\" :style {:width \"50%\"}}]\n   [:figcaption {:class \"post-caption\"}\n    \"Fig. 8. \\\\(\\\\boldsymbol{E_q}\\\\) is on the right and \\\\(\\\\boldsymbol{E_k}\\\\)\n     is on the left. Notice that the representation of \" [:q \"it\"] \" in\n     \\\\(\\\\boldsymbol{E_q}\\\\) is close to the representation of \" [:q \"ball\"] \"\n     in \\\\(\\\\boldsymbol{E_k}\\\\) (\" [:q \"it\"] \" attends highly to \" [:q \"ball\"] \")\n     but the representation of \" [:q \"ball\"] \" in \\\\(\\\\boldsymbol{E_q}\\\\) is\n     rather far from the representation of \" [:q \"it\"] \" in \\\\(\\\\boldsymbol{E_k}\\\\)\n     (\" [:q \"ball\"] \" attends weakly to \" [:q \"it\"]\").\"]]\n\n\n  [:figure {:style {:width \"100%\" :margin \"auto\" :margin-top \"25px\"}}\n   [:div {:style {:text-align \"center\"}}\n     [:img {:src \"/it-ball-3.svg\" :class \"post-img\" :style {:width \"70%\"}}]]\n   [:figcaption {:class \"post-caption\"}\n     \"Fig. 9. A juxtaposition of the two embedding spaces \\\\(\\\\boldsymbol{E_q}\\\\) and\n      \\\\(\\\\boldsymbol{E_k}\\\\). This makes clear  that \" [:q \"it\"] \" attends to \"\n      [:q \"ball\"] \" (small angle) without \" [:q \"ball\"] \" attending as highly to \" [:q \"it\"]\" (large angle).\"]]\n\n\n\n   [:p \"So what does this all mean mathematically? Let's imagine we are given some sentence of\n        variable length and need to find \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\) for the ith word\n        in this sentence. Given this word's query \\\\(\\\\boldsymbol{\\\\vec{q}}\\\\) and the matrix\n        \\\\(\\\\boldsymbol{K}\\\\) whose columns are the keys of each word n the input, we can\n        calculate it like so: \"]\n\n   [:p {:id \"math-2\"} \"$$\n          \\\\boldsymbol{K} =\n          \\\\begin{bmatrix}\n          \\\\vert & \\\\vert & \\\\hspace{1cm} & \\\\vert \\\\\\\\\n          \\\\boldsymbol{\\\\vec{k}_1} & \\\\boldsymbol{\\\\vec{k}_2} & \\\\cdots & \\\\boldsymbol{\\\\vec{k}_n} \\\\\\\\\n          \\\\vert & \\\\vert & \\\\hspace{1cm} & \\\\vert \\\\\\\\\n          \\\\end{bmatrix}\n          \\\\hspace{1cm}\n          \\\\boldsymbol{\\\\vec{q}} \\\\in \\\\boldsymbol{E_{q}}\n          \\\\hspace{1cm}\n          \\\\boldsymbol{\\\\vec{\\\\alpha}} = \\\\boldsymbol{K}^\\\\top\\\\boldsymbol{\\\\vec{q}}$$\"]\n\n   [:p \"This is equivelant to dotting \\\\(\\\\boldsymbol{\\\\vec{q}}\\\\) with each entry of\n        \\\\(\\\\boldsymbol{K}\\\\) and storing the results in a vector, which is exactly how\n        we defined key-query attention. Furthermore, if we recall, the matrix\n        \\\\(\\\\boldsymbol{V}\\\\) contains the semantic embeddings of each\n        word \u2013 or the values of each word, in the transformer's lingo. Thus we\n        can compute the context vector for the word in question,\n        \\\\(\\\\boldsymbol{\\\\vec{\\\\gamma}}\\\\), to be the\n        \\\\(\\\\boldsymbol{V}\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\), or more generally:\"]\n\n   [:p \"$$\\\\boldsymbol{\\\\vec{\\\\gamma}} =\n        \\\\boldsymbol{V}\\\\boldsymbol{K}^\\\\top\\\\boldsymbol{\\\\vec{q}}$$\"]\n\n   [:p \"This vector \\\\(\\\\boldsymbol{\\\\vec{\\\\gamma}}\\\\) can then be fed into our neural\n        net and should theoretically contain sufficient information\n        for our model to exploit the relationships between words, ideally yielding\n        performance far better than when we fashioned our word-embeddings on\n        semantic meaning alone.\"]\n\n\n[:h1 {:class \"post-section-header\"} \"Practical Self-Attention\"]\n\n  [:p \"So, are we done then? Not quite. We have derived the theoretical foundation of\n       the transformer's attention mechanism, yes, but have yet to arrive\n       at its full formula for self-attention or understand how this\n       self-attention operation is composed, which is what gives the\n       architecture its true power.\"]\n\n   [:p \"Well, what is the full formula for self-attention then? Given a set of matrices\n        \\\\(\\\\boldsymbol{K}\\\\), \\\\(\\\\boldsymbol{Q}\\\\), and \\\\(\\\\boldsymbol{V}\\\\) whose\n        columns contain the key, query, and value representations for the words in some given input text,\n        the full formula for the transformer's attention mechanism\n        is given by the function shown below: \" (utils/make-footnote \"1\" \"first-footnote-a\" \"first-footnote-b\")]\n\n   [:p \"$$Attention(\\\\boldsymbol{K}, \\\\boldsymbol{Q}, \\\\boldsymbol{V})  =\n      softmax(\\\\frac{\\\\boldsymbol{K}^\\\\top \\\\boldsymbol{Q}}{\\\\sqrt{d_k}})\\\\boldsymbol{V}^\\\\top$$\"]\n\n   [:p \"Let's break it down. Firstly,\n       instead of computing \\\\(\\\\boldsymbol{K}^\\\\top\\\\boldsymbol{\\\\vec{q}}\\\\)\n       we are computing \\\\(\\\\boldsymbol{K}^\\\\top\\\\boldsymbol{Q}\\\\). Wheras the\n       former generates \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\)\n       for the word whose query is \\\\(\\\\boldsymbol{\\\\vec{q}}\\\\) the latter\n       generates \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\) for every word in the input, and stores\n       the results in the columns of a matrix. In other words, this is just a batch operation.\"]\n\n   [:p \" But what's with the   \\\\(\\\\sqrt{d_k}\\\\) term? Well, the nature of the dot\n         product is such that higher dimensional vectors tend to have larger\n         dot products. In a space with possibly hundreds of dimensions,\n         this means dot products are going to be on the big side. Thus the transformer\n        incorporates a scaling factor of \\\\(\\\\frac{1}{\\\\sqrt{d_k}}\\\\) to account for this,\n        where \\\\(d_k\\\\) is the dimensionality of the keys' embeddings.\"]\n\n   [:p \"Finally, we normalize the columns of this scaled matrix\n      (which contain every words' \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\) vector)\n      via a softmax, so that the attention coefficients  of each word's\n      \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\) vector sum to one (this also prevents\n      things from getting too big). Multiplying with\n      \\\\(\\\\boldsymbol{V}^\\\\top\\\\) then yields a matrix whose rows\n      are the context vectors \\\\(\\\\boldsymbol{\\\\vec{\\\\gamma}}\\\\) for each word in the input,\n      the things that we're going to send through our neural net.\"]\n\n      [:figure {:class \"img-container\"}\n       [:div {:style {:text-align \"center\"}}\n          [:img {:src \"/scaled-dot-prod-atten.png\" :id \"scaled-dot\" :style {:width \"30%\"}}]]\n         [:figcaption {:class \"post-caption\"}\n         \"Fig. 10. A visual illustration of the the above self-attention\n         formula. The step labeled \" [:q \"Masking\"] \" has to do specfically\n         with language-based tasks, and we won't get into it in this post. (Source: \"\n         (utils/link \"Vsawni. et al., 2017\" \"https://arxiv.org/pdf/1706.03762.pdf\") \").\"]]\n\n\n       [:p \"And there you have it: the full formula for the transformer's\n            self-attention mechanism! But thus far we've sort of glossed over how\n            exactly we generate the key, query, and value representations for\n            each word \u2013\u00a0and this is more important than it may seem. We've acknowledged\n            that they're learned, but does that necessarily mean we need a\n            different neural net to compute each representation?\"]\n\n      [:p \"The answer is no, and thankfully so, as this would just add to our\n           computational load. Indeed, it turns out that we can get away with\n           using only a single neural net, by training it to compute an\n           asbtract representation of each word from which all other\n           representations can be derived.\"]\n\n       [:p \"This abstract representation lives in embedding space \\\\(\\\\boldsymbol{E}\\\\),\n            and we can think of it as having all the necessary information about a word\n            to calculate its key, query, and value without actually being any of the\n            three itself. As such, this representation can be transformed into\n            the key, query, or value for its respective word via a set of learned\n            linear maps, represented by the matrices\n            \\\\(\\\\boldsymbol{W^K},\n           \\\\boldsymbol{W^Q},\\\\) and \\\\(\\\\boldsymbol{W^V}\\\\).\"]\n\n      ; [:p \"$$ \\\\boldsymbol{\\\\varphi_k} : \\\\boldsymbol{E} \\\\rightarrow \\\\boldsymbol{E_k}\\\\hspace{2cm}\n      ;         \\\\boldsymbol{\\\\varphi_q} : \\\\boldsymbol{E} \\\\rightarrow \\\\boldsymbol{E_q}\\\\hspace{2cm}\n      ;         \\\\boldsymbol{\\\\varphi_v} : \\\\boldsymbol{E} \\\\rightarrow \\\\boldsymbol{E_v}$$\"]\n\n      [:p (utils/bold \"In other words, this representation is abstract precisely because it\n           can be concretized into any of the network's three preceptions of a\n           given word!\") \" \\\\(\\\\boldsymbol{K},\n          \\\\boldsymbol{Q},\\\\) and \\\\(\\\\boldsymbol{V}\\\\) can be calcuated\n          by transforming the matrix \\\\(\\\\boldsymbol{X}\\\\) whose columns\n          are the abstract representations for every word in some input:\"]\n\n          [:p {:style {:margin-bottom \"55px\"}} \"$$\\\\boldsymbol{K} = (\\\\boldsymbol{W^K}\\\\boldsymbol{X})^\\\\top \\\\hspace{2cm}\n                 \\\\boldsymbol{Q} = (\\\\boldsymbol{W^Q}\\\\boldsymbol{X})^\\\\top\\\\hspace{2cm}\n                 \\\\boldsymbol{V} = (\\\\boldsymbol{W^K}\\\\boldsymbol{X})^\\\\top$$\"]\n\n\n    [:figure {:style {:width \"100%\" :margin \"auto\" :margin-top \"25px\"}}\n     [:div {:style {:text-align \"center\"}}\n           [:img {:src \"/abstract_embedding-2.svg\" :style {:width \"40%\"}}]\n     [:span {:style {:width \"5%\" :display \"inline-block\"}}]\n      [:img {:src \"/abstract_embedding.svg\" :style {:width \"40%\"}}]]\n     [:figcaption {:class \"post-caption\" :style {:margin-top \"25px\"}}\n      \"Fig. 11. An example of what representations for \" [:q \"it\"] \" and \" [:q \"ball\"] \" might\n       look like (\"[:q \"it\"] \"  is on the left, \" [:q \"ball\"] \" is on the right). A\n       word's abstract representation (top-right of each diagram) is used to compute all the others.\"]]\n\n     [:p \"This idea is very powerful, and while we'll expound more on it later,\n          it's worth reiterating. Rather than trying to compute a single,\n          highly-detailed representation of each word, the transformer knowingly\n          computes a general, albeit incomplete representation, and instead learns how\n          to transform this representation into several more specialized\n          ones (e.g., a key, query, or value).\"]\n\n\n   [:h1 {:class \"post-section-header\"} \"Multi-Head Self-Attention\"]\n\n   [:p \"But the transformer takes this one step further! Instead of just\n        learning one key, query, and value for a single word, it learns multiple.\n        Why might we want to do this though? Well, one way to motivate this idea is to\n        compare and contrast the self-attention operation to convolutions.\"]\n\n   [:p \"If you haven't noticed already these two operations are strikingly\n        similar \u2013 they're both weighted averages of some kind. However while\n        the weights of a convolution remain constant throughout the whole input,\n        weights (attention coefficients) change from element to element in\n        self-attention.\"]\n\n   [:p \"It would be unwise to assume that convolutions, as a rule,\n        are always less powerful then self-attention, though\n        (at least in the form we've defined it so far). In fact, there\n        is a desirable property of the convolution operator\n        that we would like to extend self-attention operation to be applied\n        based on relative position . \"]\n\n   [:p \"Self-attention is just a linear combination, or in other words\n        a weighted average. It would be like a convolution filter in\n        which all columns are only one number. It fundamentally limits\n        the ability to compress informagio  In other words we can think\n        of it as limting what our context vector could be because we can only\n        scale and add the original in our\n        sentence thet limit what we can acheive our span is limited ye s\n        \"]\n\n   ;Single-head self-attention can be defined as a convolution\n\n   [:p \"But a self-attention alone isn't as powerful as\n        After all, self attention is just a linear combination\n        of its\n\n\"]\n\n   [:p \"But unlike self attention\n        There is no rotating of vectors only scaling\"]\n\n        ;albeit at the cost of reduced effective resolution due\n        ;to averaging attention-weighted positions\n\n        ;Multi-head attention allows the model to jointly attend to information from different representation\n        ;subspaces at different positions.\n\n   ;ITS ALL ABOUT SPLITTING INFORMATION APART!\n\n    [:img {:src \"/conv-vs-atten.png\" :style {:width \"80%\"}}]\n\n   [:p \"Because its weights are global, a doesn't only convovle one filter\n        around . These give it representations containing specific pieces of\n        information. This information can then be \"]\n\n   [:p \"In the transformer's lingo, we call these attentional units heads, where each\n        head is associated with different \\\\(\\\\boldsymbol{W^K},\n       \\\\boldsymbol{W^Q},\\\\) and \\\\(\\\\boldsymbol{W^V}\\\\) matrices. It then\n       computes the matrix of context vectors like so:\"]\n\n   [:p \"$$\\\\textrm{head}_\\\\textrm{i} = Attention\\\\left((\\\\boldsymbol{W^K_i}\\\\boldsymbol{X})^\\\\top,\n        (\\\\boldsymbol{W^Q_i}\\\\boldsymbol{X})^\\\\top, (\\\\boldsymbol{W^V_i}\\\\boldsymbol{X})^\\\\top\\\\right)$$\"]\n\n   [:p \"In other words, each attention head learns a different set of linear maps\n        that transforms the same abstract representation into a different key,\n        query, and value. This means that given \\\\(n\\\\) attention heads\n        we wind up with \\\\(n\\\\) matrices containing a different \\\\(\\\\boldsymbol{\\\\vec{\\\\gamma}}\\\\)\n        for each word. The transformer aggregates\n        this information by concatenating these matrices together and then multiplying\n        by some other learned matrix \\\\(\\\\boldsymbol{W^O}\\\\), such\n        that each word ends up with only a single context vector:\"]\n\n   [:p \"$$\\\\textrm{Concat}(\\\\textrm{head}_1, \\\\ldots, \\\\textrm{head}_n) \\\\boldsymbol{W^O}$$\"]\n\n   [:p \"This final matrix should then hold the informtion of all the indivdual attention heads\"]\n\n   [:p \"So the operation itself is simple enough \u2013 but why are we doing it to begin with? The\n        transformer paper's answer's are somewhat cryptic here, but we will do\n        our bst to decipher them. The first reason is that f reduced effective resolution due\n          to averaging attention-weighted positions In other words compared to RNNs which\n          use a non-linearity here\n          also\n\n          Multi-head attention allows the model to jointly attend to information from different representation\n          subspaces at different positions. With a single attention head, averaging inhibits this.\"]\n\n[:h1 {:class \"post-section-header\"} \"What Does It All Mean?\"]\n[:h1 {:class \"post-section-header\"} \"Multi-Representation Learning\"]\n[:h1 {:class \"post-section-header\"} \"An Ode To The Transformer\"]\n\n[:h1 {:class \"post-section-header\"} \"References\"]\n[:h1 {:class \"post-section-header\" :id \"further-reading\"} \"Further Reading\"]\n[:h1 {:class \"post-section-header\"} \"Footnotes\"]\n\n[:p \"1. Technically this isn't the exact formula given in the actual transformer paper,\n        but is equivelant. That's because the paper regards words' representations as being\n        stored in the rows of the \\\\(\\\\boldsymbol{K}\\\\), \\\\(\\\\boldsymbol{Q}\\\\), and \\\\(\\\\boldsymbol{V}\\\\)\n        matrices where as in this post we regard them as being stored in their columns.\"\n\n  (utils/make-footnote \"\u21a9\" \"first-footnote-b\" \"first-footnote-a\")]\n\n\n   ])\n\n\n\n\n(defn media-query-1 []\n  (at-media {:max-width \"600px\"}\n      [\n       [:#scaled-dot {:width \"70%\"}]\n       ;[:#math-1 {:font-size \"13.5px\"}]\n       ;[:#math-2 {:font-size \"13.5px\"}]\n\n       ;[:#hot-cold-2 {:width \"200px\"}]\n       ]))\n\n(def post\n  {:title \"Attention Via The Transformer\"\n   :date \"2020/08/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :overarching \"writing\"\n   :id \"0\"\n   :css (media-query-1)})\n"]}