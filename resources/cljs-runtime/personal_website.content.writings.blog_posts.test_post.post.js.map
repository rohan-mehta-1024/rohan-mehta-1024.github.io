{"version":3,"sources":["personal_website/content/writings/blog_posts/test_post/post.cljs"],"mappings":";AAGA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAKC;AASL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIF,AACGC","names":["personal-website.content.writings.blog-posts.test-post.post/post-preview","personal-website.content.writings.blog-posts.test-post.post/post-content","personal-website.content.writings.blog-posts.test-post.post/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.test-post.post)\n\n\n(def post-preview\n  \"Automatic differentiation is a technique for computing derivatives\n   exactly and efficiently, and serves as the foundation of\n   differentiable programming paradigm.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   \"The fundamental idea behind the differentiable programming paradigm\n   is to allow computational systems to optimize their own performance\n   over some given metric. This simple yet powerful idea \u2013 along with\n   machines powerful enough to enable the co - has given rise\n   to the staggering progress the field of deep learning has seen in past\n   two decades\"])\n\n(def post\n  {:title \"An Introduction To Attention Via The Transformer\"\n   :date \"2020/8/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :id \"0\"})\n"]}