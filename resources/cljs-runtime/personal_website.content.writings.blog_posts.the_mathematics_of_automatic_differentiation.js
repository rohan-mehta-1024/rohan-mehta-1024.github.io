goog.provide('personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation');
personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.post_preview = "Automatic differentiation is the numerical computing technique\n   that gave us the backpropagation algorithm, which is how\n   neural nets learn. In this post, we will explore it\n   from both a mathematics\n   and computer science perspective.";
personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.post_content = cljs.core.PersistentVector.fromArray([new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Backpropagation is the cornerstone of the deep learning\n        paradigm \u2014 the idea that learning is nothing more\n        than optimizing some composition of differentiable functions.\n        In fact, it is the very algorithm that allows neural\n        nets to learn! But it is really only one of the many applications\n        of an even broader numerical computing technique known as automatic differentiation."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Its contribution to neural nets is perhaps its most popularized use today,\n        but the idea has a long history, and its story is ongoing. In its modern\n        day incarnation, it draws on a broad swath of mathematical\n        knowledge and is\n        a beautiful amalgamation of many different ideas. While\n        our discussion of the technique will be phrased in terms of its utility to\n        neural nets, the ideas underpinning it are far more applicable,\n        illustrating some deeper truths about calculus, dynamic programming, and the\n        art of problem solving in general."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309)," What follows is a concise review of\n   multivariable calculus, before we delve into some more complex math.\n   If you don't need it, skip ahead. Otherwise, read on!"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Learning To Differentiate"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Derivatives of single-variable functions are\n        measurements of how infinitesimal variations\n        to a function's input-space correspond to the\n        resulting variations in its output-space.\n        And the same is true of  multivariable functions,\n        except now we have many more ways in which to\n        vary our input-space."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"To start, let\u2019s consider a function of two\n        variables. Such a function can be pictured\n        as a surface above the Cartesian plane, where\n        the height of a point on that surface is\n        calculated using the function \\(f(x,y)\\)\n        in question. With this in mind, the geometrical\n        interpretation of the derivative is how\n        the elevation of this surface changes given\n        a small step in some direction."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/multi-fn-2.png",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"45%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/multi-fn-4.png",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"45%"], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"left"], null)], null),"Fig. 1. Examples of surfaces generated by a function of two variables (Source: ",personal_website.utils.link("CalcPlot3D","https://www.monroecc.edu/faculty/paulseeburger/calcnsf/CalcPlot3D/"),")."], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"An important thing to notice here is that\n        this step could be in any direction: the\n        \\(x\\) direction, \\(y\\) direction, or some\n        combination of the two. Say, for instance, we\n        wanted to differentiate the function\n        \\(f(x, y) = x^2 + y^2\\) with respect to \\(x\\).\n        We now know what this means geometrically,\n        but how exactly would we go about calculating it?"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Imagine standing at some point on this\n        function's surface, and walking horizontally\n        in both directions. If we walk such that a spool\n        of yarn unravels behind us, the shape this yarn\n        takes will resemble the graph of the single-variate\n        function \\(f(x) = x^2 + C\\) (a parabola) where\n        \\(C\\) is some constant, namely whatever the\n        \\(y\\)-value of our original point was."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"It's not immediately obvious, but by walking\n        horizontally across our surface we only varied\n        our \\(x\\)-coordinate, while our \\(y\\)-coordinate\n        remained constant. When walking in this way,\n        the surface is described by a special case of the\n        original function where \\(y\\) is a constant."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Similarly, a walk\n           in a purely vertical direction is described\n           when \\(x\\) is a constant. In fact, we would find that\n           all multivariable functions\n           behave in this way when being walked upon\n           purely in the direction of one of their variables,\n           such that all others become constants."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/parabaloid-1.png",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/parabaloid-2.png",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"left"], null)], null),"Fig. 2. The graph the function \\(f(x,y) = x^2 + y^2\\), known as a paraboloid.\n    Slicing it across the \\(x\\)-axis (or walking in the \\(x\\) direction) reveals that \\(x\\)-wise\n    cross-sections are parabolas. (Source: ",personal_website.utils.link("GeoGebra3D","https://www.geogebra.org/3d?lang=en"),")."], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In that vein of thought, we might conjecture\n        that when computing the derivative of our\n        function \\(f(x,y) = x^2 + y^2\\) with\n        respect to \\(x\\), we can imagine we are\n        instead differentiating \\(f(x) = x^2 + C\\),\n        as this is what the surface looks like when\n        moving in the \\(x\\) direction. Then the\n        derivative with respect to \\(x\\) (what\n        we call the partial derivative) would\n        be \\(2x\\), or \\(\\frac{\\partial{f}}{\\partial{x}}\\ = 2x\\)"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And it turns out that this thought is in fact\n        correct!",personal_website.utils.make_footnote("1","first-footnote-a","first-footnote-b")," When taking the derivative of a multivariable\n        function with respect to only a single variable,\n        we can treat all other variables as constants\n        and just apply our single-variable\n        differentiation rules."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But how do we generalize this idea when taking\n        steps across our surface that aren't only in the\n        \\(x\\) direction or only in the \\(y\\) direction,\n        but rather some combination of both? For instance,\n        how would we calculate the derivative for some\n        infinitesimal step along the vector\n        \\(\\langle 1, 1 \\rangle \\)?"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Well, a step in this direction is equivalent\n         to one step in both the \\(x\\) direction and\n         \\(y\\) direction. And since we know how our\n         function changes for both a pure step in the\n         \\(x\\) direction (\\(\\frac{\\partial{f}}{\\partial{x}}\\))\n         and \\(y\\) direction (\\(\\frac{\\partial{f}}{\\partial{y}}\\)),\n         the derivative in this direction is just their sum:\n         \\(\\frac{\\partial{f}}{\\partial{x}} + \\frac{\\partial{f}}{\\partial{y}}\\ = 2x + 2y\\)."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"More generally, given any vector \\(\\boldsymbol{\\vec{v}}\\)\n        we can compute its corresponding derivative\n        \u2013 or directional derivative \u2013 by dotting it with\n        the vector containing our function's partials."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$D_{\\boldsymbol{\\vec{v}}}f(x_1, x_2, \\ldots, x_n) = \\boldsymbol{\\vec{v}} \\cdot\n          \\begin{bmatrix}\n            \\frac{\\partial{f}}{\\partial{x_1}} \\\\\n            \\frac{\\partial{f}}{\\partial{x_2}} \\\\\n            \\vdots \\\\\n            \\frac{\\partial{f}}{\\partial{x_n}} \\\\\n          \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/parabaloid-5.png",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/parabaloid-6.png",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 3. The derivative along the vector \\(\\langle 1, 1 \\rangle\\)\n    is greater than either partial, since it is their sum.\n    This means that the paraboloid's surface is steeper\n    in this direction than in just the horizontal or vertical ones.\n    In fact, it is two times steeper. (Source: ",personal_website.utils.link("GeoGebra3D","https://www.geogebra.org/3d?lang=en"),")."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"We can now\n        calculate all of a function's possible derivatives.\n        A natural question then, is what derivative is the greatest?\n        In what direction does a function increase most rapidly?"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The equation for directional derivatives says that\n        the derivative with respect to some vector is that\n        vector dotted with the vector of our function's\n        partials, so answering this question means finding\n        whatever vector yields the largest possible dot\n        product in this situation. "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And since a vector's dot product is largest when dotted\n        against itself, the vector that maximizes this dot product\n        is itself the vector containing all the partials of our function!\n        So the direction represented by this vector must be\n        the direction of steepest descent \u2013 the greatest\n        derivative of our function."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Intuitively, this makes sense too,\n        as how much this vector points in a given\n        direction is equivalent to the derivative\n        in that direction, such that it points\n        more in steeper directions and less in shallow\n        ones, thus becoming the steepest direction itself.\n        This vector is known as a function's gradient,\n        and is denoted like so:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\nabla f(x_1, x_2, \\ldots, x_n) =\n          \\begin{bmatrix}\n            \\frac{\\partial{f}}{\\partial{x_1}} \\\\\n            \\frac{\\partial{f}}{\\partial{x_2}} \\\\\n            \\vdots \\\\\n            \\frac{\\partial{f}}{\\partial{x_n}} \\\\\n          \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Gradient Descent"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"We've now gone through all the basics necessary\n        to consider how we might optimize neural nets,\n        and the first step lies in understanding how\n        we can model them mathematically."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"As a quick review, we think of neural nets as universal\n       function approximators. Theoretically, if given enough\n       data and made deep enough, they can represent any\n       conceivable function."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And what gives them this ability is their constituent unit,\n       the neuron:  a computational machine with the ability to\n       ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"learn"], null),". Mathematically, a neuron's activation\n       (output) is defined like so, where \\(\\boldsymbol{\\vec{x}}\\)\n       is some input vector, \\(\\boldsymbol{\\vec{w}}\\) and \\(b\\)\n       are the neuron's two learnable parameters (known as a\n       weight and bias respectively) and \\(\\sigma\\) denotes\n       some non-linearity:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\alpha = \\sigma(\\boldsymbol{\\vec{x}} \\cdot \\boldsymbol{\\vec{w}} + b)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"We refer to a neuron's parameters as learnable\n       because they can be updated such that the\n       neuron returns the desired output for a\n       given input. And somewhat counter to intuition,\n       by stacking these simplistic units together, we\n       can get the resulting structure \u2013 known as a neural\n       net \u2013 to exhibit surprisingly intelligent behavior."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"What's more, all of these behaviors can be framed\n       as some sort of prediction task: given lots of labeled\n       data, how do optimize the parameters of all neurons in\n       a given network such that it predicts the correct label\n       for a given input?"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/neural-net-1.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"65%"], null)], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"left"], null)], null),"Fig. 4. Visual diagram of a neuron. It computes a dot product\n     between the incoming values and its weight vector, adds a bias\n     term, and then pumps everything through some non-linearity."], null)], null)], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"All optimization is just a matter of searching\n     the space of possibilities. If we consider only a single\n     neuron, then this space is defined by all\n     \\((\\boldsymbol{\\vec{w}}, b)\\) pairs that the neuron\n     could have. But to search this space intelligently,\n     we need to mathematically define what exactly makes\n     a specific pair ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"good"], null)," \u2013 more concretely,\n     what criterion does a good pair minimize?"], null),new cljs.core.PersistentVector(null, 6, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Good pairs allow the neuron to make predictions that\n     are close to the corresponding label. Said another way,\n     they minimize the margin of error between the prediction\n     and the correct label. If we define an error function",personal_website.utils.make_footnote("2","second-footnote-a","second-footnote-b")," \\(E(\\hat{y}, y)\\)  that takes a prediction \\(\\hat{y}\\)\n     and a label \\(y\\) and computes this margin of error,\n     then a good pair minimizes the following ",personal_website.utils.link("objective function","https://en.wikipedia.org/wiki/Mathematical_optimization"),":\n     $$J(\\boldsymbol{\\vec{x}}, y;  \\boldsymbol{\\theta}) = E(\\alpha, y)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This function takes in some labeled datapoint\n      \\((\\boldsymbol{\\vec{x}}, y)\\) and our parameters\n      \\(\\boldsymbol{\\theta}\\). Using these, it computes\n      our neuron's activation, and passes it and the label\n      \\(y\\) to our error function. It returns low\n      values for good parameters, because good parameters\n      generate good predictions, and good predictions have\n      a low margin of error."], null),new cljs.core.PersistentVector(null, 6, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So optimizing our neuron boils down to finding parameters\n     located in the local minima of this function, or the\n     ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"valleys"], null)," of whatever high-dimensional\n     ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"error-surface"], null)," it encodes."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"For a single neuron, it would be easy enough to obtain\n     these analytically (by solving for critical points),\n     but as we scale things up this approach quickly becomes\n     intractable. We might instead consider the simpler sub-problem\n     of how to get a slightly better parameter pair\n     (instead of an optimal one), and then repeat this\n     process over and over."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"A better pair is just one for whom the function\n     returns a lower value \u2013 so any pair on our error-surface\n     with lower elevation than our current one. We can get\n     to such a pair by taking some step across this surface\n     in the downhill direction. To be most efficient,\n     we would want to move in the direction of steepest\n     descent \u2013 or the negative gradient,\n     \\(-\\nabla_{\\boldsymbol{\\theta}} J\\)!",personal_website.utils.make_footnote("3","third-footnote-a","third-footnote-b")], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/loss_landscape-4.jpg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"65%"], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"left"], null)], null),"Fig. 5. A visualization of the error-surface for a single neuron. Each point\n      is some parameter pair \\((\\boldsymbol{\\vec{w}}, b)\\) and the height of each\n      pair indicates how badly the neuron performs when it uses them.\n      By traveling along the negative gradient\n      we can arrive at good pairs (Source: ",personal_website.utils.link("Amini et. al, 2018","file:///Users/Rohan%20Mehta/Downloads/Spatial_Uncertainty_Sampling_for_End-to-End_Contro%20(1).pdf"),")."], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And it turns out that repeatedly stepping in the\n      direction of the negative gradient \u2013 a process\n      aptly named gradient descent \u2013 does indeed\n      produce a \\((\\boldsymbol{\\vec{w}}, b)\\)\n      pair that minimizes our objective function,\n      and thus maximizes our neuron's performance."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Since the gradient is only valid for infinitesimal\n     steps, though, we need to  make sure the steps we\n     are taking, while not infinitesimal, are appropriately\n     small. Thus we scale down the gradient by some factor\n     \\(\\eta\\),",personal_website.utils.make_footnote("4","fourth-footnote-a","fourth-footnote-b"),"\n     such that the step we actually take is: \\(-\\eta \\nabla_{\\boldsymbol{\\theta}}J\\)."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Mathematically, ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"taking a step"], null)," in the direction\n      of some vector just means adding that vector to our\n      parameter vector \\(\\boldsymbol{\\theta}\\),\n      so the equation for gradient descent looks like this:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\boldsymbol{\\theta}^{new} = \\boldsymbol{\\theta}^{old} - \\eta\\nabla_{\\boldsymbol{\\theta}}J$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And this one-liner is, more or less, how neural nets learn!\n       But we've made one simplification so far. Our current objective\n       function \\(J\\) only takes into account one labeled datapoint\n       \\((\\boldsymbol{\\vec{x}}, y)\\), but being able to correctly\n       predict the label for a single datapoint doesn't guarantee the\n       neuron is learning the underlying mapping from datapoints\n       to labels that we want it to."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"It could, for instance, be learning the constant function that\n        maps every input to the same label. So instead of calculating\n        the error in our neuron's prediction against a single datapoint,\n        we must do so against all datapoints in our dataset, and then\n        take the average. In other words, the function we're\n        really trying to minimize looks more like this:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$J(\\boldsymbol{X}, \\boldsymbol{\\vec{y}}; \\boldsymbol{\\theta}) = \\frac{1}{n}\\sum_{i=1}^{n} E(\\alpha, \\boldsymbol{\\vec{y}}_i)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And since the gradient of a sum equals the sum of gradients,\n        the gradient of this function is the average\n        of the gradients of our previous objective function\n        evaluated across all samples."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In so many words, we're going to have to take a lot of gradients,\n        one for each of our thousands (or millions) of samples,\n        possibly hundreds of times (one per step). So we'll need a way\n        to compute them quickly if this approach is to be at all practical."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Computational Graphs"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Before we focus on computing these gradients, though,\n       we first need to upgrade our current working model\n       of only a single neuron to a fully-fledged neural net."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"As we touched on before, a neural net is just many\n       individual neurons stacked atop one another. More\n       specifically, a given network is a horizontal stack\n       of many ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"layers"], null),", which are themselves vertical\n       stacks of neurons. Computation happens as activations are\n       propagated between adjacent layers, such that each neuron\n       takes in all activations of the previous layer."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In other words, each entry in a given neuron's weight\n       vector is multiplied with the activation of a neuron\n       in the previous layer. So we can visualize each of these\n       entries as a weighted edge connecting the current neuron to\n       a neuron in the previous layer."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"After computing the weighted sum these edges encode,\n       adding a bias, and passing the whole thing through a\n       non-linearity, we obtain a neuron's activation, which\n       is propagated forward and becomes a part of the weighted\n       sum computed by neurons in the next layer. "], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/neural-net-3.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"70%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 6. An example of a neural network that maps 2D vectors to scalars. Each\n        neuron is connected to every neuron in the previous layer by some weighted edge\n        determined by its weight vector."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This view of the neuron, where we decompose each\n       neuron's weight vector into a set of weighted edges,\n       is admittedly slightly more messy and less elegant\n       than our previous model. But as we don't know how\n       to take the derivative with respect to an entire vector\n       (yet), it is a necessary evil if we want to be able to\n       optimize our parameters by gradient descent."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So how do we represent all this mathematically?\n        Well, we can index each neuron by its layer and\n        position in that layer. Each of a neuron's weights can\n        then be indexed with one additional term, representing\n        the position of the neuron it connects to in the previous\n        layer. Thus the activation of the \\(i\\)th neuron in\n        the \\(L\\)th layer, given \\(j\\) neurons in the previous layer, is:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\alpha^{(L)}_i = \\sigma \\left( \\sum_{n=0}^{j} \\alpha^{(L - 1)}_n \\cdot w^{(L)}_{(i, \\hspace{0.1cm} n)} + b^{(L)}_i \\right)$$"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"margin-bottom","margin-bottom",388334941),"30px"], null)], null),"Writing things out in this way makes it exceedingly\n    clear that a neuron's activation is defined recursively,\n    since it appears in its own definition. The mathematical\n    consequence of this recursiveness is that the expression\n    representing a network's output is incredibly nested, so that\n    even a trivial net like the one above is actually surprisingly\n    complex, once expanded out."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 4, [new cljs.core.Keyword(null,"font-size","font-size",-1847940346),"15px",new cljs.core.Keyword(null,"margin-bottom","margin-bottom",388334941),"30px",new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null),new cljs.core.Keyword(null,"id","id",-1388402092),"special"], null),"\n   $$\\sigma \\left(\\bbox[yellow]{\\alpha^{(1)}_1} \\cdot w^{(2)}_{(1, \\hspace{0.1cm} 1)}+ \\bbox[orange]{\\alpha^{(1)}_2} \\cdot w^{(2)}_{(1, \\hspace{0.1cm} 2)} + b^{(2)}_1\\right)$$\n   $$\\sigma \\left(\\bbox[yellow]{\\sigma\\left(x^{(0)}_1 \\cdot w^{(1)}_{(1, \\hspace{0.1cm} 1)} + x^{(0)}_2 \\cdot w^{(1)}_{(1, \\hspace{0.1cm} 2)} + b^{(1)}_ 1\\right)} \\cdot w^{(2)}_{(1, \\hspace{0.1cm} 1)} + \\bbox[orange]{\\sigma\\left(x^{(0)}_1 \\cdot w^{(1)}_{(1, \\hspace{0.1cm} 1)} + \\alpha^{(0)}_2 \\cdot w^{(1)}_{(1, \\hspace{0.1cm} 2)} + b^{(1)}_2 \\right) \\cdot w^{(2)}_{(1, \\hspace{0.1cm} 1)}} + b^{(2)}_1 \\right)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And, unfortunately for us, it is this bulky expression\n       we're going to have to pass into our error function, because\n       it represents the network's prediction. Worse, we're going have\n       to differentiate it with respect to each of our weights and biases\n       to calculate the gradient."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But this expression is so convoluted, it's unclear where we would\n       even start. Luckily, we can simplify things with a helpful model\n       known as a computational graph, where the nodes of this graph\n       represent some fundamental operations (e.g., addition, multiplication)\n       and its edges represent the values flowing into these operations."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"margin-bottom","margin-bottom",388334941),"30px"], null)], null),"We think of each node as performing its operation\n     on the values it receives, and then spitting out some\n     output. What's more, by storing each node's output\n     in some associated variable we can track all intermediate\n     states an expression goes through before arriving\n     at its final state \u2013 the last node in our graph \u2013\n     and this will turn out to be incredibly helpful. "], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"90%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Let's imagine constructing such a graph to represent\n       the evaluation of our error function at this three-layer\n       net's prediction for some labeled datapoint \\((\\boldsymbol{\\vec{x}}, y)\\).\n       Calculating the gradient means finding the derivatives\n       of the final node with respect to each of our weights\n       and biases. To make things easier, we'll abstract away\n       each node with some generic function."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"With that in mind, let's examine the following segment\n       of the graph. We see that we can represent our network's\n       error as some composition of functions:\n       \\(\\varphi = a(b(c(\\xi, b^{(2)}_1)), y)\\)."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_7.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"75%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"What we want is to find the derivative of this error\n       with respect to the bias \\(b^{(2)}_1\\).\n       But because it is not a direct function of this bias, we must apply\n       the chain rule."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The chain rule tells us that when trying to find\n       the derivative of some expression with respect to\n       a variable that it does not directly depend on, we\n       first must find the derivative of that expression with\n       respect to the variable it does directly depend on. "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Each node in the graph directly depends on\n       the node(s) before it, so, per the chain rule, we must\n       calculate the derivative of each node with respect to\n       its predecessor(s). Then we can construct another\n       computational graph that runs in the opposite direction,\n       where each node passes down its derivative\n       with respect to the previous node."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_11.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"75%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This might seem like a somewhat artificial construction,\n      but bear with me for a minute \u2013 phrasing things in this way\n      will make the solution to our problem just pop out. That said,\n      now that we have identified all necessary derivatives,\n      the chain rule tells us we must multiply them together\n      to get our answer \u2013 the derivative of our error with\n      respect to the bias."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"span","span",1394872991),"And if we study the graph a bit more carefully, we see\n        that all the derivatives we must multiply together "], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"span","span",1394872991),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"font-size","font-size",-1847940346),"20px"], null)], null)," \\(\\Big( \\frac{\\partial{\\varphi}}{\\partial{\\tau}}, \\frac{d\\tau}{d\\pi}, \\frac{\\partial{\\pi}}{\\partial{b^{(2)}_1}} \\Big)\\)"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"span","span",1394872991)," lie somewhere on the path from \\(\\varphi\\) to\n         our bias \\(b^{(2)}_1\\)."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So we can compute the derivative in question\n      simply by traversing the graph, accumulating\n      all the derivatives being passed down from node to node,\n      and multiplying them together."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And this fact is no less true for any other\n     nodes in our graph. Since moving between any two\n     nodes means encountering all the intermediary states\n     between them, and as the chain rule is fundamentally\n     a statement about how we must multiply the derivatives\n     of these states when differentiating nested expressions,\n     any path on this graph computes some derivative!"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_2.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"100%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Moreover, we can imagine constructing such a\n       graph for the whole network. Then calculating the\n       gradient corresponds to traversing the paths from our final\n       node to each weight and bias."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But we might notice that many of these paths\n       are essentially the same, only diverging at the very end.\n       For instance, consider the paths from \\(\\varphi\\)\n       to the weights \\(w^{(1)}_{(1,\\hspace{0.1cm}1)}\\) and\n       \\(w^{(1)}_{(1,\\hspace{0.1cm}2)}\\). They are equivalent\n       except for two divisions \u2013 in other words, they differ by\n       only two derivatives."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Traversing the graph two separate times\n        \u2013 one for each weight \u2013 seems wasteful. Wouldn't it\n        be better if we could somehow ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"remember"], null)," what the shared\n        segment of their paths was, so we wouldn't have to re-traverse the\n        whole graph all over again?"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Then we could calculate the entire gradient in one traversal.\n        To see this in action, imagine that every time we step to a new node\n        we multiply the derivative the current node is passing down to\n        it by all previous derivatives, and store it in some variable \\(\\delta_n\\)\n        denoting the local error signal \u2013 or the derivative of\n        the error with respect to the current node."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Now, when we reach a branch in our graph \u2013 say\n        where \\(\\pi\\) branches out to\n        \\(\\xi\\) and the bias \\(b^{(2)}_1\\) \u2013\u00A0\n        we have the derivative chain up to that point\n        stored in the local error signal, so even if we choose to step\n        to \\(\\xi\\) first, we don't have to re-traverse the entire graph when\n        we want to find the derivative with respect to the bias.\n        We can just pick back up from where we left off."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_18.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"75%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This process of caching the current derivative\n       chain in an associated variable for each node\n       is called ",personal_website.utils.link("memoization","https://en.wikipedia.org/wiki/Memoization#:~:text=In%20computing%2C%20memoization%20or%20memoisation,the%20same%20inputs%20occur%20again")," and does wonders for our efficiency."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But the problem is not completely solved just yet.\n        Even though we now know which derivatives we have to multiply\n        together to find each element of the gradient,\n        we still don't know what the actual numerical values\n        for these derivatives are."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_3.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"70%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Calculating these values, however, is much easier than it might seem.\n        Even though we've been dealing with each node as if it were some abstract function,\n        in reality it's a rather simple operation \u2013 additions,\n        multiplications, or applications of our non-linearity.\n        And since we know the derivatives of these operations,\n        finding the derivatives between nodes is trivial."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"For instance, examine the node \\(\\iota\\) in the graph above. It is the sum of\n        two variables: the node \\(\\eta\\) and the bias \\(b^{(1)}_1\\).\n        So obviously the derivative with respect to both variables\n        is just one. "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"We can similarly consider node \\(\\epsilon\\) which is\n        the product of the first entry of our input vector \\(x_1\\)\n        and the weight \\(w^{(1)}_{(1, \\hspace{0.1cm} 1)}\\).\n        Again, it's obvious that the weight's derivative\n        is just \\(x_1\\)."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"It is worth noting that things aren't always this simple, though.\n        The computational graph for a feedforward neural net is just\n        some linear chain of nodes; there is only one path by which to reach\n        each node. But as we up the complexity of our architectures with things\n        like skip connections and recurrence,\n        we can get graphs for which this property no longer holds."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_20.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"special",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"50%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 7. An example of one\n           of these more complex computational graphs. There are three different\n           paths from the final node (beige) to the starting node (dark green)."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"That might seem like a problem at first \u2013 if there are multiple paths to a single node,\n        then there are also multiple definitions for its derivative. So which one do we choose?\n        The answer, perhaps unsurprisingly, is all of them, in that we add them all together."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This is just a more general version of the chain rule,\n        what you might call the multivariable chain rule. If a variable\n        depends on another variable in two different ways, then we have\n        to sum up the derivatives of both of these dependencies in order to\n        find the total derivative. In other words, the local\n        error signal at a given node is the sum of all paths\n        that lead to it."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\frac{d}{dx}\\left(f(a(x), b(x))\\right) = \\frac{da}{dx}\\frac{\\partial{f}}{\\partial{a}} + \\frac{db}{dx}\\frac{\\partial{f}}{\\partial{b}}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So what does all of this mean, taken together? Well, the first key\n        idea here is that any complex expression\n        we might want to differentiate \u2013 like our network's\n        error \u2013\u00A0is actually just a composition of many\n        elementary operations whose derivatives we already know. And\n        a computational graph is nothing more than the visual manifestation\n        of this composition."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Secondly, the chain rule provides us with a very\n        straightforward way to find the derivatives\n        of such compositions. And even though we, as humans,\n        wouldn't usually use the chain rule, to, say,\n        differentiate an expression like \\(x^2 + 2x\\),\n        technically you could. After all, the chain rule\n        is the only differentiation rule you really need to know \u2013\n        everything else follows from it. Plus, it generalizes\n        rather nicely to the multivariable case \u2013 all we have\n        to do is add ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"paths"], null)," together."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And this approach works great for computers, because\n        it allows us to hardcode the expressions for only a few\n        basic derivatives while still enabling them to\n        differentiate pretty complex expressions. If we also\n        think to memoize derivative chains, then what we get is\n        an algorithm for quickly and efficiently computing exact\n        derivatives, which is exactly what we need to make\n        gradient descent viable."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"An Algorithmic Perspective"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Together this entire process is known as reverse-mode\n        automatic differentiation (AD). In the context of neural\n        networks, though, it goes under another name \u2013 backpropagation. "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But it's actually not the only way we could traverse\n        a computational graph. After all, our decision to start\n        at the end of the graph was an arbitrary one \u2013 we could\n        have just as easily chosen to start at the beginning\n        of the graph and propagate derivatives forward instead."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In fact, this method \u2013 forward-mode AD \u2013 and reverse-mode\n        AD are conceptually equivalent, because they are just different\n        ways of parsing the chain rule. Given some function composition\n        \\(f(g(h(x)))\\), forward-mode computes derivatives from the\n        inside-out, whereas reverse-mode does the opposite, computing\n        them from the outside-in."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\left(\\frac{dh}{dx}\\right)\\left(\\frac{dg}{dh}\\frac{df}{dg}\\right) \\hspace{1cm} \\textrm{vs.} \\hspace{1cm}\n          \\left(\\frac{df}{dg}\\frac{dg}{dh}\\right)\\left(\\frac{dh}{dx}\\right)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So why do we use reverse-mode in neural nets then? It has to do with the fact\n        that each method is only optimal in a specific situation. For instance, let's imagine we want\n        to differentiate some expression whose graph has many inputs but only a single\n        output."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"When using forward mode,\n        we start at one of the input nodes and explore all the paths from that node to the output node.\n        So after one pass through the graph, we end up with the derivative\n        of the output with respect to that one input. Reverse-mode on the other hand \u2013\n        which explores the paths from the one output node to all input nodes \u2013\n        can get the derivatives of the output with respect to every inputin a single go."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/adiff_1.png",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"80%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 7. (Source: ",personal_website.utils.link("Ha\u030Avard Berland, 2006","https://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf"),")."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Of course, there's the flip side: when we have a graph with many outputs, but only\n        a single input, forward-mode can grab everything in one go, since it only has to\n        worry about a single input anyway. Reverse-mode, however, only gets the\n        the derivative of output with the respesct to the input each time it runs."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/adiff_2.png",new cljs.core.Keyword(null,"class","class",-2030961996),"post-i mg",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"80%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null),"Fig. 8. (Source: ",personal_website.utils.link("Ha\u030Avard Berland, 2006","https://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf"),")."], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The general rule is, given some function \\(f : M \\rightarrow N\\),\n       when \\(M \\gg N\\) \u2013\u00A0when we have many more inputs than outputs \u2013\u00A0we should\n       use reverse-mode, since forward mode will have to traverse the graph \\(M\\) times\n       while reverse-mode will only have to traverse it \\(N\\) times. For the same reasons,\n       when \\(M \\ll N\\), forward-mode is the optimal method."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And obviously neural nets fit the first description \u2013 they are functions from millions\n        or even billions of parameters (GPT-3, anyone?) to usually no more than a hundred outputs,\n        and most of the time, even less than that. If we used forward mode\n        to differentiate neural nets instead, our performance could\n        potentially be millions of times worse!"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Let's Vectorize!"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But what we've described so far still isn't how things are done in practice.\n        As I mentioned before, concerning ourselves each individual each weight and\n        bias is not the most efficient nor most elegant way of doing things.\n        Instead, a much cleaner conception of what's going on\n        can be achieved by representing things in a vectorized fashion."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Given a \\(k\\)-neuron layer,\n        with \\(j\\) neurons in the previous layer,\n        we can stack the weights and biases of each of these \\(k\\) neurons into\n        a matrix and vector respectively."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\boldsymbol{W}^{(L)} =\n           \\begin{bmatrix}\n           w^{(L)}_{(1, \\hspace{0.1cm} 1)} & \\cdots &w^{(L)}_{(1, \\hspace{0.1cm} j)} \\\\\n           \\vdots  & \\ddots & \\vdots \\\\\n           w^{(L)}_{(k, \\hspace{0.1cm} 1)} & \\cdots &w^{(L)}_{(k, \\hspace{0.1cm} j)} \\\\\n           \\end{bmatrix} \\in \\mathbb{R}^{k \\times j}\n\n           \\hspace{2cm}\n\n        \\boldsymbol{\\vec{b}}^{(L)} =\n        \\begin{bmatrix}\n        b^{(L)}_1 \\\\\n        b^{(L)}_2 \\\\\n        \\vdots \\\\\n        b^{(L)}_k \\\\\n        \\end{bmatrix} \\in \\mathbb{R}^{k}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Multiplying this matrix against the vector of previous\n        activations yields a new vector containing the weighted\n        sum each neuron computes. This is an equivalent construction\n        to our earlier model,  where each neuron dotted its weight\n        vector against the incoming input vector, as matrix-vector\n        multiplications are just a way of batching many dot products."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"If we add this resulting vector to our bias vector and do\n        an element-wise application of our non-linearity, then\n        we have computed the vector of activations for that layer like so:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\boldsymbol{\\vec{\\alpha}}^{(L)} = \\sigma\\left(\\boldsymbol{W}^{(L)}\\boldsymbol{\\vec{\\alpha}}^{(L-1)} + \\boldsymbol{\\vec{b}}^{(L)}\\right)$$"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"margin-bottom","margin-bottom",388334941),"30px"], null)], null),"As before, our network's output is just some recursive composition\n       of these layers, which we can also represent\n       with a (noticeably more succinct) set of computational graphs."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_13.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"100%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figure","figure",-561394079),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"class","class",-2030961996),"img-container",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"margin-top","margin-top",392161226),"50px"], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"div","div",1057191632),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"text-align","text-align",1786091845),"center"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"img","img",1442687358),new cljs.core.PersistentArrayMap(null, 3, [new cljs.core.Keyword(null,"src","src",-1651076051),"/resources/comp_graph_14.svg",new cljs.core.Keyword(null,"class","class",-2030961996),"post-img",new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"width","width",-384071477),"100%"], null)], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"figcaption","figcaption",-1790122047),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-caption"], null)], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The only problem here is that the derivatives being passed down\n        in the adjoint graph are not\n        scalar-to-scalar derivatives like we dealt with earlier,\n        but vector-to-vector, or even vector-to-matrix derivatives.\n        As such, we need to develop some reasonable notion of what these\n        derivatives should look like."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Let's imagine some function mapping an input\n        vector \\(\\boldsymbol{\\vec{x}}\\) to an\n        output vector \\(\\boldsymbol{\\vec{y}}\\).\n        If we want to measure the effect varying the\n        input vector has on the output vector (i.e., its derivative), we need\n        to determine how each element of the input\n        affects each element of the output."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In other words, we need to find the individual\n        derivatives of each element of the output with\n        respect to each element of the input, which we\n        could organize into a matrix like so:"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"font-size","font-size",-1847940346),"20px"], null)], null),"$$\n      \\begin{bmatrix}\n        \\frac{\\partial{\\boldsymbol{\\vec{y}}_1}}{\\partial\\boldsymbol{\\vec{x}}_1} & \\cdots & \\frac{\\partial{\\boldsymbol{\\vec{y}}_1}}{\\partial{\\boldsymbol{\\vec{x}}_n}} \\\\\n        \\vdots & \\ddots & \\vdots \\\\\n         \\frac{\\partial{\\boldsymbol{\\vec{y}}_n}}{\\partial{\\boldsymbol{\\vec{x}}_1}} & \\cdots & \\frac{\\partial{\\boldsymbol{\\vec{y}}_n}}{\\partial{\\boldsymbol{\\vec{x}}_n}} \\\\\n        \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In some sense then, this matrix is the derivative\n        of that vector-to-vector function, because it represents\n        all the ways the output vector could change given some\n        infinitesimal nudge to the input vector. We refer to such\n        a matrix as the Jacobian, and can think of it as a generalization\n        of the gradient, since it similarly consolidates all of a\n        function's derivative ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"information"], null),"."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In other words, each derivative being passed down in the adjoint graph\n        is a Jacobian. So we can perform reverse-mode AD just like before,\n        substituting matrix multiplication for the scalar kind.\n        Of course, this requires storing the derivatives of some basic\n        matrix operations: vector addition, matrix-vector multiplication,\n        and element-wise function applications."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Let's start out easy with vector addition. Remember,\n        calculating the Jacobian means differentiating each entry of the output\n        with respect to each entry of the input."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\frac{\\partial}{\\partial{\\boldsymbol{\\vec{x}}}}\n\n\n\n\n\\left(        \\begin{bmatrix}\n          x_1 \\\\\n          x_2 \\\\\n          \\vdots \\\\\n          x_n \\\\\n          \\end{bmatrix} +\n          \\begin{bmatrix}\n             y_1 \\\\\n             y_2 \\\\\n             \\vdots \\\\\n             y_n\\\\\n             \\end{bmatrix}\\right) = \\begin{bmatrix}\n                \\frac{\\partial}{\\partial{x_1}}(x_1 + y_1) & \\cdots & \\frac{\\partial}{\\partial{x_n}}(x_1 + y_1) \\\\\n                \\vdots & \\ddots & \\vdots \\\\\n                \\frac{\\partial}{\\partial{x_1}}(x_n + y_n) & \\cdots & \\frac{\\partial}{\\partial{x_n}}(x_n + y_n)  \\\\\n                \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In this case, the result is the identity matrix\n           \\(\\boldsymbol{I}\\). This offers a nice parallel to\n           classical calculus, as differentiating the sum\n           of two different variables also returns the\n           multiplicative identity.",personal_website.utils.make_footnote("5","fifth-footnote-a","fifth-footnote-b")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Slightly more complex is the idea of differentiating a matrix-vector product\n        with respect to the vector. It turns out though that this also has a nice\n        interpretation under the rules of classical calculus, if we think of the matrix\n        as a coefficient. Setting up the Jacobian: "], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\frac{\\partial}{\\partial{\\boldsymbol{\\vec{x}}}} \\left(\n          \\begin{bmatrix}\n          a & b \\\\\n          c & d \\end{bmatrix}\n          \\begin{bmatrix} x_1 \\\\ x_ 2 \\\\ \\end{bmatrix} \\right)\n          =\n          \\begin{bmatrix}\n            \\frac{\\partial}{\\partial{x_1}}(ax_1 + bx_2) & \\frac{\\partial}{\\partial{x_2}}(ax_1 + bx_2) \\\\\n            \\frac{\\partial}{\\partial{x_1}}(cx_1 + dx_2) & \\frac{\\partial}{\\partial{x_2}}(cx_1 + dx_2) \\\\\n          \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"We see that it simplifies to the matrix\n        \\(\\boldsymbol{W}\\). In other words, the derivative\n        of a matrix-vector product with respect to the\n        vector is the matrix the vector is being multiplied by."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"But what if we want to differentiate this product the other\n        way around, with respect to the matrix? This is a bit\n        harder, because differentiating each entry of a vector with\n        respect to each entry of a matrix means we'll need multiple matrices,\n        one per element of our output."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The generalization of a matrix to higher dimensions is known as a tensor,\n        and they're not the most pleasant things to work with. But if we examine\n        the situation more closely, we'll see that it is actually possible to\n        represent the Jacobian in matrix form. To see what I mean, let's try differentiating\n        only a single element of the output vector \\(\\boldsymbol{\\vec{y}}\\)\n        with respect to the matrix."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\frac{\\partial{\\boldsymbol{\\vec{y}}}_1}{\\partial{\\boldsymbol{W}}} =\n          \\begin{bmatrix}\n          \\frac{\\partial}{\\partial{a}}(ax_1 + bx_2) & \\frac{\\partial}{\\partial{b}}(ax_1 + bx_2) \\\\\n          \\frac{\\partial}{\\partial{c}}(ax_1 + bx_2) & \\frac{\\partial}{\\partial{d}}(ax_1 + bx_2) \\\\\n          \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Simplifying, we see that the resulting matrix only has one non-zero row.\n        Furthermore, we would find that this trend holds for any matrix-vector product\n        we could imagine. And if we think about it, this makes sense, too \u2013\u00A0after all,\n        each element of the output vector only depends on one row of the\n        actual matrix, so all its other derivatives will be zero."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"If we do this for each element in our output vector, then we get\n        a series of mostly non-zero matrices with a single row filled in.\n        But the filled-in rows for each of these matrices never collide,\n        because each element of the output vector depends on a different\n        row of the matrix."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\frac{\\partial{\\boldsymbol{\\vec{y}}}_1}{\\partial{\\boldsymbol{W}}} =\n            \\begin{bmatrix}\n            x_1 & x_2 \\\\\n            0 & 0 \\\\\n            \\end{bmatrix}\n            \\hspace{1cm}\n            \\frac{\\partial{\\boldsymbol{\\vec{y}}}_2}{\\partial{\\boldsymbol{W}}} =\n            \\begin{bmatrix}\n            0 & 0 \\\\\n            x_1 & x_2 \\\\\n            \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So even though the Jacobian, as we've previously defined it, should\n                 be some 3D stack of these matrices \u2013 some tensor \u2013\n                 because none of their non-trivial entries interfere,\n                 we could imagine smushing this 3D stack down into a single matrix."," Thus:"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\frac{\\partial}{\\partial{\\boldsymbol{W}}} \\left(\n                  \\begin{bmatrix}\n                  a & b \\\\\n                  c & d \\end{bmatrix}\n                  \\begin{bmatrix} x_1 \\\\ x_ 2 \\\\ \\end{bmatrix} \\right)\n                  =\n               \\begin{bmatrix}\n               x_1 & x_2 \\\\\n               x_1 & x_2\\\\\n               \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The final derivative we must consider is that of element-wise function applications \u2013\u00A0such as with our\n                    non-linearity \u2013 where an element-wise function is defined like so:"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$ f\\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} \\right) =\n            \\begin{bmatrix}\n            f(x_1) \\\\\n            f(x_2) \\\\\n            \\vdots \\\\\n            f(x_n) \\\\\n            \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The Jacobian of such a function, then, looks something like this,\n       where the diagonal entries hold the derivative of the function,\n       and all non-diagonal entries are zero:"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\begin{bmatrix}\n          \\frac{\\partial}{\\partial{x_1}}(f(x_1)) & \\cdots & \\frac{\\partial}{\\partial{x_1}}(f(x_n)) \\\\\n          \\vdots & \\ddots & \\vdots \\\\\n          \\frac{\\partial}{\\partial{x_1}}(f(x_n)) & \\cdots & \\frac{\\partial}{\\partial{x_n}}(f(x_n)) \\\\\n         \\end{bmatrix} =\n\n         \\begin{bmatrix}\n          f\\prime & 0 & \\cdots & 0 \\\\\n          0 & f\\prime & \\dots & 0 \\\\\n          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n          0 & 0 & \\cdots & f\\prime \\end{bmatrix}\n         $$"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Moreover, matrix multiplying against a diagonal matrix like this one\n       is analogous to performing element-wise multiplication against a matrix filled\n       with whatever is occupying this diagonal,",personal_website.utils.make_footnote("6","sixth-footnote-a","sixth-footnote-b")," where the element-wise \u2013 or Hadamard \u2013\u00A0product is denoted\n       \\(\\boldsymbol{A} \\odot \\boldsymbol{B}\\). This is what we do in practice,\n       because it is much more efficient than matrix multiplication."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In fact, the whole reason we vectorize to begin with\n       is because bundling things into matrices and doing batch\n       operations is faster than doing things one-by-one. And knowing\n       only these matrix calculus primitives, we can pretty much\n       differentiate through any vanilla neural net!"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"The Generalized Chain Rule"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"I would be remiss if I didn't mention one more thing though.\n     Isn't it kind of convenient how the chain rule just automatically\n     generalized to Jacobians? Why did that happen?"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"It has to do with the deeper definition of derivatives as linear maps.\n       Even if we don't often think of them in this way, derivatives naturally satisfy\n       the two properties of linearity: the derivative of a sum is the sum\n       of derivatives, and the derivative of a function times a scalar is the derivative\n       of that function times the scalar (English kind of breaks down here...)."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$D(f + g) = Df + Dg \\hspace{1cm} D(cf) = c(Df)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"And because we can represent linear maps as matrices, we can represent\n        derivatives as matrices too. And these are what we've been calling Jacobians all along!\n        When our function maps from scalars to scalars, then our Jacobian\n        is just a one-element matrix \u2013 a scalar, or the classical\n        definition of the derivative. When it maps\n        from vectors to scalars, it's a matrix with a single column \u2013\u00A0a vector,\n        a.k.a the gradient. And when it maps from vectors to vectors, it's a completely\n        filled matrix, the way we originally introduced it."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In other words, we're always working with Jacobians, where a Jacobian\n        represents all the ways a function could change given some nudge\n        to its input. And the chain rule is nothing more than a statement about the Jacobian of\n        a function composition, which says that given two functions \\(f\\) and \\(g\\),\n        the Jacobian of their composition \\(f(g(x))\\), where \\(x\\) could be a scalar,\n        vector, matrix, or even tensor, is the Jacobian of \\(f\\) at \\(g\\) ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"em","em",707813035),"composed"], null)," with\n         the Jacobian of \\(g\\) at \\(x\\).\n        "], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$D(f \\circ g)(x) = Df(g(x)) \\circ Dg(x)$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So it's not that the chain rule magically generalizes to Jacobians \u2013\n        it's defined in terms of them to begin with. Traditional calculus\n        just doesn't expose them in their full generality. And the only reason\n        there's a notion of multiplication in the chain rule in the first place\n        is because composing linear maps is defined as matrix multiplication."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This is a long-winded way of saying that the derivative is the Jacobian - they're equivalent\n        concepts. And the different branches of calculus just study increasingly less specialized\n        versions of it. In that sense, all automatic differentiation does\n        is compute a Jacobian-vector product, or the Jacobian at a specific input."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"Thinking about things in terms of Jacobians also highlights an important difference between forward-mode\n        and reverse-mode. As we ackowledged before, one pass with forward-mode\n        computes the derivatives of all outputs with respect to a single input, while\n        one pass with reverse-mode computes the derivatives of a single output with respect to all inputs."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"$$\\begin{bmatrix}\\frac{\\partial{\\boldsymbol{\\vec{y}}}_1}{\\partial{\\boldsymbol{\\vec{x}}}_1} \\\\\n                          \\frac{\\partial{\\boldsymbol{\\vec{y}}}_2}{\\partial{\\boldsymbol{\\vec{x}}}_1} \\\\\n                          \\vdots \\\\\n                          \\frac{\\partial{\\boldsymbol{\\vec{y}}}_n}{\\partial{\\boldsymbol{\\vec{x}}}_1} \\\\\n                          \\end{bmatrix} \\hspace{1cm}\\textrm{vs.}\n                    \\hspace{1cm}\\begin{bmatrix}\\frac{\\partial{\\boldsymbol{\\vec{y}}}_1}{\\partial{\\boldsymbol{\\vec{x}}}_1}  & \\frac{\\partial{\\boldsymbol{\\vec{y}}}_1}{\\partial{\\boldsymbol{\\vec{x}}}_2} & \\ldots & \\frac{\\partial{\\boldsymbol{\\vec{y}}}_1}{\\partial{\\boldsymbol{\\vec{x}}}_n}  \\end{bmatrix}$$"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"In other words, forward-mode computes the Jacobian column-by-column whereas reverse-mode does\n        so row-by-row. That's why functions with many outputs (many rows) are better suited to forward-mode\n        and those with many inputs (many columns) to reverse-mode."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Differentiable Programming: A Vision"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"One last thing, I promise! Automatic differentiation is not just\n        limited to typical mathematical expressions. Anything that can be\n        represented as a computational graph is fair game. That includes\n        control flow constructs like loops and conditionals, and so as a consequence,\n        entire programs can be differentiated over too!"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"This is where the idea of differentiable programming was borne:\n        automatic differentiation that can run directly on programs.\n        We're still a ways off from being able to do this (the extent of our differentiable\n        programs right now \u2013 neural nets \u2013 are basically just increasingly inventive compositions\n        of matrix multiplication), but this is where things are headed in the future\n        (check the further reading if you're interested), and personally, I think\n        a good language for differentible programming will take the whole\n        deep learning paradigm to the next level."], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"The main point here, though, is that as a tool, automatic differentiation\n        is very flexible, and can give us some notion of a derivative\n        where even classical calculus fails."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Conclusion"], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"So to recap, automatic differentiation is a way of quickly computing derivatives with computers.\n     It has both a forward-mode and reverse-mode implementation, the latter of which is\n     used in neural nets. And like all truly great ideas it is based on a simple, but\n     piercing insight: that the amount of expressions we can differentiate grows\n     exponentially with the amount of elementary derivatives that we know. In other words,\n     the chain rule is a lot more powerful than we may always give it credit for."], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"It also shows us the power in simple tricks such as a memoization \u2013 tools\n     of the trade from a branch of computer science known as ",personal_website.utils.link("dynamic programming","https://en.wikipedia.org/wiki/Dynamic_programming"),". The thing to take away from this is that even if something looks\n     like it'd be expensive to compute at first glance,\n     it might be unintuitively cheap once we figure out how\n     to reuse computation and reduce redundancy."], null),new cljs.core.PersistentVector(null, 6, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"All in all, automatic differentiation is one powerhouse of an algorithm,\n     often cited as one of the most important ones to have come forward in\n     the twentieth century. Understanding it well opens\n     up many doors in scientific computing and beyond, and research\n     in the field is ongoing. Given that neural networks only\n     discovered it relatively recently,",personal_website.utils.make_footnote("7","seventh-footnote-a","seventh-footnote-b")," it might very well be\n     that there are other ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"killer"], null)," applications of the\n     technique yet to be found. After all, there's not\n     much you can't solve with derivatives."], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"References"], null),new cljs.core.PersistentVector(null, 9, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"ul","ul",-1349521403),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"list-style-type","list-style-type",-1703248598),"circle"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("Ha\u030Avard Berland's Slides","https://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("Calculus on Computational Graphs","https://colah.github.io/posts/2015-08-Backprop/")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("How to Differentiate with a Computer","http://www.ams.org/publicoutreach/feature-column/fc-2017-12")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("Computing Neural Network Gradients","https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("Vector, Matrix, and Tensor Derivatives","http://cs231n.stanford.edu/vecDerivs.pdf")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("Automatic Differentiation in Machine Learning: A Survey","https://arxiv.org/abs/1502.05767")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("The CS 6120 Course Blog: Automatic Differentiation in Bril","https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/autograd/")], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Further Reading"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"ul","ul",-1349521403),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"list-style-type","list-style-type",-1703248598),"circle"], null)], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("The Simple Essence of Automatic Differentiation","https://arxiv.org/abs/1804.00746")], null),new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"li","li",723558921),personal_website.utils.link("Automatic Differentiation in ML: Where We Are and Where We Should Be Going","https://arxiv.org/abs/1810.11530")], null)], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"h1","h1",-1896887462),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"class","class",-2030961996),"post-section-header"], null),"Footnotes"], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"id","id",-1388402092),"first-footnote-b"], null),personal_website.utils.bold("1.")," It's relatively trivial to demonstrate this:"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"$$\\begin{aligned}\n    \\lim_{h\\to 0} \\frac{f(x+h, y) - f(x, y)}{h}\\hspace{1cm}\n    &\\textrm{(Definition of the derivative of } f(x,y) = x^2 + y^2 \\hspace{0.1cm}\\textrm{ w.r.t to } x)\n    \\\\ \\\\ \\lim_{h\\to 0} \\frac{(x+h)^2 + y^2 - (x^2 + y^2)}{h}\\hspace{1cm}\n    &\\textrm{(Simplification)}\n    \\\\ \\\\ \\lim_{h\\to 0} \\frac{(x+h)^2 - x^2}{h}\\hspace{1cm}\n    &\\textrm{(Definition of the derivative of } f(x) = x^2 \\hspace{0.1cm}\\textrm{ w.r.t to } x)\n    \\end{aligned}$$"], null),new cljs.core.PersistentVector(null, 5, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"As you see, \\(y\\) washes out. Another way to think\n    about this is that since \\(y\\) isn't being nudged by some infinitesimal\n    \\(h\\) like \\(x\\) is, its presence in the first term will be completely canceled\n    out by its presence in the second, which is just another way of saying\n    that since it isn't changing, it is ",new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"q","q",689001697),"invisible"], null)," as far as the\n    derivative is concerned.",personal_website.utils.make_footnote("\u21A9","first-footnote-b","first-footnote-a")], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),personal_website.utils.bold("2.")," The simplest error function you could have is just\n         the difference between the prediction and the label:\n         \\(E(\\hat{y}, y) = \\hat{y} - y\\). In practice\n         we actually square this difference to accentuate\n         really bad predictions and so that we don't have to deal\n         with a negative result. This is known as the square error:\n         \\(E(\\hat{y}, y) = (\\hat{y} - y)^2\\).\n         ",personal_website.utils.make_footnote("\u21A9","second-footnote-b","second-footnote-a")], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),personal_website.utils.bold("3.")," What is \\(\\boldsymbol{\\theta}\\) doing in the subscript of the gradient symbol?\n       It's just a way of saying that even though our function\n       also takes in an input and a label, we don't want to include their partials\n       in our gradient \u2013 after all, we can't\n       change our input or label, those two pieces\n       of information are set for the problem.",personal_website.utils.make_footnote("\u21A9","third-footnote-b","third-footnote-a")], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),personal_website.utils.bold("4.")," \\(\\eta\\) is a hyperparameter, or a variable\n          the neural net does not learn by itself, but must be explicitly\n          set. It's known as learning rate, since it controls\n          how quickly we descend the error-surface, and thus how\n          fast we learn.",personal_website.utils.make_footnote("\u21A9","fourth-footnote-b","fourth-footnote-a")], null),new cljs.core.PersistentVector(null, 4, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),personal_website.utils.bold("5.")," All this means is that addition is some\n                        operation, that, when you change its inputs,\n                         reflects the exact same change in its output.",personal_website.utils.make_footnote("\u21A9","fifth-footnote-b","fifth-footnote-a")], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),personal_website.utils.bold("6.")," A demonstration of this property with concrete matrices:"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"style","style",-496642736),new cljs.core.PersistentArrayMap(null, 2, [new cljs.core.Keyword(null,"overflow-x","overflow-x",-26547754),"auto",new cljs.core.Keyword(null,"overflow-y","overflow-y",-1436589285),"hidden"], null)], null),"\n        $$\\begin{align}\n          \\begin{bmatrix}\n            a & b \\\\\n            c & d \\\\\n          \\end{bmatrix}\n          \\begin{bmatrix}\n              x & 0 \\\\\n              0 & x \\\\\n          \\end{bmatrix} =\n          \\begin{bmatrix}\n              ax & bx \\\\\n              cx & dx \\\\\n          \\end{bmatrix}\\hspace{1cm}&\\textrm{(Multiplication against diagonal matrix)} \\\\ \\\\\n\n          \\begin{bmatrix}\n              a & b \\\\\n              c & d \\\\\n            \\end{bmatrix}\n            \\odot\n            \\begin{bmatrix}\n                x & x \\\\\n                x & x \\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                ax & bx \\\\\n                cx & dx \\\\\n            \\end{bmatrix}\\hspace{1cm}&\\textrm{(Hadamard product)}\n            \\end{align}$$"], null),new cljs.core.PersistentVector(null, 3, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),"\n            As you can see, they are equivalent. But whereas\n            the Hadamard product only has to do \\(N \\times M\\)\n            operations when multiplying two\n            \\(N \\times M\\) matrices, matrix multiplication\n            has to do \\(N \\times M \\times M \\times (M - 1)\\) operations.",personal_website.utils.make_footnote("\u21A9","sixth-footnote-b","sixth-footnote-a")], null),new cljs.core.PersistentVector(null, 6, 5, cljs.core.PersistentVector.EMPTY_NODE, [new cljs.core.Keyword(null,"p","p",151049309),personal_website.utils.bold("7.")," While researching for this post, I found out\n        that before AD was commonly used, neural net researchers\n        would calculate the derivatives for their networks by hand\n        and then hardcode them into computers.\n        So obviously networks couldn't be deep! Read more ",personal_website.utils.link("here","https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/"),".",personal_website.utils.make_footnote("\u21A9","seventh-footnote-b","seventh-footnote-a")], null)], true);
personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.media_query_1 = (function personal_website$content$writings$blog_posts$the_mathematics_of_automatic_differentiation$media_query_1(){
return garden.stylesheet.at_media.cljs$core$IFn$_invoke$arity$variadic(new cljs.core.PersistentArrayMap(null, 1, [new cljs.core.Keyword(null,"max-width","max-width",-1939924051),"600px"], null),cljs.core.prim_seq.cljs$core$IFn$_invoke$arity$2([cljs.core.PersistentVector.EMPTY], 0));
});
personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.post = cljs.core.PersistentHashMap.fromArrays([new cljs.core.Keyword(null,"tags","tags",1771418977),new cljs.core.Keyword(null,"overarching","overarching",696949346),new cljs.core.Keyword(null,"date","date",-1463434462),new cljs.core.Keyword(null,"content","content",15833224),new cljs.core.Keyword(null,"css","css",1135045163),new cljs.core.Keyword(null,"type","type",1174270348),new cljs.core.Keyword(null,"title","title",636505583),new cljs.core.Keyword(null,"id","id",-1388402092),new cljs.core.Keyword(null,"show","show",-576705889)],[new cljs.core.PersistentVector(null, 2, 5, cljs.core.PersistentVector.EMPTY_NODE, ["mathematics","deep learning"], null),"writing","12/22/2020",personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.post_content,personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.media_query_1(),"blog-post","Automatic Differentiation In Neural Nets","1",personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.post_preview]);

//# sourceMappingURL=personal_website.content.writings.blog_posts.the_mathematics_of_automatic_differentiation.js.map
