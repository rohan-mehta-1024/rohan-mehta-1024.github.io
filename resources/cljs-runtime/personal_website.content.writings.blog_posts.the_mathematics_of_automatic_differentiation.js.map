{"version":3,"sources":["personal_website/content/writings/blog_posts/the_mathematics_of_automatic_differentiation.cljs"],"mappings":";AAGA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC;AAwCL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIF,AACGC","names":["personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-preview","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-content","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation\n  (:require [personal-website.utils :as utils]))\n\n(def post-preview\n  \"Automatic differentiation is the numerical computing algorithm that allows us\n   to optimize neural nets with practical time and space costs. In this post, we will\n   explore the mathematics that makes it possible and implmenent it ourselves.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   [:p \"Automatic differentiation is the cornerstone of the differentiable programming paradigm \u2014\n       the idea that we can allow programs to optimize their behavior against certain metrics by\n       differentiating over and updating sets of  learnable functions. In fact, it is the very algorithm\n       that allows neural nets to learn! And while it\u2019s role in the story of deep learning is not always a leading one,\n       it is an elegant piece of mathematics whose cleverness deserves to be chronicled. Furthermore,\n       I doubt I can put better than Feynman the importance of leaving the realm of theory every once in a while\n       and using one\u2019s own two hands (\u201cWhat I cannot make, I do not understand\u201d), so as we build up to this idea\n       we will implement a toy AD engine in C which will be capable of training vanilla neural nets.\"]\n\n   [:p \"With that in mind, let\u2019s start very simple. Neural nets are just hierarchies of \u201clayers\u201d -\n        vertical stacks of interconnected units known as neurons. Each neuron is associated with some learnable\n        weight vector which computes a dot product with the vector of incoming values, adjusts it by some (also learnable)\n        scalar and pumps it through a non-linearity, such that optimizing these learnable parameters\n        results in a function with   the desired behavior, ranging from language translation\n        to caption generation.\"]\n\n\n    [:figure {:class \"img-container\"}\n      [:div {:style {:text-align \"center\"}}\n        [:img {:src \"/auto_diff_nn_pic.svg\"  :style {:width \"65%\"}}]]\n      [:figcaption {:class \"post-caption\"}\n        \"Fig. 1. An example of a simple neural network. Learnable transformations of information allow\n        them to approximate all kinds of functions.\"]]\n\n\n    [:p \"This is known - it is neural networks\n        in their simplest, most straightforward form. But treating optimization as a block box makes things seem deceptively\n        easy, when in fact these optimization algorithms (along with the hardware they\u2019re  running on) are the sole\n        differentiator between neural nets being only of theoretical interest versus practical utility, and automatic\n        differentiation underpins every single one of them. So how does it actually work?\"]\n\n   [:h1 {:class \"post-section-header\"} \"Learning To Differentiate\"]\n\n   [:h1 \"Computational Graphs\"]\n\n\n   ])\n\n(def post\n  {:title \"The Mathematics Of Automatic Differentiation\"\n   :date \"2020/10/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :id \"1\"})\n"]}