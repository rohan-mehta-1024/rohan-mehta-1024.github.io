{"version":3,"sources":["personal_website/content/writings/blog_posts/the_mathematics_of_automatic_differentiation.cljs"],"mappings":";AAIA,AAAA,AAAKA;AAOL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAkDC,AAAA,AAAA,AAACC,AA2CsC,AAAA,AAAA,AAACA,AAc5B,AAAA,AAAA,AAAA,AAACC,AA8Cf,AAAA,AAAA,AAACD,AAkGA,AAAA,AAAA,AAAA,AAACC,AAID,AAAA,AAAA,AAACD,AAiCD,AAAA,AAAA,AAAA,AAACC,AAUsC,AAAA,AAAA,AAACD,AAa3B,AAAA,AAAA,AAAA,AAACC,AAgQA,AAAA,AAAA,AAACD,AAsIqC,AAAA,AAAA,AAACA,AAaS,AAAA,AAAA,AAACA,AAWP,AAAA,AAAA,AAACA,AA4J1B,AAAA,AAAA,AAAA,AAACC,AAyHY,AAAA,AAAA,AAAA,AAACA,AA4EY,AAAA,AAAA,AAACD,AAWvB,AAAA,AAAA,AAAA,AAACC,AAShC,AAAA,AAAA,AAACD,AACD,AAAA,AAAA,AAACA,AACD,AAAA,AAAA,AAACA,AACD,AAAA,AAAA,AAACA,AACD,AAAA,AAAA,AAACA,AAOmB,AAAA,AAACE,AAmB7B,AAAA,AAAA,AAAA,AAACD,AAIG,AAAA,AAACC,AAOK,AAAA,AAAA,AAAA,AAACD,AAGN,AAAA,AAACC,AAMA,AAAA,AAAA,AAAA,AAACD,AAEH,AAAA,AAACC,AAKC,AAAA,AAAA,AAAA,AAACD,AAGJ,AAAA,AAACC,AAAiB,AAAA,AAAA,AAAA,AAACD,AAGjB,AAAA,AAACC,AAkCA,AAAA,AAAA,AAAA,AAACD,AAGA,AAAA,AAACC,AAGkD,AAAA,AAAA,AAACF,AACpD,AAAA,AAAA,AAAA,AAACC;AAWT,AAAA,AAAME;AAAN,AACC,AAAA,AAAA,AAAA,AAAA,AAACC;;AASF,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIP,AACGC,AAKJ,AAACI","names":["personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-preview","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-content","personal-website.utils/link","personal-website.utils/make-footnote","personal-website.utils/bold","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/media-query-1","garden.stylesheet.at_media","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation\n  (:require [personal-website.utils :as utils]\n            [garden.stylesheet :refer [at-media]]))\n\n(def post-preview\n  \"Automatic differentiation is the numerical computing technique\n   that gave us the backpropagation algorithm, which is how\n   neural nets learn. In this post, we will explore it\n   from both a mathematics\n   and computer science perspective.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   [:p \"Backpropagation is the cornerstone of the differentiable\n        programming paradigm \u2014 the idea that we can allow programs\n        to optimize their behavior against certain metrics by\n        differentiating over and updating sets of learnable functions.\n        In fact, it is the very algorithm that allows neural\n        nets to learn! But it is really only one of the many applications\n        of an even broader numerical computing technique known as automatic differentiation.\"]\n\n   [:p \"Its contribution to neural nets is perhaps its most popularized use today,\n        but the idea has a long history, and its story is ongoing. In its modern\n        day incarnation, it draws on a broad swath of mathmatical\n        knowledge and is\n        a beautiful amalgamation of many different ideas. While\n        our discussion of the technique will be phrased in terms of its utility to\n        neural nets, though, the ideas underpinning it are far more applicable,\n        illustrating some deeper truths about calculus, dynamic programming, and the\n        art of problem solving in general.\"]\n\n   [:p \" What follows is a concise review of\n   multivariable calculus, before we delve into some more complex math.\n   If you don't need it, skip ahead. Otherwise, read on!\"]\n\n   [:h1 {:class \"post-section-header\"} \"Learning To Differentiate\"]\n\n   [:p \"Derivatives of single-variable functions are\n        measurements of how infinitesimal variations\n        to a function's input-space correspond to the\n        resulting variations in its output-space.\n        And the same is true of  multivariable functions,\n        except now we have many more ways in which to\n        vary our input-space.\"]\n\n   [:p \"To start, let\u2019s consider a function of two\n        variables. Such a function can be pictured\n        as a surface above the Cartesian plane, where\n        the height of a point on that surface is\n        calculated using the function \\\\(f(x,y)\\\\)\n        in question. With this in mind, the geometrical\n        interpretation of the derivative is how\n        the elevation of this surface changes given\n        a small step in some direction.\"]\n\n  [:figure {:class \"img-container\"}\n   [:div {:style {:text-align \"center\"}}\n    [:img {:src \"/resources/multi-fn-2.png\" :style {:width \"45%\"}}]\n    [:img {:src \"/resources/multi-fn-4.png\" :style {:width \"45%\"}}]\n     [:figcaption {:class \"post-caption\" :style {:text-align \"left\"}}\n     \"Fig. 1. Examples of surfaces generated by a function of two variables (Source: \"\n      (utils/link \"CalcPlot3D\" \"https://www.monroecc.edu/faculty/paulseeburger/calcnsf/CalcPlot3D/\")\").\"]]]\n\n\n   [:p \"An important thing to notice here is that\n        this step could be in any direction: the\n        \\\\(x\\\\) direction, \\\\(y\\\\) direction, or some\n        combination of the two. Say, for instance, we\n        wanted to differentiate the function\n        \\\\(f(x, y) = x^2 + y^2\\\\) with respect to \\\\(x\\\\).\n        We now know what this means geometrically,\n        but how exactly would we go about calculating it?\"]\n\n   [:p \"Imagine standing at some point on this\n        function's surface, and walking horizontally\n        in both directions. If we walk such that a spool\n        of yarn unravels behind us, the shape this yarn\n        takes will resemble the graph of the single-variate\n        function \\\\(f(x) = x^2 + C\\\\) (a parabola) where\n        \\\\(C\\\\) is some constant, namely whatever the\n        \\\\(y\\\\)-value of our original point was.\"]\n\n   [:p \"It's not immediately obvious, but by walking\n        horizontally across our surface we only varied\n        our \\\\(x\\\\)-coordinate, while our \\\\(y\\\\)-coordinate\n        remained constant. When walking in this way,\n        the surface is described by a special case of the\n        original function where \\\\(y\\\\) is a constant.\"]\n\n      [:p \"Similarly, a walk\n           in a purely vertical direction is described\n           when \\\\(x\\\\) is a constant. In fact, we would find that\n           all multivariable functions\n           behave in this way when being walked upon\n           purely in the direction of one of their variables,\n           such that all others become constants.\"]\n\n  [:figure {:class \"img-container\"}\n   [:div {:style {:text-align \"center\"}}\n    [:img {:src \"/resources/parabaloid-1.png\" :style {:width \"50%\"}}]\n    [:img {:src \"/resources/parabaloid-2.png\" :style {:width \"50%\"}}]\n    [:figcaption {:class \"post-caption\" :style {:text-align \"left\"}}\n    \"Fig. 2. The graph the function \\\\(f(x,y) = x^2 + y^2\\\\), known as a paraboloid.\n    Slicing it across the \\\\(x\\\\)-axis (or walking in the \\\\(x\\\\) direction) reveals that \\\\(x\\\\)-wise\n    cross-sections are parabolas. (Source: \" (utils/link \"GeoGebra3D\" \"https://www.geogebra.org/3d?lang=en\")\").\"]]]\n\n   [:p \"In that vein of thought, we might conjecture\n        that when computing the derivative of our\n        function \\\\(f(x,y) = x^2 + y^2\\\\) with\n        respect to \\\\(x\\\\), we can imagine we are\n        instead differentiating \\\\(f(x) = x^2 + C\\\\),\n        as this is what the surface looks like when\n        moving in the \\\\(x\\\\) direction. Then the\n        derivative with respect to \\\\(x\\\\) (what\n        we call the partial derivative) would\n        be \\\\(2x\\\\), or \\\\(\\\\frac{\\\\partial{f}}{\\\\partial{x}}\\\\ = 2x\\\\)\"]\n\n   [:p \"And it turns out that this thought is in fact\n        correct!\" (utils/make-footnote \"1\" \"first-footnote-a\" \"first-footnote-b\")\n        \" When taking the derivative of a multivariable\n        function with respect to only a single variable,\n        we can treat all other variables as constants\n        and just apply our single-variable\n        differentiation rules.\"]\n\n   [:p \"But how do we generalize this idea when taking\n        steps across our surface that aren't only in the\n        \\\\(x\\\\) direction or only in the \\\\(y\\\\) direction,\n        but rather some combination of both? For instance,\n        how would we calculate the derivative for some\n        infinitesimal step along the vector\n        \\\\(\\\\langle 1, 1 \\\\rangle \\\\)?\"]\n\n    [:p \"Well, a step in this direction is equivalent\n         to one step in both the \\\\(x\\\\) direction and\n         \\\\(y\\\\) direction. And since we know how our\n         function changes for both a pure step in the\n         \\\\(x\\\\) direction (\\\\(\\\\frac{\\\\partial{f}}{\\\\partial{x}}\\\\))\n         and \\\\(y\\\\) direction (\\\\(\\\\frac{\\\\partial{f}}{\\\\partial{y}}\\\\)),\n         the derivative in this direction is just their sum:\n         \\\\(\\\\frac{\\\\partial{f}}{\\\\partial{x}} + \\\\frac{\\\\partial{f}}{\\\\partial{y}}\\\\ = 2x + 2y\\\\).\"]\n\n   [:p \"More generally, given any vector \\\\(\\\\boldsymbol{\\\\vec{v}}\\\\)\n        we can compute its corresponding derivative\n        \u2013 or directional derivative \u2013 by dotting it with\n        the vector containing our function's partials.\"]\n\n   [:p \"$$D_{\\\\boldsymbol{\\\\vec{v}}}f(x_1, x_2, \\\\ldots, x_n) = \\\\boldsymbol{\\\\vec{v}} \\\\cdot\n          \\\\begin{bmatrix}\n            \\\\frac{\\\\partial{f}}{\\\\partial{x_1}} \\\\\\\\\n            \\\\frac{\\\\partial{f}}{\\\\partial{x_2}} \\\\\\\\\n            \\\\vdots \\\\\\\\\n            \\\\frac{\\\\partial{f}}{\\\\partial{x_n}} \\\\\\\\\n          \\\\end{bmatrix}$$\"]\n\n   [:figure {:class \"img-container\"}\n   [:img {:src \"/resources/parabaloid-5.png\" :style {:width \"50%\"}}]\n   [:img {:src \"/resources/parabaloid-6.png\" :style {:width \"50%\"}}]\n  [:figcaption {:class \"post-caption\"}\n   \"Fig. 3. The derivative along the vector \\\\(\\\\langle 1, 1 \\\\rangle\\\\)\n    is greater than either partial, since it is their sum.\n    This means that the paraboloid's surface is steeper\n    in this direction than in just the horizontal or vertical ones.\n    In fact, it is two times steeper. (Source: \"\n    (utils/link \"GeoGebra3D\" \"https://www.geogebra.org/3d?lang=en\")\").\"]]\n\n   [:p \"We can now\n        calculate all of a function's possible derivatives.\n        A natural question then, is what derivative is the greatest?\n        In what direction does a function increase most rapidly?\"]\n\n   [:p \"The equation for directional derivatives says that\n        the derivative with respect to some vector is that\n        vector dotted with the vector of our function's\n        partials, so answering this question means finding\n        whatever vector yields the largest possible dot\n        product in this situation. \"]\n\n   [:p \"And since a vector's dot product is largest when dotted\n        against itself, the vector that maximizes this dot product\n        is itself the vector containing all the partials of our function!\n        So the direction represented by this vector must be\n        the direction of steepest descent \u2013 the greatest\n        derivative of our function.\"]\n\n   [:p \"Intuitively, this makes sense too,\n        as how much this vector points in a given\n        direction is equivalent to the derivative\n        in that direction, such that it points\n        more in steeper directions and less in shallow\n        ones, thus becoming the steepest direction itself.\n        This vector is known as a function's gradient,\n        and is denoted like so:\"]\n\n   [:p \"$$\\\\nabla f(x_1, x_2, \\\\ldots, x_n) =\n          \\\\begin{bmatrix}\n            \\\\frac{\\\\partial{f}}{\\\\partial{x_1}} \\\\\\\\\n            \\\\frac{\\\\partial{f}}{\\\\partial{x_2}} \\\\\\\\\n            \\\\vdots \\\\\\\\\n            \\\\frac{\\\\partial{f}}{\\\\partial{x_n}} \\\\\\\\\n          \\\\end{bmatrix}$$\"]\n\n\n\n   [:h1 {:class \"post-section-header\"} \"Gradient Descent\"]\n\n   [:p \"We've now gone through all the basics necessary\n        to consider how we might optimize neural nets,\n        and the first step lies in understanding how\n        we can model them mathematically.\"]\n\n  [:p \"As a quick review, we think of neural nets as universal\n       function approximators. Theoretically, if given enough\n       data and made deep enough, they can represent any\n       conceivable function.\"]\n\n  [:p \"And what gives them this ability is their constituent unit,\n       the neuron:  a computational machine with the ability to\n       \" [:q \"learn\"] \". Mathematically, a neuron's activation\n       (output) is defined like so, where \\\\(\\\\boldsymbol{\\\\vec{x}}\\\\)\n       is some input vector, \\\\(\\\\boldsymbol{\\\\vec{w}}\\\\) and \\\\(b\\\\)\n       are the neuron's two learnable parameters (known as a\n       weight and bias respectively) and \\\\(\\\\sigma\\\\) denotes\n       some non-linearity:\"]\n\n  [:p \"$$\\\\alpha = \\\\sigma(\\\\boldsymbol{\\\\vec{x}} \\\\cdot \\\\boldsymbol{\\\\vec{w}} + b)$$\"]\n\n  [:p \"We refer to a neuron's parameters as learnable\n       because they can be updated such that the\n       neuron returns the desired output for a\n       given input. And somewhat counter to intuition,\n       by stacking these simplistic units together, we\n       can get the resulting structure \u2013 known as a neural\n       net \u2013 to exhibit surprisingly intelligent behavior.\"]\n\n  [:p \"What's more, all of these behaviors can be framed\n       as some sort of prediction task: given lots of labeled\n       data, how do optimize the parameters of all neurons in\n       a given network such that it predicts the correct label\n       for a given input?\"]\n\n[:figure {:class \"img-container\"}\n     [:div {:style {:text-align \"center\"}}\n     [:img {:src \"/resources/neural-net-1.svg\" :class \"post-img\" :style {:width \"65%\"}}]\n     [:figcaption {:class \"post-caption\" :style {:text-align \"left\"}}\n     \"Fig. 4. Visual diagram of a neuron. It computes a dot product\n     between the incoming values and its weight vector, adds a bias\n     term, and then pumps everything through some non-linearity.\"]]]\n\n[:p \"All optimization is just a matter of searching\n     the space of possibilities. If we consider only a single\n     neuron, then this space is defined by all\n     \\\\((\\\\boldsymbol{\\\\vec{w}}, b)\\\\) pairs that the neuron\n     could have. But to search this space intelligently,\n     we need to mathematically define what exactly makes\n     a specific pair \" [:q \"good\"] \" \u2013 more concretely,\n     what criterion does a good pair minimize?\"]\n\n[:p \"Good pairs allow the neuron to make predictions that\n     are close to the corresponding label. Said another way,\n     they minimize the margin of error between the prediction\n     and the correct label. If we define an error function\"\n     (utils/make-footnote \"2\" \"second-footnote-a\" \"second-footnote-b\")\n     \" \\\\(E(\\\\hat{y}, y)\\\\)  that takes a prediction \\\\(\\\\hat{y}\\\\)\n     and a label \\\\(y\\\\) and computes this margin of error,\n     then a good pair minimizes the following \"\n     (utils/link \"objective function\" \"https://en.wikipedia.org/wiki/Mathematical_optimization\") \":\n     $$J(\\\\boldsymbol{\\\\vec{x}}, y;  \\\\boldsymbol{\\\\theta}) = E(\\\\alpha, y)$$\"]\n\n [:p \"This function takes in some labeled datapoint\n      \\\\((\\\\boldsymbol{\\\\vec{x}}, y)\\\\) and our parameters\n      \\\\(\\\\boldsymbol{\\\\theta}\\\\). Using these, it computes\n      our neuron's activation, and passes it and the label\n      \\\\(y\\\\) to our error function. It returns low\n      values for good parameters, because good parameters\n      generate good predictions, and good predictions have\n      a low margin of error.\"]\n\n[:p \"So optimizing our neuron boils down to finding parameters\n     located in the local minima of this function, or the\n     \"[:q \"valleys\"] \" of whatever high-dimensional\n     \"[:q \"error-surface\"] \" it encodes.\"]\n\n[:p \"For a single neuron, it would be easy enough to obtain\n     these analytically (by solving for critical points),\n     but as we scale things up this approach quickly becomes\n     intractable. We might instead consider the simpler sub-problem\n     of how to get a slightly better parameter pair\n     (instead of an optimal one), and then repeat this\n     process over and over.\"]\n\n[:p \"A better pair is just one for whom the function\n     returns a lower value \u2013 so any pair on our error-surface\n     with lower elevation than our current one. We can get\n     to such a pair by taking some step across this surface\n     in the downhill direction. To be most efficient,\n     we would want to move in the direction of steepest\n     descent \u2013 or the negative gradient,\n     \\\\(-\\\\nabla_{\\\\boldsymbol{\\\\theta}} J\\\\)!\"\n     (utils/make-footnote \"3\" \"third-footnote-a\" \"third-footnote-b\")]\n\n [:figure {:class \"img-container\"}\n      [:div {:style {:text-align \"center\"}}\n     [:img {:src \"/resources/loss_landscape-4.jpg\" :class \"post-img\" :style {:width \"65%\"}}]\n      [:figcaption {:class \"post-caption\" :style {:text-align \"left\"}}\n      \"Fig. 5. A visualization of the error-surface for a single neuron. Each point\n      is some parameter pair \\\\((\\\\boldsymbol{\\\\vec{w}}, b)\\\\) and the height of each\n      pair indicates how badly the neuron performs when it uses them.\n      By traveling along the negative gradient\n      we can arrive at good pairs (Source: \"(utils/link \"Amini et. al, 2018\" \"file:///Users/Rohan%20Mehta/Downloads/Spatial_Uncertainty_Sampling_for_End-to-End_Contro%20(1).pdf\")\").\"]]]\n\n [:p \"And it turns out that repeatedly stepping in the\n      direction of the negative gradient \u2013 a process\n      aptly named gradient descent \u2013 does indeed\n      produce a \\\\((\\\\boldsymbol{\\\\vec{w}}, b)\\\\)\n      pair that minimizes our objective function,\n      and thus maximizes our neuron's performance.\"]\n\n[:p \"Since the gradient is only valid for infinitesimal\n     steps, though, we need to  make sure the steps we\n     are taking, while not infinitesimal, are appropriately\n     small. Thus we scale down the gradient by some factor\n     \\\\(\\\\eta\\\\),\"(utils/make-footnote \"4\" \"fourth-footnote-a\" \"fourth-footnote-b\")\"\n     such that the step we actually take is: \\\\(-\\\\eta \\\\nabla_{\\\\boldsymbol{\\\\theta}}J\\\\).\"]\n\n [:p \"Mathematically, \" [:q \"taking a step\"] \" in the direction\n      of some vector just means adding that vector to our\n      parameter vector \\\\(\\\\boldsymbol{\\\\theta}\\\\),\n      so the equation for gradient descent looks like this:\"]\n\n  [:p \"$$\\\\boldsymbol{\\\\theta}^{new} = \\\\boldsymbol{\\\\theta}^{old} - \\\\eta\\\\nabla_{\\\\boldsymbol{\\\\theta}}J$$\"]\n\n\n  [:p \"And this one-liner is, more or less, how neural nets learn!\n       But we've made one simplification so far. Our current objective\n       function \\\\(J\\\\) only takes into account one labeled datapoint\n       \\\\((\\\\boldsymbol{\\\\vec{x}}, y)\\\\), but being able to correctly\n       predict the label for a single datapoint doesn't guarantee the\n       neuron is learning the underlying mapping from datapoints\n       to labels that we want it to.\"]\n\n   [:p \"It could, for instance, be learning the constant function that\n        maps every input to the same label. So instead of calculating\n        the error in our neuron's prediction against a single datapoint,\n        we must do so against all datapoints in our dataset, and then\n        take the average. In other words, the function we're\n        really trying to minimize looks more like this:\"]\n\n   [:p \"$$J(\\\\boldsymbol{X}, \\\\boldsymbol{\\\\vec{y}}; \\\\boldsymbol{\\\\theta}) = \\\\frac{1}{n}\\\\sum_{i=1}^{n} E(\\\\alpha, \\\\boldsymbol{\\\\vec{y}}_i)$$\"]\n\n   [:p \"And since the gradient of a sum equals the sum of gradients,\n        the gradient of this function is the average\n        of the gradients of our previous objective function\n        evaluated across all samples.\"]\n\n   [:p \"In so many words, we're going to have to take a lot of gradients,\n        one for each of our thousands (or millions) of samples,\n        possibly hundreds of times (one per step). So we'll need a way\n        to compute them quickly if this approach is to be at all practical.\"]\n\n\n\n  [:h1 {:class \"post-section-header\"} \"Computational Graphs\"]\n\n\n  [:p \"Before we focus on computing these gradients, though,\n       we first need to upgrade our current working model\n       of only a single neuron to a fully-fledged neural net.\"]\n\n  [:p \"As we touched on before, a neural net is just many\n       individual neurons stacked atop one another. More\n       specifically, a given network is a horizontal stack\n       of many \" [:q \"layers\"] \", which are themselves vertical\n       stacks of neurons. Computation happens as activations are\n       propagated between adjacent layers, such that each neuron\n       takes in all activations of the previous layer.\"]\n\n  [:p \"In other words, each entry in a given neuron's weight\n       vector is multiplied with the activation of a neuron\n       in the previous layer. So we can visualize each of these\n       entries as a weighted edge connecting the current neuron to\n       a neuron in the previous layer.\"]\n\n  [:p \"After computing the weighted sum these edges encode,\n       adding a bias, and passing the whole thing through a\n       non-linearity, we obtain a neuron's activation, which\n       is propagated forward and becomes a part of the weighted\n       sum computed by neurons in the next layer. \"]\n\n\n  [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/resources/neural-net-3.svg\" :class \"post-img\" :style {:width \"70%\"}}]]\n      [:figcaption {:class \"post-caption\"}\n       \"Fig. 6. An example of a neural network that maps 2D vectors to scalars. Each\n        neuron is connected to every neuron in the previous layer by some weighted edge\n        determined by its weight vector.\"]]\n\n  [:p \"This view of the neuron, where we decompose each\n       neuron's weight vector into a set of weighted edges,\n       is admittedly slightly more messy and less elegant\n       than our previous model. But as we don't know how\n       to take the derivative with respect to an entire vector\n       (yet), it is a necessary evil if we want to be able to\n       optimize our parameters by gradient descent.\"]\n\n   [:p \"So how do we represent all this mathematically?\n        Well, we can index each neuron by its layer and\n        position in that layer. Each of a neuron's weights can\n        then be indexed with one additional term, representing\n        the position of the neuron it connects to in the previous\n        layer. Thus the activation of the \\\\(i\\\\)th neuron in\n        the \\\\(L\\\\)th layer, given \\\\(j\\\\) neurons in the previous layer, is:\"]\n\n   [:p \"$$\\\\alpha^{(L)}_i = \\\\sigma \\\\left( \\\\sum_{n=0}^{j} \\\\alpha^{(L - 1)}_n \\\\cdot w^{(L)}_{(i, \\\\hspace{0.1cm} n)} + b^{(L)}_i \\\\right)$$\"]\n\n   [:p {:style {:margin-bottom \"30px\"}}\n    \"Writing things out in this way makes it exceedingly\n    clear that a neuron's activation is defined recursively,\n    since it appears in its own definition. The mathematical\n    consequence of this recursiveness is that the expression\n    representing a network's output is incredibly nested, so that\n    even a trivial net like the one above is actually surprisingly\n    complex, once expanded out.\"]\n\n   [:p {:style {:font-size \"15px\" :margin-bottom \"30px\" :overflow-x \"auto\" :overflow-y \"hidden\"} :id \"special\"} \"\n   $$\\\\sigma \\\\left(\\\\bbox[yellow]{\\\\alpha^{(1)}_1} \\\\cdot w^{(2)}_{(1, \\\\hspace{0.1cm} 1)}+ \\\\bbox[orange]{\\\\alpha^{(1)}_2} \\\\cdot w^{(2)}_{(1, \\\\hspace{0.1cm} 2)} + b^{(2)}_1\\\\right)$$\n   $$\\\\sigma \\\\left(\\\\bbox[yellow]{\\\\sigma\\\\left(x^{(0)}_1 \\\\cdot w^{(1)}_{(1, \\\\hspace{0.1cm} 1)} + x^{(0)}_2 \\\\cdot w^{(1)}_{(1, \\\\hspace{0.1cm} 2)} + b^{(1)}_ 1\\\\right)} \\\\cdot w^{(2)}_{(1, \\\\hspace{0.1cm} 1)} + \\\\bbox[orange]{\\\\sigma\\\\left(x^{(0)}_1 \\\\cdot w^{(1)}_{(1, \\\\hspace{0.1cm} 1)} + \\\\alpha^{(0)}_2 \\\\cdot w^{(1)}_{(1, \\\\hspace{0.1cm} 2)} + b^{(1)}_2 \\\\right) \\\\cdot w^{(2)}_{(1, \\\\hspace{0.1cm} 1)}} + b^{(2)}_1 \\\\right)$$\"]\n\n\n  [:p \"And, unfortunately for us, it is this bulky expression\n       we're going to have to pass into our error function, because\n       it represents the network's prediction. Worse, we're going have\n       to differentiate it with respect to each of our weights and biases\n       to calculate the gradient.\"]\n\n  [:p \"But this expression is so convoluted, it's unclear where we would\n       even start. Luckily, we can simplify things with a helpful model\n       known as a computational graph, where the nodes of this graph\n       represent some fundamental operations (e.g., addition, multiplication)\n       and its edges represent the values flowing into these operations.\"]\n\n  [:p {:style {:margin-bottom \"30px\"}}\n    \"We think of each node as performing its operation\n     on the values it receives, and then spitting out some\n     output. What's more, by storing each node's output\n     in some associated variable we can track all intermediate\n     states an expression goes through before arriving\n     at its final state \u2013 the last node in our graph \u2013\n     and this will turn out to be incredibly helpful. \"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/resources/comp_graph.svg\" :class \"post-img\" :style {:width \"90%\"}}]]\n      [:figcaption {:class \"post-caption\"}]]\n\n  [:p \"Let's imagine constructing such a graph to represent\n       the evaluation of our error function at this three-layer\n       net's prediction for some labeled datapoint \\\\((\\\\boldsymbol{\\\\vec{x}}, y)\\\\).\n       Calculating the gradient means finding the derivatives\n       of the final node with respect to each of our weights\n       and biases. To make things easier, we'll abstract away\n       each node with some generic function.\"]\n\n  [:p \"With that in mind, let's examine the following segment\n       of the graph. We see that we can represent our network's\n       error as some composition of functions:\n       \\\\(\\\\varphi = a(b(c(\\\\xi, b^{(2)}_1)), y)\\\\).\"]\n\n  [:figure {:class \"img-container\"}\n   [:div {:style {:text-align \"center\"}}\n     [:img {:src \"/resources/comp_graph_7.svg\" :class \"post-img\" :style {:width \"75%\"}}]]\n     [:figcaption {:class \"post-caption\"}]]\n\n\n  [:p \"What we want is to find the derivative of this error\n       with respect to the bias \\\\(b^{(2)}_1\\\\).\n       But because it is not a direct function of this bias, we must apply\n       the chain rule.\"]\n\n  [:p \"The chain rule tells us that when trying to find\n       the derivative of some expression with respect to\n       a variable that it does not directly depend on, we\n       first must find the derivative of that expression with\n       respect to the variable it does directly depend on. \"]\n\n  [:p \"Each node in the graph directly depends on\n       the node(s) before it, so, per the chain rule, we must\n       calculate the derivative of each node with respect to\n       its predecessor(s). Then we can construct another\n       computational graph that runs in the opposite direction,\n       where each node passes down its derivative\n       with respect to the previous node.\"]\n\n [:figure {:class \"img-container\"}\n   [:div {:style {:text-align \"center\"}}\n     [:img {:src \"/resources/comp_graph_11.svg\" :class \"post-img\" :style {:width \"75%\"}}]]\n     [:figcaption {:class \"post-caption\"}]]\n\n [:p \"This might seem like a somewhat artificial construction,\n      but bear with me for a minute \u2013 phrasing things in this way\n      will make the solution to our problem just pop out. That said,\n      now that we have identified all necessary derivatives,\n      the chain rule tells us we must multiply them together\n      to get our answer \u2013 the derivative of our error with\n      respect to the bias.\"]\n\n [:span \"And if we study the graph a bit more carefully, we see\n        that all the derivatives we must multiply together \" ]\n\n [:span {:style {:font-size \"20px\"}} \" \\\\(\\\\Big( \\\\frac{\\\\partial{\\\\varphi}}{\\\\partial{\\\\tau}}, \\\\frac{d\\\\tau}{d\\\\pi}, \\\\frac{\\\\partial{\\\\pi}}{\\\\partial{b^{(2)}_1}} \\\\Big)\\\\)\"]\n\n [:span \" lie somewhere on the path from \\\\(\\\\varphi\\\\) to\n         our bias \\\\(b^{(2)}_1\\\\).\"]\n\n [:p \"So we can compute the derivative in question\n      simply by traversing the graph, accumulating\n      all the derivatives being passed down from node to node,\n      and multiplying them together.\"]\n\n [:p \"And this fact is no less true for any other\n     nodes in our graph. Since moving between any two\n     nodes means encountering all the intermediary states\n     between them, and as the chain rule is fundamentally\n     a statement about how we must multiply the derivatives\n     of these states when differentiating nested expressions,\n     any path on this graph computes some derivative!\"]\n\n\n [:figure {:class \"img-container\"}\n  [:div {:style {:text-align \"center\"}}\n    [:img {:src \"/resources/comp_graph_2.svg\" :class \"post-img\" :style {:width \"100%\"}}]]\n    [:figcaption {:class \"post-caption\"}]]\n\n\n  [:p \"Moreover, we can imagine constructing such a\n       graph for the whole network. Then calculating the\n       gradient corresponds to traversing the paths from our final\n       node to each weight and bias.\"]\n\n  [:p \"But we might notice that many of these paths\n       are essentially the same, only diverging at the very end.\n       For instance, consider the paths from \\\\(\\\\varphi\\\\)\n       to the weights \\\\(w^{(1)}_{(1,\\\\hspace{0.1cm}1)}\\\\) and\n       \\\\(w^{(1)}_{(1,\\\\hspace{0.1cm}2)}\\\\). They are equivalent\n       except for two divisions \u2013 in other words, they differ by\n       only two derivatives.\"]\n\n   [:p \"Traversing the graph two separate times\n        \u2013 one for each weight \u2013 seems wasteful. Wouldn't it\n        be better if we could somehow \"[:q \"remember\"] \" what the shared\n        segment of their paths was, so we wouldn't have to re-traverse the\n        whole graph all over again?\"]\n\n   [:p \"Then we could calculate the entire gradient in one traversal.\n        To see this in action, imagine that every time we step to a new node\n        we multiply the derivative the current node is passing down to\n        it by all previous derivatives, and store it in some variable \\\\(\\\\delta_n\\\\)\n        denoting the local error signal \u2013 or the derivative of\n        the error with respect to the current node.\"]\n\n   [:p \"Now, when we reach a branch in our graph \u2013 say\n        where \\\\(\\\\pi\\\\) branches out to\n        \\\\(\\\\xi\\\\) and the bias \\\\(b^{(2)}_1\\\\) \u2013\u00a0\n        we have the derivative chain up to that point\n        stored in the local error signal, so even if we choose to step\n        to \\\\(\\\\xi\\\\) first, we don't have to re-traverse the entire graph when\n        we want to find the derivative with respect to the bias.\n        We can just pick back up from where we left off.\"]\n\n    [:figure {:class \"img-container\"}\n     [:div {:style {:text-align \"center\"}}\n       [:img {:src \"/resources/comp_graph_18.svg\" :class \"post-img\" :style {:width \"75%\"}}]]\n       [:figcaption {:class \"post-caption\"}]]\n\n\n  [:p \"This process of caching the current derivative\n       chain in an associated variable for each node\n       is called \" (utils/link \"memoization\" \"https://en.wikipedia.org/wiki/Memoization#:~:text=In%20computing%2C%20memoization%20or%20memoisation,the%20same%20inputs%20occur%20again\")\n       \" and does wonders for our efficiency.\"]\n\n   [:p \"But the problem is not completely solved just yet.\n        Even though we now know which derivatives we have to multiply\n        together to find each element of the gradient,\n        we still don't know what the actual numerical values\n        for these derivatives are.\"]\n\n              [:figure {:class \"img-container\"}\n               [:div {:style {:text-align \"center\"}}\n                 [:img {:src \"/resources/comp_graph_3.svg\" :class \"post-img\" :style {:width \"70%\"}}]]\n                 [:figcaption {:class \"post-caption\"}]]\n\n\n   [:p \"But calculating these values is much easier than it might seem.\n        Even though we've been dealing with each node as if it were some abstract function,\n        in reality it's a rather simple operation \u2013 additions,\n        multiplications, or applications of our non-linearity.\n        And since we know the derivatives of these operations,\n        finding the derivatives between nodes is trivial.\"]\n\n   [:p \"For instance, examine the node \\\\(\\\\iota\\\\) in the graph above. It is the sum of\n        two variables: the node \\\\(\\\\eta\\\\) and the bias \\\\(b^{(1)}_1\\\\).\n        So obviously the derivative with respect to both variables\n        is just one. \"]\n\n   [:p \"We can similarly consider node \\\\(\\\\epsilon\\\\) which is\n        the product of the first entry of our input vector \\\\(x_1\\\\)\n        and the weight \\\\(w^{(1)}_{(1, \\\\hspace{0.1cm} 1)}\\\\).\n        Again, it's obvious that the weight's derivative\n        is just \\\\(x_1\\\\).\"]\n\n   [:p \"It is worth noting that things aren't always this simple, though.\n        The computational graph for a feedforward neural net is just\n        some linear chain of nodes; there is only one path by which to reach\n        each node. But as we up the complexity of our architectures with things\n        like skip connections and recurrence,\n        we can get graphs for which this property no longer holds.\"]\n\n        [:figure {:class \"img-container\"}\n         [:div {:style {:text-align \"center\"}}\n           [:img {:src \"/resources/comp_graph_20.svg\" :class \"special\" :style {:width \"50%\"}}]]\n           [:figcaption {:class \"post-caption\"} \"Fig. 7. An example of one\n           of these more complex computational graphs. There are three different\n           paths from the final node (beige) to the starting node (dark green).\"]]\n\n\n\n\n   [:p \"That might seem like a problem at first \u2013 if there are multiple paths to a single node,\n        then there are also multiple definitions for its derivative. So which one do we choose?\n        The answer, perhaps unsurprisingly, is all of them, in that we add them all together.\"]\n\n   [:p \"This is just a more general version of the chain rule,\n        what you might call the multivariable chain rule. If a variable\n        depends on another variable in two different ways, then we have\n        to sum up the derivatives of both of these dependencies in order to\n        find the total derivative. In other words, the local\n        error signal at a given node is the sum of all paths\n        that lead to it.\"]\n\n   [:p \"$$\\\\frac{d}{dx}\\\\left(f(a(x), b(x))\\\\right) = \\\\frac{da}{dx}\\\\frac{\\\\partial{f}}{\\\\partial{a}} + \\\\frac{db}{dx}\\\\frac{\\\\partial{f}}{\\\\partial{b}}$$\"]\n\n\n   [:p \"So what does all of this mean, taken together? Well, the first key\n        idea here is that any complex expression\n        we might want to differentiate \u2013 like our network's\n        error \u2013\u00a0is actually just a composition of many\n        elementary operations whose derivatives we already know. And\n        a computational graph is nothing more than the visual manifestation\n        of this composition.\"]\n\n   [:p \"Secondly, the chain rule provides us with a very\n        straightforward way to find the derivatives\n        of such compositions. And even though we, as humans,\n        wouldn't usually use the chain rule, to, say,\n        differentiate an expression like \\\\(x^2 + 2x\\\\),\n        technically you could. After all, the chain rule\n        is the only differentiation rule you really need to know \u2013\n        everything else follows from it. Plus, it generalizes\n        rather nicely to the multivariable case \u2013 all we have\n        to do is add paths together.\"]\n\n   [:p \"And this approach works great for computers, because\n        it allows us to hardcode the expressions for only a few\n        basic derivatives while still enabling them to\n        differentiate pretty complex expressions. If we also\n        think to memoize derivative chains, then what we get is\n        an algorithm for quickly and efficiently computing exact\n        derivatives, which is exactly what we need to make\n        gradient descent viable.\"]\n\n\n\n\n\n   [:h1 {:class \"post-section-header\"} \"An Algorithmic Perspective\"]\n\n   [:p \"Together this entire process is known as reverse-mode\n        automatic differentiation (AD). In the context of neural\n        networks, though, it goes under another name \u2013 backpropagation. \"]\n\n   [:p \"But it's actually not the only way we could traverse\n        a computational graph. After all, our decision to start\n        at the end of the graph was an arbitrary one \u2013 we could\n        have just as easily chosen to start at the beginning\n        of the graph and propagate derivatives forward instead.\"]\n\n   [:p \"In fact, this method \u2013 forward-mode AD \u2013 and reverse-mode\n        AD are conceptually equivalent, because they are just different\n        ways of parsing the chain rule. Given some function composition\n        \\\\(f(g(h(x)))\\\\), forward-mode computes derivatives from the\n        inside-out, whereas reverse-mode does the opposite, computing\n        them from the outside-in.\"]\n\n   [:p \"$$\\\\left(\\\\frac{dh}{dx}\\\\right)\\\\left(\\\\frac{dg}{dh}\\\\frac{df}{dg}\\\\right) \\\\hspace{1cm} \\\\textrm{vs.} \\\\hspace{1cm}\n          \\\\left(\\\\frac{df}{dg}\\\\frac{dg}{dh}\\\\right)\\\\left(\\\\frac{dh}{dx}\\\\right)$$\"]\n\n  [:p \"In forward-mode when we step to a node, we are not\n      computing the derivative of our previous node with respect to it (like we were in reverse-mode),\n      but rather the derivative of it with respect to our previous node.\n      Let's look at a simple example to \"]\n\n   [:p \"Both examples first work by traversing the graph\"]\n\n      [:figure {:class \"img-container\"}\n       [:div {:style {:text-align \"center\"}}\n         [:img {:src \"comp_graph_19.svg\" :class \"post-img\" :style {:width \"50%\"}}]]\n         [:figcaption {:class \"post-caption\"}]]\n\n\n\n   [:p \"We won't get into the specifics, but the solution is the\n        use of a higher-dimensional number system \u2013\u00a0the \"(utils/link \"dual numbers\" \"https://en.wikipedia.org/wiki/Dual_number\") \" \u2013\n        that is seemingly able to compute derivatives for \"[:q \"free\"]\".\"]\n\n   [:p \"So why do we use reverse-mode in neural nets then? Well, it has to do with the fact\n        that each method is only optimal in a specific situation. While dual numbers\n        can compute derivatives very cheaply, they can only do so with respect to one variable at a time.\n        Thus, if our graph has multiple inputs and only a single outout,\n        we must traverse the entire thing multiple times. Reverse-mode, one the other hand,\n        finds all derivatives in a single sweep.\"]\n\n        [:figure {:class \"img-container\"}\n         [:div {:style {:text-align \"center\"}}\n           [:img {:src \"/resources/adiff_1.png\" :class \"post-img\" :style {:width \"80%\"}}]]\n           [:figcaption {:class \"post-caption\"} \"Fig. 7. (Source: \"(utils/link  \"Ha\u030avard Berland, 2006\" \"https://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf\")\").\"]]\n\n   [:p \"Of course, there's the flip side: when we have a graph with many outputs, but only\n        a single input, forward-mode can grab everything in one go (since it only\n        needs to keep track of one variable), while reverse-mode must\n        traverse the whole graph once per output, because even though it memoizes\n        derivative chains, it does so with respect to a specific output.\"]\n\n  [:figure {:class \"img-container\"}\n   [:div {:style {:text-align \"center\"}}\n     [:img {:src \"/resources/adiff_2.png\" :class \"post-i mg\" :style {:width \"80%\"}}]]\n     [:figcaption {:class \"post-caption\"} \"Fig. 8. (Source: \"(utils/link  \"Ha\u030avard Berland, 2006\" \"https://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf\")\").\"]]\n\n\n  [:p \"Thus the general rule is, given some function \\\\(f : M \\\\rightarrow N\\\\),\n       when \\\\(M \\\\gg N\\\\) \u2013\u00a0when we have many more inputs than outputs \u2013\u00a0we should\n       use reverse-mode, since forward mode will have to traverse the graph \\\\(M\\\\) times\n       while reverse-mode will only have to traverse it \\\\(N\\\\) times. For the same reasons,\n       when \\\\(M \\\\ll N\\\\), forward-mode is the optimal method.\"]\n\n   [:p \"And obviously neural nets fit the first description \u2013 they are functions from millions\n        or even billions of parameters (GPT-3, anyone?) to usually no more than a few hundred\n        outputs, and most of the time, even less than that. If we used forward mode\n        to differentiate neural nets instead, our performance could\n        potentially be millions of times worse!\"]\n\n\n\n\n\n\n\n   [:h1 {:class \"post-section-header\"} \"Let's Vectorize!\"]\n\n   [:p \"But what we've described so far still isn't how things are done in practice.\n        As I mentioned before, concerning ourselves each indivdual each weight and\n        bias is not the most efficient nor most elegant way of doing things.\n        Instead, a much cleaner conception of what's going on\n        can be achieved by representing things in a vectorized fashion.\"]\n\n   [:p \"Given a \\\\(k\\\\)-neuron layer,\n        with \\\\(j\\\\) neurons in the previous layer,\n        we can stack the weights and biases of each of these \\\\(k\\\\) neurons into\n        a matrix and vector respectively.\"]\n\n    [:p {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}} \"$$\\\\boldsymbol{W}^{(L)} =\n           \\\\begin{bmatrix}\n           w^{(L)}_{(1, \\\\hspace{0.1cm} 1)} & \\\\cdots &w^{(L)}_{(1, \\\\hspace{0.1cm} j)} \\\\\\\\\n           \\\\vdots  & \\\\ddots & \\\\vdots \\\\\\\\\n           w^{(L)}_{(k, \\\\hspace{0.1cm} 1)} & \\\\cdots &w^{(L)}_{(k, \\\\hspace{0.1cm} j)} \\\\\\\\\n           \\\\end{bmatrix} \\\\in \\\\mathbb{R}^{k \\\\times j}\n\n           \\\\hspace{2cm}\n\n        \\\\boldsymbol{\\\\vec{b}}^{(L)} =\n        \\\\begin{bmatrix}\n        b^{(L)}_1 \\\\\\\\\n        b^{(L)}_2 \\\\\\\\\n        \\\\vdots \\\\\\\\\n        b^{(L)}_k \\\\\\\\\n        \\\\end{bmatrix} \\\\in \\\\mathbb{R}^{k}$$\"]\n\n\n   [:p \"Multiplying this matrix against the vector of previous\n        activations yields a new vector containing the weighted\n        sum each neuron computes. This is an equivalent construction\n        to our earlier model,  where each neuron dotted its weight\n        vector against the incoming input vector, as matrix-vector\n        multiplications are just a way of batching many dot products.\"]\n\n   [:p \"If we add this resulting vector to our bias vector and do\n        an element-wise application of our non-linearity, then\n        we have computed the vector of activations for that layer like so:\"]\n\n\n   [:p \"$$\\\\boldsymbol{\\\\vec{\\\\alpha}}^{(L)} = \\\\sigma\\\\left(\\\\boldsymbol{W}^{(L)}\\\\boldsymbol{\\\\vec{\\\\alpha}}^{(L-1)} + \\\\boldsymbol{\\\\vec{b}}^{(L)}\\\\right)$$\"]\n\n   [:p {:style {:margin-bottom \"30px\"}}\n      \"As before, our network's output is just some recursive composition\n       of these layers, which we can also represent\n       with a (noticeably more succint) set of computational graphs.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/resources/comp_graph_13.svg\" :class \"post-img\" :style {:width \"100%\"}}]]\n      [:figcaption {:class \"post-caption\"}]]\n\n   [:figure {:class \"img-container\" :style {:margin-top \"50px\"}}\n    [:div {:style {:text-align \"center\"}}\n      [:img {:src \"/resources/comp_graph_14.svg\" :class \"post-img\" :style {:width \"100%\"}}]]\n      [:figcaption {:class \"post-caption\"}]]\n\n\n   [:p \"The only problem here is that the derivatives being passed down\n        in the adjoint graph are not\n        scalar-to-scalar derivatives like we dealt with earlier,\n        but vector-to-vector, or even vector-to-matrix derivatives.\n        As such, we need to develop some reasonable notion of what these\n        derivatives should look like.\"]\n\n   [:p \"Let's imagine some function mapping an input\n        vector \\\\(\\\\boldsymbol{\\\\vec{x}}\\\\) to an\n        output vector \\\\(\\\\boldsymbol{\\\\vec{y}}\\\\).\n        If we want to measure the effect varying the\n        input vector has on the output vector (i.e., its derivative), we need\n        to determine how each element of the input\n        affects each element of the output.\"]\n\n   [:p \"In other words, we need to find the individual\n        derivatives of each element of the output with\n        respect to each element of the input, which we\n        could organize into a matrix like so:\"]\n\n    [:p {:style {:font-size \"20px\"}} \"$$\n      \\\\begin{bmatrix}\n        \\\\frac{\\\\partial{\\\\boldsymbol{\\\\vec{y}}_1}}{\\\\partial\\\\boldsymbol{\\\\vec{x}}_1} & \\\\cdots & \\\\frac{\\\\partial{\\\\boldsymbol{\\\\vec{y}}_1}}{\\\\partial{\\\\boldsymbol{\\\\vec{x}}_n}} \\\\\\\\\n        \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\n         \\\\frac{\\\\partial{\\\\boldsymbol{\\\\vec{y}}_n}}{\\\\partial{\\\\boldsymbol{\\\\vec{x}}_1}} & \\\\cdots & \\\\frac{\\\\partial{\\\\boldsymbol{\\\\vec{y}}_n}}{\\\\partial{\\\\boldsymbol{\\\\vec{x}}_n}} \\\\\\\\\n        \\\\end{bmatrix}$$\" ]\n\n\n   [:p \"In some sense then, this matrix is the derivative\n        of that vector-to-vector function, because it represents\n        all the ways the output vector could change given some\n        infinitesimal nudge to the input vector. We refer to such\n        a matrix as the Jacobian, and can think of it as a generalization\n        of the gradient, since it similarly consolidates all of a\n        function's derivative \" [:q \"information\"]\".\"]\n\n   [:p \"In other words, each derivative being passed down in the adjoint graph\n        is a Jacobian. So we can perform reverse-mode AD just like before,\n        substituting matrix multiplication for the scalar kind.\n        Of course, this requires storing the derivatives of some basic\n        matrix operations: vector addition, matrix-vector multiplication,\n        and element-wise function applications.\"]\n\n   [:p \"Let's start out easy with vector addition. Remember,\n        calculating the Jacobian means differentiating each entry of the output\n        with respect to each entry of the input.\"]\n\n   [:p {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}} \"$$\\\\frac{\\\\partial}{\\\\partial{\\\\boldsymbol{\\\\vec{x}}}}\n\n\n\n\n\\\\left(        \\\\begin{bmatrix}\n          x_1 \\\\\\\\\n          x_2 \\\\\\\\\n          \\\\vdots \\\\\\\\\n          x_n \\\\\\\\\n          \\\\end{bmatrix} +\n          \\\\begin{bmatrix}\n             y_1 \\\\\\\\\n             y_2 \\\\\\\\\n             \\\\vdots \\\\\\\\\n             y_n\\\\\\\\\n             \\\\end{bmatrix}\\\\right) = \\\\begin{bmatrix}\n                \\\\frac{\\\\partial}{\\\\partial{x_1}}(x_1 + y_1) & \\\\cdots & \\\\frac{\\\\partial}{\\\\partial{x_n}}(x_1 + y_1) \\\\\\\\\n                \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\n                \\\\frac{\\\\partial}{\\\\partial{x_1}}(x_n + y_n) & \\\\cdots & \\\\frac{\\\\partial}{\\\\partial{x_n}}(x_n + y_n)  \\\\\\\\\n                \\\\end{bmatrix}$$\"]\n\n\n      [:p \"In this case, the result is the identity matrix\n           \\\\(\\\\boldsymbol{I}\\\\). This offers a nice parallel to\n           classical calculus, as differentiating the sum\n           of two different variables also returns the\n           multiplicative identity.\"(utils/make-footnote \"5\" \"fifth-footnote-a\" \"fifth-footnote-b\")]\n\n   [:p \"Slightly more complex is the idea of differentiating a matrix-vector product\n        with respect to the vector. It turns out though that this also has a nice\n        interpretation under the rules of classical calculus, if we think of the matrix\n        as a coefficient. Setting up the Jacobian: \"]\n\n   [:p {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}} \"$$\\\\frac{\\\\partial}{\\\\partial{\\\\boldsymbol{\\\\vec{x}}}} \\\\left(\n          \\\\begin{bmatrix}\n          a & b \\\\\\\\\n          c & d \\\\end{bmatrix}\n          \\\\begin{bmatrix} x_1 \\\\\\\\ x_ 2 \\\\\\\\ \\\\end{bmatrix} \\\\right)\n          =\n          \\\\begin{bmatrix}\n            \\\\frac{\\\\partial}{\\\\partial{x_1}}(ax_1 + bx_2) & \\\\frac{\\\\partial}{\\\\partial{x_2}}(ax_1 + bx_2) \\\\\\\\\n            \\\\frac{\\\\partial}{\\\\partial{x_1}}(cx_1 + dx_2) & \\\\frac{\\\\partial}{\\\\partial{x_2}}(cx_1 + dx_2) \\\\\\\\\n          \\\\end{bmatrix}$$\"]\n\n   [:p \"We see that it simplifies to the matrix\n        \\\\(\\\\boldsymbol{W}\\\\). In other words, the derivative\n        of a matrix-vector product with respect to the\n        vector is the matrix the vector is being multiplied by.\"]\n\n\n   ;[:p \"$$\\\\frac{\\\\partial}{\\\\partial{\\\\boldsymbol{\\\\vec{x}}}}(\\\\boldsymbol{W}\\\\boldsymbol{\\\\vec{x}}) = \\\\boldsymbol{W}$$\"]\n\n   [:p \"But what if we want to differentiate this product the other\n        way around, with respect to the matrix? This is a bit\n        harder, because differentiating each entry of a vector with\n        respect to each entry of a matrix means we'll need multiple matrices,\n        one per element of our output.\"]\n\n   [:p \"The generalization of a matrix to higher dimensions is known as a tensor,\n        and they're not the most pleasant things to work with. But if we examine\n        the situation more closely, we'll see that it is actually possible to\n        represent the Jacobian in matrix form. To see what I mean, let's try differentiating\n        only a single element of the output vector \\\\(\\\\boldsymbol{\\\\vec{y}}\\\\)\n        with respect to the matrix.\"]\n\n\n        [:p {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}} \"$$\\\\frac{\\\\partial{\\\\boldsymbol{\\\\vec{y}}}_1}{\\\\partial{\\\\boldsymbol{W}}} =\n          \\\\begin{bmatrix}\n          \\\\frac{\\\\partial}{\\\\partial{a}}(ax_1 + bx_2) & \\\\frac{\\\\partial}{\\\\partial{b}}(ax_1 + bx_2) \\\\\\\\\n          \\\\frac{\\\\partial}{\\\\partial{c}}(ax_1 + bx_2) & \\\\frac{\\\\partial}{\\\\partial{d}}(ax_1 + bx_2) \\\\\\\\\n          \\\\end{bmatrix}$$\"]\n\n   [:p \"Simplifying, we see that the resulting matrix only has one non-zero row.\n        Furthermore, we would find that this trend holds for any matrix-vector product\n        we could imagine. And if we think about it, this makes sense, too \u2013\u00a0after all,\n        each element of the output vector only depends on one row of the\n        actual matrix, so all its other derivatives will be zero.\"]\n\n   [:p \"If we do this for each element in our output vector, then we get\n        a series of mostly non-zero matrices, with a single row filled in.\n        But the filled-in rows for each of these matrices never collide,\n        because each element of the output vector depends on a different\n        row of the matrix.\"]\n\n\n        [:p {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}} \"$$\\\\frac{\\\\partial{\\\\boldsymbol{\\\\vec{y}}}_1}{\\\\partial{\\\\boldsymbol{W}}} =\n            \\\\begin{bmatrix}\n            x_1 & x_2 \\\\\\\\\n            0 & 0 \\\\\\\\\n            \\\\end{bmatrix}\n            \\\\hspace{1cm}\n            \\\\frac{\\\\partial{\\\\boldsymbol{\\\\vec{y}}}_2}{\\\\partial{\\\\boldsymbol{W}}} =\n            \\\\begin{bmatrix}\n            0 & 0 \\\\\\\\\n            x_1 & x_2 \\\\\\\\\n            \\\\end{bmatrix}$$\"]\n\n            [:p \"So even though the Jacobian, as we've previously defined it, should\n                 be some 3D stack of these matrices \u2013 some tensor \u2013\n                 because none of their non-trivial entries interfere,\n                 we could imagine smushing this 3D stack down into a single matrix.\"\n                \" Thus:\"]\n\n\n\n           [:p {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}} \"$$\\\\frac{\\\\partial}{\\\\partial{\\\\boldsymbol{W}}} \\\\left(\n                  \\\\begin{bmatrix}\n                  a & b \\\\\\\\\n                  c & d \\\\end{bmatrix}\n                  \\\\begin{bmatrix} x_1 \\\\\\\\ x_ 2 \\\\\\\\ \\\\end{bmatrix} \\\\right)\n                  =\n               \\\\begin{bmatrix}\n               x_1 & x_2 \\\\\\\\\n               x_1 & x_2\\\\\\\\\n               \\\\end{bmatrix}$$\"]\n\n               [:p \"The final derivative we must consider is that of element-wise function applications \u2013\u00a0such as with our\n                    non-linearity \u2013 where an element-wise function is defined like so:\"]\n\n\n   [:p \"$$ f\\\\left( \\\\begin{bmatrix} x_1 \\\\\\\\ x_2 \\\\\\\\ \\\\vdots \\\\\\\\ x_n \\\\\\\\ \\\\end{bmatrix} \\\\right) =\n            \\\\begin{bmatrix}\n            f(x_1) \\\\\\\\\n            f(x_2) \\\\\\\\\n            \\\\vdots \\\\\\\\\n            f(x_n) \\\\\\\\\n            \\\\end{bmatrix}$$\"]\n\n  [:p \"The Jacobian of such a function, then, looks something like this,\n       where the diagonal entries hold the derivative of the function,\n       and all non-diagonal entries are zero:\"]\n\n  [:p  {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}} \"$$\\\\begin{bmatrix}\n          \\\\frac{\\\\partial}{\\\\partial{x_1}}(f(x_1)) & \\\\cdots & \\\\frac{\\\\partial}{\\\\partial{x_1}}(f(x_n)) \\\\\\\\\n          \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\n          \\\\frac{\\\\partial}{\\\\partial{x_1}}(f(x_n)) & \\\\cdots & \\\\frac{\\\\partial}{\\\\partial{x_n}}(f(x_n)) \\\\\\\\\n         \\\\end{bmatrix} =\n\n         \\\\begin{bmatrix}\n          f\\\\prime & 0 & \\\\cdots & 0 \\\\\\\\\n          0 & f\\\\prime & \\\\dots & 0 \\\\\\\\\n          \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\n          0 & 0 & \\\\cdots & f\\\\prime \\\\end{bmatrix}\n         $$\"]\n\n  [:p \"Moreover, matrix multiplying against a diagonal matrix like this one\n       is analogous to performing element-wise multiplication against a matrix filled\n       with whatever is occupying this diagonal,\"(utils/make-footnote \"6\" \"sixth-footnote-a\" \"sixth-footnote-b\")\n       \" where the element-wise \u2013 or Hadamard \u2013\u00a0product between two matrices is denoted\n       \\\\(\\\\boldsymbol{A} \\\\odot \\\\boldsymbol{B}\\\\). This is what we do in practice,\n       because the Hadamard product has far fewer operations.\"]\n\n  [:p \"In fact, the whole reason we vectorize to begin with\n       is because bundling things into matrices and doing batch\n       operations is faster than doing things one-by-one. And knowing\n       only these matrix calculus primitives, we can pretty much\n       differentiate through any vanilla neural net!\"]\n\n[:h1 {:class \"post-section-header\"} \"The Generalized Chain Rule\"]\n\n[:p \"I would be remiss if I didn't mention one more thing though.\n     Isn't it kind of convenient how the chain rule just automatically\n     generalized to Jacoboians? Why did that happen?\"]\n\n  [:p \"It has to do with the deeper definition of derivatives as linear maps.\n       Even if we don't often think of them in this way, derivatives naturally satisfy\n       the two properties of linearity: the derivative of a sum is the sum\n       of derivatives, and the derivative of a function times a scalar is the derivative\n       of that function times the scalar (yes, I'm aware English breaks down here).\"]\n\n   [:p \"$$D(f + g) = Df + Dg \\\\hspace{1cm} D(cf) = c(Df)$$\"]\n\n   [:p \"And because we can represent linear maps as matrices, we can represent\n        derivatives as matrices too. And these are what we've been calling Jacobians all along!\n        When our function maps from scalars to scalars, then our Jacobian\n        is just a one-element matrix \u2013 a scalar, or the classical\n        definition of the derivative. When it maps\n        from vectors to scalars, it's a matrix with a single column \u2013\u00a0a vector,\n        a.k.a the gradient. And when it maps from vectors to vectors, it's a completely\n        filled matrix, the way we originally introduced it.\"]\n\n   [:p \"In other words, we're always working with Jacobians, where a Jacobian\n        represents all the ways a function could change given some nudge\n        to its input. And the chain rule is nothing more than a statement about the Jacobian of\n        a function composition, which says that given two functions \\\\(f\\\\) and \\\\(g\\\\),\n        the Jacobian of their composition \\\\(f(g(x))\\\\), where \\\\(x\\\\) could be a scalar,\n        vector, matrix, or even tensor, is the Jacobian of \\\\(f\\\\) at \\\\(g\\\\) \" [:em \"composed\"] \" with\n         the Jacobian of \\\\(g\\\\) at \\\\(x\\\\).\n        \"]\n\n   [:p \"$$D(f \\\\circ g)(x) = Df(g(x)) \\\\circ Dg(x)$$\"]\n\n   [:p \"So it's not that the chain rule magically generalizes to Jacobians \u2013\n        it's defined in terms of them to begin with. Traditional calculus\n        just doesn't expose them in their full generality. And the only reason\n        there's a notion of multiplication in the chain rule to begin with\n        is because composing linear maps is defined as matrix multiplication.\"]\n\n   [:p \"This is a long-winded way of saying that the derivative is the Jacobian - they're equivalent\n        concepts. And the different branches of calculus just study increasingly less specialized\n        versions of it. In that sense, all automatic differentiation is\n        is a way of computing some Jacobian-vector product, or the Jacobian at a specific\n        input.\"]\n\n   [:p \"Thinking about things in terms of Jacobians, also highlights an important difference between forward-mode\n        and reverse-mode.\n        One pass with forward mode computes the derivatives of all outputs with respect to a single input.\n        Conversely, one pass with reverse-mode computes the derivatives of a single output with respect to all inputs.\"]\n\n   [:p \"In other words, forward-mode computes the Jacobian column-by-column whereas reverse-mode does\n        it row-by-row. That's why functions with many outputs (many rows) are better suited to forward-mode\n        and those with many inputs (many columns) to reverse-mode.\"]\n\n[:h1 {:class \"post-section-header\"} \"Conclusion\"]\n\n[:p \"So to recap, automatic differentiation is a way of quickly computing derivatives with computers.\n     It has both a forward-mode and reverse-mode implementation, the latter of which is\n     used in neural nets. And like all truly great ideas it is based on a simple, but\n     piercing insight: that the amount of expressions we can differentiate grows\n     exponentially with the amount of elementary derivatives that we know. In other words,\n     the chain rule is a lot more powerful than we may always give it credit for.\"]\n\n[:p \"It also shows us the power in simple tricks such as a memoization \u2013 tools\n     of the trade from a branch of computer science known as \"(utils/link \"dynamic programming\" \"https://en.wikipedia.org/wiki/Dynamic_programming\")\n     \". The thing to take away from this is that even if something looks\n     like it'd be expensive to compute at first glance,\n     it might be unitituively cheap once we figure out how\n     to reuse computation and reduce redunancy.\"]\n\n[:p \"All in all, automatic differentiation is one powerhouse of an algorithm,\n     often cited as one of the most important ones to have come forward in\n     the twentieth century. Understanding it well opens\n     up many doors in scientific computing and beyond, and research\n     in the field is ongoing. Given that neural networks only\n     discovered it relatively recently,\"(utils/make-footnote \"7\" \"seventh-footnote-a\" \"seventh-footnote-b\")\n     \" it might very well be\n     that there are other \"[:q \"killer\"] \" applications of the\n     technique yet to be found. After all, there's not\n     much you can't solve with derivatives.\"]\n\n[:h1 {:class \"post-section-header\"} \"References\"]\n\n   [:ul {:style {:list-style-type \"circle\"}}\n    [:li (utils/link \"Ha\u030avard Berland's Slides\" \"https://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf\")]\n    [:li (utils/link \"Calculus on Computational Graphs\" \"https://colah.github.io/posts/2015-08-Backprop/\")]\n    [:li (utils/link \"How to Differentiate with a Computer\" \"http://www.ams.org/publicoutreach/feature-column/fc-2017-12\")]\n    [:li (utils/link \"Computing Neural Network Gradients\" \"https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf\")]\n    [:li (utils/link \"Vector, Matrix, and Tensor Derivatives\" \"http://cs231n.stanford.edu/vecDerivs.pdf\")]\n    ]\n\n\n[:h1 {:class \"post-section-header\"} \"Footnotes\"]\n\n\n[:p {:id \"first-footnote-b\"} (utils/bold \"1.\") \" It's relatively trivial to demonstrate this:\"]\n  [:p {:style {:overflow-x \"auto\" :overflow-y \"hidden\"}}\n\n \"$$\\\\begin{aligned}\n    \\\\lim_{h\\\\to 0} \\\\frac{f(x+h, y) - f(x, y)}{h}\\\\hspace{1cm}\n    &\\\\textrm{(Definition of the derivative of } f(x,y) = x^2 + y^2 \\\\hspace{0.1cm}\\\\textrm{ w.r.t to } x)\n    \\\\\\\\ \\\\\\\\ \\\\lim_{h\\\\to 0} \\\\frac{(x+h)^2 + y^2 - (x^2 + y^2)}{h}\\\\hspace{1cm}\n    &\\\\textrm{(Simplification)}\n    \\\\\\\\ \\\\\\\\ \\\\lim_{h\\\\to 0} \\\\frac{(x+h)^2 - x^2}{h}\\\\hspace{1cm}\n    &\\\\textrm{(Definition of the derivative of } f(x) = x^2 \\\\hspace{0.1cm}\\\\textrm{ w.r.t to } x)\n    \\\\end{aligned}$$\"]\n\n   [:p \"As you see, \\\\(y\\\\) washes out. Another way to think\n    about this is that since \\\\(y\\\\) isn't being nudged by some inifnitesimal\n    \\\\(h\\\\) like \\\\(x\\\\) is, its presence in the first term will be completely canceled\n    out by its presence in the second, which is just another way of saying\n    that since it isn't changing, it is \" [:q \"invisble\"] \" as far as the\n    derivative is concerned.\"\n\n (utils/make-footnote \"\u21a9\" \"first-footnote-b\" \"first-footnote-a\")\n ]\n\n\n[:p  (utils/bold \"2.\") \" The simplest error function you could have is just\n         the difference between the prediction and the label:\n         \\\\(E(\\\\hat{y}, y) = \\\\hat{y} - y\\\\). In practice\n         we actually square this difference to accentuate\n         really bad predictions and so that we don't have to deal\n         with a negative result. This is known as the square error:\n         \\\\(E(\\\\hat{y}, y) = (\\\\hat{y} - y)^2\\\\).\n         \" (utils/make-footnote \"\u21a9\" \"second-footnote-b\" \"second-footnote-a\")]\n\n\n [:p  (utils/bold \"3.\") \" What is \\\\(\\\\boldsymbol{\\\\theta}\\\\) doing in the susbcript of the gradient symbol?\n       It's just a way of saying that even though our function\n       also takes in an input and a label, we don't want to include their partials\n       in our gradient \u2013 after all, we can't\n       change our input or label, those two pieces\n       of information are set for the problem.\"\n       (utils/make-footnote \"\u21a9\" \"third-footnote-b\" \"third-footnote-a\")]\n\n [:p (utils/bold \"4.\") \" \\\\(\\\\eta\\\\) is a hyperparamater, or a variable\n          the neural net does not learn by itself, but must be explicitly\n          set. It's known as learning rate, since it controls\n          how quickly we descend the error-surface, and thus how\n          fast we learn.\"\n       (utils/make-footnote \"\u21a9\" \"fourth-footnote-b\" \"fourth-footnote-a\")]\n\n\n[:p (utils/bold \"5.\") (utils/make-footnote \"\u21a9\" \"fifth-footnote-b\" \"fifth-footnote-a\") ]\n\n\n [:p  (utils/bold \"6.\")\" A demonstration of this property with conrete matrices:\n        $$\\\\begin{align}\n          \\\\begin{bmatrix}\n            a & b \\\\\\\\\n            c & d \\\\\\\\\n          \\\\end{bmatrix}\n          \\\\begin{bmatrix}\n              x & 0 \\\\\\\\\n              0 & x \\\\\\\\\n          \\\\end{bmatrix} =\n          \\\\begin{bmatrix}\n              ax & bx \\\\\\\\\n              cx & dx \\\\\\\\\n          \\\\end{bmatrix}\\\\hspace{1cm}&\\\\textrm{(Multiplication against diagonal matrix)} \\\\\\\\ \\\\\\\\\n\n          \\\\begin{bmatrix}\n              a & b \\\\\\\\\n              c & d \\\\\\\\\n            \\\\end{bmatrix}\n            \\\\odot\n            \\\\begin{bmatrix}\n                x & x \\\\\\\\\n                x & x \\\\\\\\\n            \\\\end{bmatrix} =\n            \\\\begin{bmatrix}\n                ax & bx \\\\\\\\\n                cx & dx \\\\\\\\\n            \\\\end{bmatrix}\\\\hspace{1cm}&\\\\textrm{(Hadamard product)}\n            \\\\end{align}$$\n            As you can see they are equivalent. But wheras\n            the Hadamard product only has to do \\\\(N \\\\times M\\\\)\n            operations when multiplying two\n            \\\\(N \\\\times M\\\\) matrices, matrix multiplication\n            has to do \\\\(N \\\\times M \\\\times M \\\\times (M - 1)\\\\) operations.\"\n       (utils/make-footnote \"\u21a9\" \"sixth-footnote-b\" \"sixth-footnote-a\")]\n\n       [:p\n        (utils/bold \"7.\") \" While researching for this post I found out\n        that in days of old people used to compute their network's derivatives\n        symbolically by hand, hardcode them into computers, and then optimize them\n        So obviously networks couldn't be deep! Read more \"(utils/link \"here\" \"https://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/\")\".\"\n        (utils/make-footnote \"\u21a9\" \"seventh-footnote-b\" \"seventh-footnote-a\")]\n\n\n\n\n  ])\n\n\n\n\n\n(defn media-query-1 []\n (at-media {:max-width \"600px\"}\n     [\n      ;[:#special {:display \"none\"}]\n      ;[:#special-2 {:display \"inline-block\"}]\n      ;[:#math-2 {:font-size \"13.5px\"}]\n\n      ;[:#hot-cold-2 {:width \"200px\"}]\n      ]))\n\n(def post\n  {:title \"Automatic Differentiation In Neural Nets\"\n   :date \"12/22/2020\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"deep learning\"]\n   :type \"blog-post\"\n   :overarching \"writing\"\n   :id \"1\"\n   :css (media-query-1)})\n"]}