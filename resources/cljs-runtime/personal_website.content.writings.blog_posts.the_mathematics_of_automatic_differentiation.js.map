{"version":3,"sources":["personal_website/content/writings/blog_posts/the_mathematics_of_automatic_differentiation.cljs"],"mappings":";AAIA,AAAA,AAAKA;AAOL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAqEA,AAAA,AAAA,AAACC,AA0CF,AAAA,AAAA,AAACA,AAeG,AAAA,AAAA,AAAA,AAACC,AAuDmD,AAAA,AAAA,AAAA,AAACA,AAc8C,AAAA,AAAA,AAAA,AAACA;AAoC5G,AAAA,AAAMC;AAAN,AACC,AAAA,AAAA,AAAA,AAAA,AAACC;;AAQF,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIN,AACGC,AAKJ,AAACG","names":["personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-preview","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-content","personal-website.utils/link","personal-website.utils/make-footnote","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/media-query-1","garden.stylesheet.at_media","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation\n  (:require [personal-website.utils :as utils]\n            [garden.stylesheet :refer [at-media]]))\n\n(def post-preview\n  \"Automatic differentiation is the numerical computing technique\n   that gave us the backpropogation algorithm, which is\n   how neural nets learn. In this post, we will explore\n   the mathematics behind it \u2013 both in the context of neural nets\n   and more broadly.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   [:p \"Backpropogation is the cornerstone of the differentiable\n        programming paradigm \u2014 the idea that we can allow programs to optimize\n        their behavior against certain metrics by  differentiating over and updating sets\n        of learnable functions. In fact, it is the very algorithm that allows neural\n        nets to learn! Less talked about is automatic differentiation, the numerical computing technique\n        that makes it possible, and one of the key reasons\n        neural nets are able to transcend theory and work in real life.\"]\n\n   [:p \"Both, however, are essential to a thorough understanding of\n        how and why neural nets are even possible in the first place.\n        In this post, we'll go through the mathematics underpinning both\n        of them, which is a journey far less restricted to the realm\n        of a calculus than you may expect (no spoilers!).\n        That said, there is no escaping the calculus, so if you feel it's\n        worth taking a few moments\n        to review how exactly we take the deriavatives of multivariate functions, continue on.\n        Otherwise, skip here.\n        \"]\n\n   ;[:p \"With that in mind, let\u2019s start very simple. Neural nets are just\n    ;    hierarchies of \u201clayers\u201d  \u2013\u00a0vertical stacks of interconnected units known as\n    ;    neurons. Each neuron is associated with some learnable weight vector which\n    ;    computes a dot product with the vector of incoming values, adjusts it by some\n    ;    (also learnable) scalar, and pumps it through a non-linearity, such that\n    ;    optimizing these learnable parameters can result in very complex \u2013 and\n    ;    seemingly intelligent \u2013\u00a0behavior. \"]\n\n\n    ;[:figure {:class \"img-container\"}\n    ;  [:div {:style {:text-align \"center\"}}\n    ;    [:img {:src \"/auto_diff_nn_pic.svg\"  :style {:width \"65%\"}}]]\n    ;  [:figcaption {:class \"post-caption\"}\n    ;    \"Fig. 1. An example of a simple neural network. An almost laughably basic building block \u2013\n    ;    learnable transformations of information \u2013 allows them to approximate all kinds of complex functions.\"]]\n\n\n    ;[:p \"This is known - it is neural networks in their simplest, most\n    ;    straightforward form. But treating their optimization as a block box makes\n    ;    things seem deceptively easy, when in fact these optimization algorithms\n    ;    (along with the hardware they\u2019re  running on) are the sole differentiator\n    ;    between neural nets being only of theoretical interest versus practical utility,\n    ;    and automatic differentiation underpins every single one of them. So how does it actually work?\"]\n\n   [:h1 {:class \"post-section-header\"} \"Learning To Differentiate\"]\n\n   [:p \"Derivatives of single-variable functions are\n        measurements of how infinitesimal variations to a function's\n        input-space correspond to variations in the output-space. And the same\n        is true of multivariable functions,\n        except now we have many more ways in which\n        we can vary our input-space.\"]\n\n   [:p \"To start though, let\u2019s consider the case of a\n        function of two variables. Such a function can be pictured as a surface\n        above the Cartesian plane, where the height of a point\n        on that surface is calculated using the function\n        \\\\(f(x,y)\\\\) in question. With this in mind, the geometrical\n        interpretation of the derivative is how the elevation of\n        this surface changes given a small step in some direction.\"]\n\n  [:figure {:class \"img-container\"}\n   [:div {:style {:text-align \"center\"}}\n    [:img {:src \"/multi-fn-2.png\" :style {:width \"45%\"}}]\n    [:img {:src \"/multi-fn-4.png\" :style {:width \"45%\"}}]\n     [:figcaption {:class \"post-caption\" :style {:text-align \"left\"}}\n     \"Fig. 2. Examples of surfaces that can be generated\n     by a function of two variables (Source: \"\n     (utils/link \"CalcPlot3D\" \"https://www.monroecc.edu/faculty/paulseeburger/calcnsf/CalcPlot3D/\")\").\"]]]\n\n\n   [:p \"An important thing to notice here is that this step could be in any direction:\n        vertical, horizontal, or some combination of the two (diagonal). For simplicity\n        though, we'll resitrict ourselves to the first two cases\u00a0\u2013 those in which we take\n        steps in purely the \\\\(x\\\\) and \\\\(y\\\\) directions \u2013 for right now.\n        So how would we calculate the derivative of the function \\\\(f(x,y) = x^2 + y^2\\\\)\n        with respect to \\\\(x\\\\) then?\"]\n\n   [:p \"Imagine standing at some point on this function's surface, and walking\n        horizontally in both directions. If we walk such that a spool of\n        yarn unrolls behind  us, the shape this yarn takes will\n        resemble the graph of a single-variate function.\n        More specifically, it will look like\n          \\\\(f(x) = x^2 + C\\\\) (a parabola) where \\\\(C\\\\) is some constant, namely\n        whatever the \\\\(y\\\\)-value of our original point was.\"]\n\n   [:p \"It might not be immediately obvious, but by walking horizontally\n        across our surface we only varied our \\\\(x\\\\)-coordinate, while\n        our \\\\(y\\\\)-coordinate remained constant.\n        Thus, when walking in this way,\n        the surface can be described\n        by a special case of the original function where\n        \\\\(y\\\\) is a constant.\"]\n\n      [:p \"Similarly, a walk\n           in a purely vertical direction is described\n           when \\\\(x\\\\) is a constant. More generally, we would find that\n           all multivariate functions\n           behave in this way when being sliced\n           (or walked upon) purely in the direction of one their variables, such\n           that all others become constants.\"]\n\n  [:figure {:class \"img-container\"}\n   [:div {:style {:text-align \"center\"}}\n    [:img {:src \"/parabaloid-1.png\" :style {:width \"50%\"}}]\n    [:img {:src \"/parabaloid-2.png\" :style {:width \"50%\"}}]\n    [:figcaption {:class \"post-caption\" :style {:text-align \"left\"}}\n    \"Fig. 3. The graph the function \\\\(f(x,y) = x^2 + y^2\\\\), known as a parabaloid.\n    Slicing it across the \\\\(x\\\\)-axis reveals that \\\\(x\\\\)-wise\n    cross-sections are parabolas. In fact, this property defines the surface! (Source: \"\n    (utils/link \"GeoGebra3D\" \"https://www.geogebra.org/3d?lang=en\")\").\"\n     ]]]\n\n   [:p \"In that vein of thought, we might conjecture that when computing\n        the derivative of our function \\\\(f(x,y) = x^2 + y^2\\\\) with\n        respect to \\\\(x\\\\), we can imagine we are taking the\n        derivative of \\\\(f(x) = x^2 + C\\\\), as this is what\n        the surface looks like when moving in the \\\\(x\\\\) direction.\n        Then the deriavative with respect to \\\\(x\\\\)\n        (what we call its partial derivative)\n        would be \\\\(2x\\\\). That is to say\n        \\\\(\\\\frac{\\\\partial{f}}{\\\\partial{x}}\\\\ = 2x\\\\)\n        (as would \\\\(\\\\frac{\\\\partial{f}}{\\\\partial{y}}\\\\)).\"]\n\n   [:p \"And it turns out that this thought is in fact correct!\"\n        (utils/make-footnote \"1\" \"first-footnote-a\" \"first-footnote-b\")\n        \" But how do we generalize this idea when taking steps across our surface that aren't\n          purely horizontal or vertical in nature? For instance,\n          what is the derivative associated with some step in the\n        direction of the vector \\\\(\\\\langle 1, 1 \\\\rangle \\\\)?\"]\n\n    [:p \"\n        Well, a step in this direction is equivelant to one step\n        in both the \\\\(x\\\\) direction and \\\\(y\\\\) direction.\n        And since we know how our function changes for both a pure step in the\n        \\\\(x\\\\) direction (\\\\(\\\\frac{\\\\partial{f}}{\\\\partial{x}}\\\\)) and \\\\(y\\\\)\n        direction (\\\\(\\\\frac{\\\\partial{f}}{\\\\partial{y}}\\\\)), the derivative\n        in this direction is just a sum of the two: \\\\(\\\\frac{\\\\partial{f}}{\\\\partial{x}} + \\\\frac{\\\\partial{f}}{\\\\partial{y}}\\\\ =\n        2x + 2y\\\\).\"]\n\n  [:p \"More generally, given any multivariate function of \\\\(n\\\\) variables,\n       we can express a derivative with respect to some vector \u2013 what we\n       call a directional derivative \u2013 as the linear combination of that function's partial derivatives\n       and the components of that vector. Said another way, given the vectors:\"]\n\n  [:p \"$$ \\\\boldsymbol{\\\\vec{\\\\alpha}} = \\\\begin{bmatrix}\n          a_{1} \\\\\\\\\n          a_{2} \\\\\\\\\n          \\\\vdots \\\\\\\\\n          a_{n} \\\\\\\\\n          \\\\end{bmatrix}\n          \\\\hspace{1cm}\n          \\\\boldsymbol{\\\\vec{\\\\beta}} =\n          \\\\begin{bmatrix}\n          \\\\frac{\\\\partial{f}}{\\\\partial{x_1}} \\\\\\\\\n          \\\\frac{\\\\partial{f}}{\\\\partial{x_2}} \\\\\\\\\n          \\\\vdots \\\\\\\\\n          \\\\frac{\\\\partial{f}}{\\\\partial{x_n}} \\\\\\\\\n          \\\\end{bmatrix}\n        $$\"]\n\n   [:p {:style {:margin-bottom \"25px\"}} \"Then the derivative of \\\\(f(x_1, x_2, \\\\ldots, x_n)\\\\) with respect to \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}}\\\\) is \\\\(\\\\boldsymbol{\\\\vec{\\\\alpha}} \\\\cdot \\\\boldsymbol{\\\\vec{\\\\beta}}\\\\).\n        Just as before, the given vector can be decomposed as the sum of steps in the direction\n        of a single variable (\\\\(a_1\\\\hat{\\\\imath} + a_2\\\\hspace{0.08cm}\\\\skew{2}{\\\\hat}{\\\\jmath} + \\\\ldots\\\\)),\n        and as a function's partials represent the effects of such steps,\n        the derivative in this direction is just their sum,\n        one for each step taken in the direction they represent (\\\\(a_1 \\\\frac{\\\\partial{f}}{\\\\partial{x_1}} + a_2 \\\\frac{\\\\partial{f}}{\\\\partial{x_2}} + \\\\ldots \\\\)).\"]\n\n\n   ;; some image\n   [:figure {:class \"img-container\"}\n   [:img {:src \"/parabaloid-5.png\" :style {:width \"50%\"}}]\n   [:img {:src \"/parabaloid-6.png\" :style {:width \"50%\"}}]\n  [:figcaption {:class \"post-caption\"}\n   \"Fig. 4. The slice of the parabaloid in the direction\n   \\\\(\\\\langle 1, 1 \\\\rangle\\\\) is also a parabola, albeit a\n   steeper one. It's generating function is \\\\(2n^2\\\\) for every \\\\(n\\\\) steps along this vector (see\n   right image where an \\\\(n = 1\\\\) returns \\\\(2\\\\)).\"]]\n\n   [:p \"While this isn't rigorous\"\n       \" (how can you take multiple infinitesimal steps?),\" (utils/make-footnote \"2\" \"second-footnote-a\" \"second-footnote-b\")\"\n        it suits our immediate purposes, as we can now calculate both partial\n        and directional derivatives. A natural question then, is what derivative is the greatest?\n        In what direction does a function increase most rapidly?\"]\n\n   [:p \"In other words, what vector, when dotted with the vector containing\n        the partials of our function, yields the largest possible\n        value? Well, the largest dot product a vector can have (when only\n        considering vectors of the same magnitude) is with\n        itself, so the direction of steepest ascent must be represented by the thing\n        we're dotting against \u2013\u00a0the vector containing the partials of our function! \"]\n\n   [:p \"Intuitively, this makes sense too, as how much this vector points in a given\n        direction is equivelant to the derivative in that direction, such that it points\n        more in steeper directions and less in shallow ones, thus becoming the steepest direction itself.\" (utils/make-footnote \"3\" \"third-footnote-a\" \"third-footnote-b\")\"\n        It is yielded by the \\\\(\\\\nabla\\\\) \u2013 or gradient \u2013 operation:\"]\n\n   [:p \"$$\\\\nabla f(x_1, x_2, \\\\ldots, x_n) =\n          \\\\begin{bmatrix}\n          \\\\frac{\\\\partial{f}}{\\\\partial{x_1}} \\\\\\\\\n          \\\\frac{\\\\partial{f}}{\\\\partial{x_2}} \\\\\\\\\n          \\\\vdots \\\\\\\\\n          \\\\frac{\\\\partial{f}}{\\\\partial{x_n}} \\\\\\\\\n          \\\\end{bmatrix}$$\"]\n\n\n\n   [:h1 {:class \"post-section-header\"} \"Gradient Descent\"]\n\n\n    ;The geometric interpretation associated with\n        ;    this idea is that instead of taking a step across\n;\n;            Even though we only can only really visualize\n;            functions of two variables as surfaces, it helps to hold this\n;            mental image \u2013 of multivariable functions encoding surfaces \u2013\n;            in our heads. With that in mind, we can imagine that for some\n;            multivariable function of \\\\(n\\\\) inputs some surface is generated\n;            with some higher-dimesnional analogs for the concepts we would associate\n;            with 2D and 3D surfaces \u2013 that is to say, some sense of steepness,\n;            angle and direction, etc. We don't need to concretize what these might\n;            look like (at least not visually) but accepting that these properties\n;            exist in some shape or form will allow us to reason with them and make\n;            expanding upon our definition of the derivative.\n\n\n\n   ])\n\n\n(defn media-query-1 []\n (at-media {:max-width \"600px\"}\n     [\n      ;[:#first-footnote-b {:font-size \"13.5px\"}]\n      ;[:#math-2 {:font-size \"13.5px\"}]\n\n      ;[:#hot-cold-2 {:width \"200px\"}]\n      ]))\n\n(def post\n  {:title \"The Mathematics Of Automatic Differentiation\"\n   :date \"2020/10/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"deep learning\"]\n   :type \"blog-post\"\n   :overarching \"writing\"\n   :id \"1\"\n   :css (media-query-1)})\n"]}