{"version":3,"sources":["personal_website/content/writings/blog_posts/the_mathematics_of_automatic_differentiation.cljs"],"mappings":";AAGA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC;AAyDL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIF,AACGC","names":["personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-preview","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post-content","personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.the-mathematics-of-automatic-differentiation\n  (:require [personal-website.utils :as utils]))\n\n(def post-preview\n  \"Automatic differentiation is the numerical computing algorithm that allows us\n   to optimize neural nets with practical time and space costs. In this post, we will\n   explore the mathematics that makes it possible and implmenent it ourselves.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   [:p \"Automatic differentiation is the cornerstone of the differentiable programming paradigm \u2014\n       the idea that we can allow programs to optimize their behavior against certain metrics by\n       differentiating over and updating sets of  learnable functions. In fact, it is the very algorithm\n       that allows neural nets to learn! And while it\u2019s role in the story of deep learning is not always a leading one,\n       it is an elegant piece of mathematics whose cleverness deserves to be chronicled. Furthermore,\n       I doubt I can put better than Feynman the importance of leaving the realm of theory every once in a while\n       and using one\u2019s own two hands (\u201cWhat I cannot make, I do not understand\u201d), so as we build up to this idea\n       we will implement a toy AD engine in C which will be capable of training vanilla neural nets.\"]\n\n   [:p \"With that in mind, let\u2019s start very simple. Neural nets are just hierarchies of \u201clayers\u201d -\n        vertical stacks of interconnected units known as neurons. Each neuron is associated with some learnable\n        weight vector which computes a dot product with the vector of incoming values, adjusts it by some (also learnable)\n        scalar and pumps it through a non-linearity, such that optimizing these learnable parameters\n        results in a function with   the desired behavior, ranging from language translation\n        to caption generation.\"]\n\n\n    [:figure {:class \"img-container\"}\n      [:div {:style {:text-align \"center\"}}\n        [:img {:src \"/auto_diff_nn_pic.svg\"  :style {:width \"65%\"}}]]\n      [:figcaption {:class \"post-caption\"}\n        \"Fig. 1. An example of a simple neural network. A basic building block \u2013\n        learnable transformations of information \u2013 allows them to approximate all kinds of complex functions.\"]]\n\n\n    [:p \"This is known - it is neural networks\n        in their simplest, most straightforward form. But treating optimization as a block box makes things seem deceptively\n        easy, when in fact these optimization algorithms (along with the hardware they\u2019re  running on) are the sole\n        differentiator between neural nets being only of theoretical interest versus practical utility, and automatic\n        differentiation underpins every single one of them. So how does it actually work?\"]\n\n   [:h1 {:class \"post-section-header\"} \"Learning To Differentiate\"]\n\n   [:p \"As you probably know already, it has everything to do with calculus. But before we dive deep into the\n        practical implementation (which, ironically enough, calls on the most abstract math) it's worth laying down\n        the basics of how we take the derivatives of multivariable functions.\"]\n\n   [:p \"Derivatives are just measurements of the affect of a function\u2019s inputs on its outputs, and the same remains true\n        when taking derivatives of a function with multiple such inputs. To start, let\u2019s consider a function with two inputs,\n        as its easiest to visualize.\"]\n\n   [:figure {:class \"img-container\"}\n    [:div {:style {:text-align \"center\"}}\n     [:img {:src \"/tangent_line_to_graph.png\"  :style {:width \"65%\"}}]]\n    [:figcaption {:class \"post-caption\"}\n     \"Fig. 2. The tangent line to the graph of some function. This is the classical way to\n     conceptualize the derivative for functions of a single variable. It can be extended\n     to the multivariable case as well.\"]]\n\n\n   [:h1 \"Computational Graphs\"]\n\n\n   ])\n\n(def post\n  {:title \"The Mathematics Of Automatic Differentiation\"\n   :date \"2020/10/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :overarching \"writing\"\n   :id \"1\"})\n"]}