{"version":3,"sources":["personal_website/content/writings/blog_posts/transformer_post/post.cljs"],"mappings":";AAGA,AAAA,AAAKA;AAKL,AAAA,AAAA,AAAA,AAAKC;AAIL,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAA,AAAKC,AAGIF,AACGC","names":["personal-website.content.writings.blog-posts.transformer-post.post/post-preview","personal-website.content.writings.blog-posts.transformer-post.post/post-content","personal-website.content.writings.blog-posts.transformer-post.post/post"],"sourcesContent":["(ns personal-website.content.writings.blog-posts.transformer-post.post)\n\n\n(def post-preview\n  \"The transformer \u2013 a neural network model desiged to replace traditional\n  seq2seq architecture \u2013 introduces a powerful abstraction to think about\n  attention.\")\n\n(def post-content\n  [:div ;{:style {:text-indent \"25px\"}}\n   \"blah\"])\n\n(def post\n  {:title \"An Introduction To Attention Via The Transformer\"\n   :date \"2020/8/29\"\n   :show post-preview\n   :content post-content\n   :tags [\"mathematics\", \"differentiable programming\"]\n   :type \"blog-post\"\n   :id \"0\"})\n"]}