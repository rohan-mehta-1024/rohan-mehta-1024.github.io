TITLE=PRIMES
DATE=5/12/2022
PREVIEW=You can barely read a single deep learning paper nowadays without encountering the modern  attention mechanism. But why does it work â€“ and work so well?
TAGS=Deep Learning, NLP
DRAFT=true


Introduction 
1<br>
2<br>
3<br>
4<br>
5<br>
6<br>


<h1>Problem No. 1</h1>


First key insight: the projection we care about is a trapezoid. Because the other two possible cases could not have an angle of 60 degrees.
